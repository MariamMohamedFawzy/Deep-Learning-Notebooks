{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INCREASE MNIST MODEL TO REACH AT LEAST 97%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting ../data/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting ../data/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../data/mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"../data/mnist\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 10: 49462.9\n",
      "Average loss at step 20: 6585.9\n",
      "Average loss at step 30: 1948.9\n",
      "Average loss at step 40: 1246.3\n",
      "Average loss at step 50: 848.3\n",
      "Average loss at step 60: 555.2\n",
      "Average loss at step 70: 460.3\n",
      "Average loss at step 80: 434.5\n",
      "Average loss at step 90: 376.0\n",
      "Average loss at step 100: 298.7\n",
      "Average loss at step 110: 288.6\n",
      "Average loss at step 120: 290.7\n",
      "Average loss at step 130: 201.3\n",
      "Average loss at step 140: 140.2\n",
      "Average loss at step 150: 186.7\n",
      "Average loss at step 160: 167.9\n",
      "Average loss at step 170: 128.4\n",
      "Average loss at step 180: 188.6\n",
      "Average loss at step 190: 195.5\n",
      "Average loss at step 200: 117.7\n",
      "Average loss at step 210: 140.7\n",
      "Average loss at step 220: 167.2\n",
      "Average loss at step 230: 188.0\n",
      "Average loss at step 240: 140.5\n",
      "Average loss at step 250: 161.8\n",
      "Average loss at step 260: 141.8\n",
      "Average loss at step 270: 169.2\n",
      "Average loss at step 280: 130.6\n",
      "Average loss at step 290: 149.6\n",
      "Average loss at step 300: 118.7\n",
      "Average loss at step 310: 111.7\n",
      "Average loss at step 320:  69.3\n",
      "Average loss at step 330:  84.4\n",
      "Average loss at step 340: 112.4\n",
      "Average loss at step 350:  82.9\n",
      "Average loss at step 360: 126.4\n",
      "Average loss at step 370:  86.0\n",
      "Average loss at step 380:  61.7\n",
      "Average loss at step 390:  89.3\n",
      "Average loss at step 400:  99.3\n",
      "Average loss at step 410:  70.2\n",
      "Average loss at step 420: 125.5\n",
      "Average loss at step 430:  96.8\n",
      "Average loss at step 440: 106.0\n",
      "Average loss at step 450:  71.0\n",
      "Average loss at step 460:  43.0\n",
      "Average loss at step 470:  78.5\n",
      "Average loss at step 480:  51.5\n",
      "Average loss at step 490:  77.7\n",
      "Average loss at step 500:  66.5\n",
      "Average loss at step 510:  88.5\n",
      "Average loss at step 520:  29.2\n",
      "Average loss at step 530:  92.1\n",
      "Average loss at step 540:  56.0\n",
      "Average loss at step 550:  46.6\n",
      "Average loss at step 560:  62.2\n",
      "Average loss at step 570:  73.8\n",
      "Average loss at step 580:  45.1\n",
      "Average loss at step 590:  75.6\n",
      "Average loss at step 600:  55.5\n",
      "Average loss at step 610:  85.1\n",
      "Average loss at step 620:  34.3\n",
      "Average loss at step 630:  65.0\n",
      "Average loss at step 640:  82.2\n",
      "Average loss at step 650:  61.6\n",
      "Average loss at step 660:  52.2\n",
      "Average loss at step 670:  67.5\n",
      "Average loss at step 680:  40.3\n",
      "Average loss at step 690:  71.2\n",
      "Average loss at step 700:  44.9\n",
      "Average loss at step 710:  48.2\n",
      "Average loss at step 720:  41.5\n",
      "Average loss at step 730:  51.7\n",
      "Average loss at step 740:  53.0\n",
      "Average loss at step 750:  63.7\n",
      "Average loss at step 760:  66.9\n",
      "Average loss at step 770:  33.8\n",
      "Average loss at step 780:  75.9\n",
      "Average loss at step 790:  56.9\n",
      "Average loss at step 800:  58.4\n",
      "Average loss at step 810:  58.4\n",
      "Average loss at step 820:  57.1\n",
      "Average loss at step 830:  49.5\n",
      "Average loss at step 840:  47.1\n",
      "Average loss at step 850:  35.3\n",
      "Average loss at step 860:  32.2\n",
      "Average loss at step 870:  38.8\n",
      "Average loss at step 880:  25.0\n",
      "Average loss at step 890:  40.9\n",
      "Average loss at step 900:  32.9\n",
      "Average loss at step 910:  37.1\n",
      "Average loss at step 920:  37.9\n",
      "Average loss at step 930:  25.9\n",
      "Average loss at step 940:  32.9\n",
      "Average loss at step 950:  30.3\n",
      "Average loss at step 960:  57.5\n",
      "Average loss at step 970:  55.0\n",
      "Average loss at step 980:  27.1\n",
      "Average loss at step 990:  36.3\n",
      "Average loss at step 1000:  29.8\n",
      "Average loss at step 1010:  37.7\n",
      "Average loss at step 1020:  59.4\n",
      "Average loss at step 1030:  27.8\n",
      "Average loss at step 1040:  30.7\n",
      "Average loss at step 1050:  18.9\n",
      "Average loss at step 1060:  40.3\n",
      "Average loss at step 1070:  36.7\n",
      "Average loss at step 1080:  25.0\n",
      "Average loss at step 1090:  20.0\n",
      "Average loss at step 1100:  38.8\n",
      "Average loss at step 1110:  47.2\n",
      "Average loss at step 1120:  30.6\n",
      "Average loss at step 1130:  24.6\n",
      "Average loss at step 1140:  33.5\n",
      "Average loss at step 1150:  47.5\n",
      "Average loss at step 1160:  36.8\n",
      "Average loss at step 1170:  41.5\n",
      "Average loss at step 1180:  36.7\n",
      "Average loss at step 1190:  35.3\n",
      "Average loss at step 1200:  37.2\n",
      "Average loss at step 1210:  35.4\n",
      "Average loss at step 1220:  29.5\n",
      "Average loss at step 1230:  31.6\n",
      "Average loss at step 1240:  39.6\n",
      "Average loss at step 1250:  42.3\n",
      "Average loss at step 1260:  30.3\n",
      "Average loss at step 1270:  18.5\n",
      "Average loss at step 1280:  12.5\n",
      "Average loss at step 1290:  44.7\n",
      "Average loss at step 1300:  44.3\n",
      "Average loss at step 1310:  29.0\n",
      "Average loss at step 1320:  33.6\n",
      "Average loss at step 1330:  26.6\n",
      "Average loss at step 1340:  27.7\n",
      "Average loss at step 1350:  28.8\n",
      "Average loss at step 1360:  31.3\n",
      "Average loss at step 1370:  31.7\n",
      "Average loss at step 1380:  14.3\n",
      "Average loss at step 1390:  22.8\n",
      "Average loss at step 1400:  29.9\n",
      "Average loss at step 1410:  30.3\n",
      "Average loss at step 1420:  31.0\n",
      "Average loss at step 1430:  35.4\n",
      "Average loss at step 1440:  37.7\n",
      "Average loss at step 1450:  26.6\n",
      "Average loss at step 1460:  25.0\n",
      "Average loss at step 1470:  20.0\n",
      "Average loss at step 1480:   7.3\n",
      "Average loss at step 1490:  32.8\n",
      "Average loss at step 1500:  33.1\n",
      "Average loss at step 1510:  36.1\n",
      "Average loss at step 1520:  41.3\n",
      "Average loss at step 1530:  25.3\n",
      "Average loss at step 1540:  21.5\n",
      "Average loss at step 1550:  36.8\n",
      "Average loss at step 1560:  28.2\n",
      "Average loss at step 1570:  14.1\n",
      "Average loss at step 1580:  43.1\n",
      "Average loss at step 1590:  30.9\n",
      "Average loss at step 1600:  25.5\n",
      "Average loss at step 1610:  31.2\n",
      "Average loss at step 1620:  26.6\n",
      "Average loss at step 1630:  28.6\n",
      "Average loss at step 1640:  35.3\n",
      "Average loss at step 1650:  14.5\n",
      "Average loss at step 1660:  19.8\n",
      "Average loss at step 1670:  13.6\n",
      "Average loss at step 1680:  20.9\n",
      "Average loss at step 1690:  18.8\n",
      "Average loss at step 1700:  18.8\n",
      "Average loss at step 1710:  17.3\n",
      "Average loss at step 1720:  12.4\n",
      "Average loss at step 1730:   9.1\n",
      "Average loss at step 1740:  13.7\n",
      "Average loss at step 1750:  17.3\n",
      "Average loss at step 1760:  10.4\n",
      "Average loss at step 1770:   5.3\n",
      "Average loss at step 1780:  11.8\n",
      "Average loss at step 1790:  17.4\n",
      "Average loss at step 1800:  14.8\n",
      "Average loss at step 1810:   8.6\n",
      "Average loss at step 1820:  14.3\n",
      "Average loss at step 1830:  11.4\n",
      "Average loss at step 1840:   7.9\n",
      "Average loss at step 1850:  17.2\n",
      "Average loss at step 1860:  12.1\n",
      "Average loss at step 1870:  18.5\n",
      "Average loss at step 1880:  24.8\n",
      "Average loss at step 1890:   9.8\n",
      "Average loss at step 1900:   9.4\n",
      "Average loss at step 1910:  15.6\n",
      "Average loss at step 1920:  19.6\n",
      "Average loss at step 1930:  16.6\n",
      "Average loss at step 1940:   8.5\n",
      "Average loss at step 1950:  14.6\n",
      "Average loss at step 1960:  15.5\n",
      "Average loss at step 1970:  27.5\n",
      "Average loss at step 1980:  13.3\n",
      "Average loss at step 1990:  29.1\n",
      "Average loss at step 2000:  22.2\n",
      "Average loss at step 2010:  16.9\n",
      "Average loss at step 2020:  21.7\n",
      "Average loss at step 2030:  16.4\n",
      "Average loss at step 2040:  26.2\n",
      "Average loss at step 2050:  27.9\n",
      "Average loss at step 2060:  16.5\n",
      "Average loss at step 2070:  37.1\n",
      "Average loss at step 2080:  12.9\n",
      "Average loss at step 2090:  17.6\n",
      "Average loss at step 2100:  17.9\n",
      "Average loss at step 2110:  21.5\n",
      "Average loss at step 2120:  12.2\n",
      "Average loss at step 2130:  24.2\n",
      "Average loss at step 2140:  16.4\n",
      "Average loss at step 2150:  21.4\n",
      "Average loss at step 2160:  17.9\n",
      "Average loss at step 2170:  27.6\n",
      "Average loss at step 2180:  15.8\n",
      "Average loss at step 2190:  24.6\n",
      "Average loss at step 2200:  13.3\n",
      "Average loss at step 2210:  23.1\n",
      "Average loss at step 2220:  26.5\n",
      "Average loss at step 2230:  20.8\n",
      "Average loss at step 2240:  18.2\n",
      "Average loss at step 2250:  13.5\n",
      "Average loss at step 2260:  22.2\n",
      "Average loss at step 2270:  18.6\n",
      "Average loss at step 2280:   4.3\n",
      "Average loss at step 2290:  11.3\n",
      "Average loss at step 2300:  14.8\n",
      "Average loss at step 2310:  11.2\n",
      "Average loss at step 2320:  23.3\n",
      "Average loss at step 2330:   5.2\n",
      "Average loss at step 2340:  19.5\n",
      "Average loss at step 2350:  10.2\n",
      "Average loss at step 2360:  21.9\n",
      "Average loss at step 2370:  15.8\n",
      "Average loss at step 2380:  35.4\n",
      "Average loss at step 2390:  17.6\n",
      "Average loss at step 2400:  23.5\n",
      "Average loss at step 2410:  13.7\n",
      "Average loss at step 2420:  17.8\n",
      "Average loss at step 2430:  24.8\n",
      "Average loss at step 2440:  13.1\n",
      "Average loss at step 2450:  18.6\n",
      "Average loss at step 2460:  15.4\n",
      "Average loss at step 2470:  19.0\n",
      "Average loss at step 2480:  22.7\n",
      "Average loss at step 2490:  25.3\n",
      "Average loss at step 2500:  19.4\n",
      "Average loss at step 2510:  21.7\n",
      "Average loss at step 2520:  24.9\n",
      "Average loss at step 2530:  10.6\n",
      "Average loss at step 2540:  12.2\n",
      "Average loss at step 2550:   6.3\n",
      "Average loss at step 2560:  12.9\n",
      "Average loss at step 2570:   7.4\n",
      "Average loss at step 2580:  12.2\n",
      "Average loss at step 2590:  10.0\n",
      "Average loss at step 2600:  10.3\n",
      "Average loss at step 2610:  13.1\n",
      "Average loss at step 2620:   9.6\n",
      "Average loss at step 2630:   9.0\n",
      "Average loss at step 2640:   8.1\n",
      "Average loss at step 2650:  10.1\n",
      "Average loss at step 2660:  11.7\n",
      "Average loss at step 2670:   7.4\n",
      "Average loss at step 2680:   9.9\n",
      "Average loss at step 2690:  10.8\n",
      "Average loss at step 2700:  19.9\n",
      "Average loss at step 2710:  13.7\n",
      "Average loss at step 2720:  24.6\n",
      "Average loss at step 2730:  15.2\n",
      "Average loss at step 2740:  13.2\n",
      "Average loss at step 2750:   8.7\n",
      "Average loss at step 2760:  23.4\n",
      "Average loss at step 2770:  14.8\n",
      "Average loss at step 2780:  26.6\n",
      "Average loss at step 2790:  13.7\n",
      "Average loss at step 2800:  13.8\n",
      "Average loss at step 2810:  21.8\n",
      "Average loss at step 2820:  18.4\n",
      "Average loss at step 2830:   8.6\n",
      "Average loss at step 2840:   6.5\n",
      "Average loss at step 2850:   1.4\n",
      "Average loss at step 2860:  16.0\n",
      "Average loss at step 2870:   9.4\n",
      "Average loss at step 2880:  11.4\n",
      "Average loss at step 2890:   6.8\n",
      "Average loss at step 2900:  12.8\n",
      "Average loss at step 2910:  16.7\n",
      "Average loss at step 2920:  12.0\n",
      "Average loss at step 2930:  15.8\n",
      "Average loss at step 2940:   6.3\n",
      "Average loss at step 2950:  15.6\n",
      "Average loss at step 2960:  10.1\n",
      "Average loss at step 2970:   9.2\n",
      "Average loss at step 2980:  15.3\n",
      "Average loss at step 2990:  16.1\n",
      "Average loss at step 3000:  13.7\n",
      "Average loss at step 3010:  16.9\n",
      "Average loss at step 3020:  11.9\n",
      "Average loss at step 3030:  14.9\n",
      "Average loss at step 3040:  18.5\n",
      "Average loss at step 3050:  16.6\n",
      "Average loss at step 3060:  26.0\n",
      "Average loss at step 3070:  12.2\n",
      "Average loss at step 3080:  21.4\n",
      "Average loss at step 3090:  13.9\n",
      "Average loss at step 3100:  11.6\n",
      "Average loss at step 3110:  16.5\n",
      "Average loss at step 3120:   2.9\n",
      "Average loss at step 3130:  10.9\n",
      "Average loss at step 3140:  19.5\n",
      "Average loss at step 3150:   9.8\n",
      "Average loss at step 3160:   8.3\n",
      "Average loss at step 3170:  20.1\n",
      "Average loss at step 3180:  13.0\n",
      "Average loss at step 3190:  22.7\n",
      "Average loss at step 3200:  12.6\n",
      "Average loss at step 3210:  14.5\n",
      "Average loss at step 3220:   8.1\n",
      "Average loss at step 3230:  13.8\n",
      "Average loss at step 3240:  10.4\n",
      "Average loss at step 3250:  18.0\n",
      "Average loss at step 3260:  20.3\n",
      "Average loss at step 3270:  12.7\n",
      "Average loss at step 3280:  22.6\n",
      "Average loss at step 3290:  13.3\n",
      "Average loss at step 3300:  14.4\n",
      "Average loss at step 3310:  11.0\n",
      "Average loss at step 3320:  20.5\n",
      "Average loss at step 3330:  19.2\n",
      "Average loss at step 3340:  16.3\n",
      "Average loss at step 3350:  17.1\n",
      "Average loss at step 3360:  13.0\n",
      "Average loss at step 3370:   7.8\n",
      "Average loss at step 3380:   6.8\n",
      "Average loss at step 3390:  10.0\n",
      "Average loss at step 3400:  10.6\n",
      "Average loss at step 3410:   5.2\n",
      "Average loss at step 3420:   5.9\n",
      "Average loss at step 3430:   1.4\n",
      "Average loss at step 3440:   8.5\n",
      "Average loss at step 3450:   4.4\n",
      "Average loss at step 3460:  10.0\n",
      "Average loss at step 3470:  11.9\n",
      "Average loss at step 3480:   9.0\n",
      "Average loss at step 3490:   3.0\n",
      "Average loss at step 3500:   4.8\n",
      "Average loss at step 3510:   1.6\n",
      "Average loss at step 3520:   2.6\n",
      "Average loss at step 3530:   3.4\n",
      "Average loss at step 3540:   4.4\n",
      "Average loss at step 3550:   4.7\n",
      "Average loss at step 3560:   4.2\n",
      "Average loss at step 3570:   5.3\n",
      "Average loss at step 3580:   9.3\n",
      "Average loss at step 3590:   3.7\n",
      "Average loss at step 3600:   7.4\n",
      "Average loss at step 3610:   4.9\n",
      "Average loss at step 3620:   6.5\n",
      "Average loss at step 3630:   7.3\n",
      "Average loss at step 3640:   7.0\n",
      "Average loss at step 3650:   3.7\n",
      "Average loss at step 3660:  12.1\n",
      "Average loss at step 3670:  11.7\n",
      "Average loss at step 3680:   4.6\n",
      "Average loss at step 3690:   7.3\n",
      "Average loss at step 3700:  10.0\n",
      "Average loss at step 3710:   4.2\n",
      "Average loss at step 3720:   9.3\n",
      "Average loss at step 3730:  10.0\n",
      "Average loss at step 3740:   7.5\n",
      "Average loss at step 3750:   4.2\n",
      "Average loss at step 3760:   4.8\n",
      "Average loss at step 3770:  13.8\n",
      "Average loss at step 3780:   8.3\n",
      "Average loss at step 3790:   5.8\n",
      "Average loss at step 3800:   5.2\n",
      "Average loss at step 3810:   5.3\n",
      "Average loss at step 3820:   8.4\n",
      "Average loss at step 3830:   8.5\n",
      "Average loss at step 3840:   5.3\n",
      "Average loss at step 3850:   5.9\n",
      "Average loss at step 3860:  11.4\n",
      "Average loss at step 3870:   6.8\n",
      "Average loss at step 3880:  15.3\n",
      "Average loss at step 3890:   9.1\n",
      "Average loss at step 3900:   5.7\n",
      "Average loss at step 3910:   6.1\n",
      "Average loss at step 3920:   3.9\n",
      "Average loss at step 3930:   5.9\n",
      "Average loss at step 3940:   5.0\n",
      "Average loss at step 3950:   9.5\n",
      "Average loss at step 3960:   4.4\n",
      "Average loss at step 3970:   9.6\n",
      "Average loss at step 3980:  13.1\n",
      "Average loss at step 3990:   6.0\n",
      "Average loss at step 4000:   5.9\n",
      "Average loss at step 4010:   4.6\n",
      "Average loss at step 4020:  13.6\n",
      "Average loss at step 4030:   6.7\n",
      "Average loss at step 4040:   4.0\n",
      "Average loss at step 4050:  10.0\n",
      "Average loss at step 4060:   5.7\n",
      "Average loss at step 4070:   8.7\n",
      "Average loss at step 4080:  10.0\n",
      "Average loss at step 4090:   6.8\n",
      "Average loss at step 4100:  14.3\n",
      "Average loss at step 4110:   7.8\n",
      "Average loss at step 4120:  16.0\n",
      "Average loss at step 4130:  13.7\n",
      "Average loss at step 4140:   3.9\n",
      "Average loss at step 4150:  10.4\n",
      "Average loss at step 4160:   6.7\n",
      "Average loss at step 4170:  10.7\n",
      "Average loss at step 4180:  11.6\n",
      "Average loss at step 4190:   4.6\n",
      "Average loss at step 4200:  11.8\n",
      "Average loss at step 4210:  10.8\n",
      "Average loss at step 4220:  10.9\n",
      "Average loss at step 4230:  12.7\n",
      "Average loss at step 4240:   9.0\n",
      "Average loss at step 4250:   8.5\n",
      "Average loss at step 4260:   9.6\n",
      "Average loss at step 4270:   4.2\n",
      "Average loss at step 4280:   9.8\n",
      "Average loss at step 4290:   4.7\n",
      "Average loss at step 4300:   2.4\n",
      "Average loss at step 4310:   5.8\n",
      "Average loss at step 4320:   6.7\n",
      "Average loss at step 4330:   5.4\n",
      "Average loss at step 4340:   3.4\n",
      "Average loss at step 4350:  14.6\n",
      "Average loss at step 4360:   9.2\n",
      "Average loss at step 4370:   7.6\n",
      "Average loss at step 4380:  12.5\n",
      "Average loss at step 4390:  15.6\n",
      "Average loss at step 4400:   7.2\n",
      "Average loss at step 4410:   7.1\n",
      "Average loss at step 4420:   4.3\n",
      "Average loss at step 4430:  13.9\n",
      "Average loss at step 4440:  12.6\n",
      "Average loss at step 4450:   3.4\n",
      "Average loss at step 4460:  12.4\n",
      "Average loss at step 4470:  10.3\n",
      "Average loss at step 4480:  10.0\n",
      "Average loss at step 4490:  13.5\n",
      "Average loss at step 4500:   5.1\n",
      "Average loss at step 4510:   5.8\n",
      "Average loss at step 4520:   8.5\n",
      "Average loss at step 4530:  11.9\n",
      "Average loss at step 4540:   2.3\n",
      "Average loss at step 4550:   8.3\n",
      "Average loss at step 4560:   9.3\n",
      "Average loss at step 4570:  11.4\n",
      "Average loss at step 4580:   4.1\n",
      "Average loss at step 4590:   4.9\n",
      "Average loss at step 4600:   2.9\n",
      "Average loss at step 4610:   5.3\n",
      "Average loss at step 4620:   7.6\n",
      "Average loss at step 4630:   6.5\n",
      "Average loss at step 4640:   3.5\n",
      "Average loss at step 4650:   6.0\n",
      "Average loss at step 4660:   6.9\n",
      "Average loss at step 4670:   6.9\n",
      "Average loss at step 4680:  19.4\n",
      "Average loss at step 4690:   5.7\n",
      "Average loss at step 4700:  14.1\n",
      "Average loss at step 4710:  12.3\n",
      "Average loss at step 4720:  12.0\n",
      "Average loss at step 4730:   7.9\n",
      "Average loss at step 4740:  21.7\n",
      "Average loss at step 4750:  13.6\n",
      "Average loss at step 4760:   3.7\n",
      "Average loss at step 4770:   8.8\n",
      "Average loss at step 4780:   7.1\n",
      "Average loss at step 4790:  12.0\n",
      "Average loss at step 4800:  10.2\n",
      "Average loss at step 4810:   6.1\n",
      "Average loss at step 4820:   9.5\n",
      "Average loss at step 4830:  10.8\n",
      "Average loss at step 4840:  15.3\n",
      "Average loss at step 4850:   5.0\n",
      "Average loss at step 4860:  12.8\n",
      "Average loss at step 4870:  16.3\n",
      "Average loss at step 4880:  12.7\n",
      "Average loss at step 4890:  10.0\n",
      "Average loss at step 4900:   9.0\n",
      "Average loss at step 4910:  11.9\n",
      "Average loss at step 4920:   7.8\n",
      "Average loss at step 4930:   8.5\n",
      "Average loss at step 4940:   7.5\n",
      "Average loss at step 4950:   6.0\n",
      "Average loss at step 4960:   4.2\n",
      "Average loss at step 4970:   5.5\n",
      "Average loss at step 4980:   6.8\n",
      "Average loss at step 4990:   6.4\n",
      "Average loss at step 5000:   5.6\n",
      "Average loss at step 5010:  10.7\n",
      "Average loss at step 5020:   8.1\n",
      "Average loss at step 5030:  10.1\n",
      "Average loss at step 5040:   8.2\n",
      "Average loss at step 5050:  15.2\n",
      "Average loss at step 5060:  11.9\n",
      "Average loss at step 5070:  13.0\n",
      "Average loss at step 5080:  11.1\n",
      "Average loss at step 5090:   7.2\n",
      "Average loss at step 5100:   9.2\n",
      "Average loss at step 5110:   8.0\n",
      "Average loss at step 5120:  11.0\n",
      "Average loss at step 5130:   4.9\n",
      "Average loss at step 5140:  11.1\n",
      "Average loss at step 5150:   2.9\n",
      "Average loss at step 5160:   9.1\n",
      "Average loss at step 5170:   2.0\n",
      "Average loss at step 5180:   3.1\n",
      "Average loss at step 5190:   2.4\n",
      "Average loss at step 5200:   3.7\n",
      "Average loss at step 5210:   0.9\n",
      "Average loss at step 5220:   4.8\n",
      "Average loss at step 5230:   6.9\n",
      "Average loss at step 5240:   5.1\n",
      "Average loss at step 5250:   5.7\n",
      "Average loss at step 5260:   4.3\n",
      "Average loss at step 5270:   3.0\n",
      "Average loss at step 5280:   6.7\n",
      "Average loss at step 5290:   7.7\n",
      "Average loss at step 5300:   4.8\n",
      "Average loss at step 5310:   5.6\n",
      "Average loss at step 5320:   2.3\n",
      "Average loss at step 5330:   4.1\n",
      "Average loss at step 5340:   4.4\n",
      "Average loss at step 5350:  10.2\n",
      "Average loss at step 5360:   6.1\n",
      "Average loss at step 5370:  11.5\n",
      "Average loss at step 5380:   5.0\n",
      "Average loss at step 5390:   5.0\n",
      "Average loss at step 5400:   5.4\n",
      "Average loss at step 5410:  12.9\n",
      "Average loss at step 5420:   4.6\n",
      "Average loss at step 5430:   8.4\n",
      "Average loss at step 5440:   6.6\n",
      "Average loss at step 5450:   4.9\n",
      "Average loss at step 5460:   9.3\n",
      "Average loss at step 5470:   2.7\n",
      "Average loss at step 5480:   9.3\n",
      "Average loss at step 5490:   9.3\n",
      "Average loss at step 5500:   0.8\n",
      "Average loss at step 5510:   4.5\n",
      "Average loss at step 5520:   7.1\n",
      "Average loss at step 5530:   6.0\n",
      "Average loss at step 5540:   7.5\n",
      "Average loss at step 5550:   7.7\n",
      "Average loss at step 5560:   4.2\n",
      "Average loss at step 5570:   2.5\n",
      "Average loss at step 5580:   6.7\n",
      "Average loss at step 5590:   5.5\n",
      "Average loss at step 5600:   3.6\n",
      "Average loss at step 5610:   7.4\n",
      "Average loss at step 5620:  10.6\n",
      "Average loss at step 5630:   3.8\n",
      "Average loss at step 5640:   2.6\n",
      "Average loss at step 5650:   6.0\n",
      "Average loss at step 5660:   3.0\n",
      "Average loss at step 5670:   3.1\n",
      "Average loss at step 5680:   5.1\n",
      "Average loss at step 5690:   7.0\n",
      "Average loss at step 5700:   5.8\n",
      "Average loss at step 5710:   4.4\n",
      "Average loss at step 5720:   8.4\n",
      "Average loss at step 5730:  11.9\n",
      "Average loss at step 5740:   6.2\n",
      "Average loss at step 5750:   3.8\n",
      "Average loss at step 5760:   6.4\n",
      "Average loss at step 5770:   6.0\n",
      "Average loss at step 5780:   9.7\n",
      "Average loss at step 5790:   7.3\n",
      "Average loss at step 5800:   5.0\n",
      "Average loss at step 5810:  10.6\n",
      "Average loss at step 5820:   3.8\n",
      "Average loss at step 5830:   9.4\n",
      "Average loss at step 5840:   7.3\n",
      "Average loss at step 5850:   5.8\n",
      "Average loss at step 5860:   4.8\n",
      "Average loss at step 5870:   7.1\n",
      "Average loss at step 5880:   2.2\n",
      "Average loss at step 5890:   6.3\n",
      "Average loss at step 5900:   5.2\n",
      "Average loss at step 5910:   4.2\n",
      "Average loss at step 5920:   6.9\n",
      "Average loss at step 5930:   3.9\n",
      "Average loss at step 5940:   4.2\n",
      "Average loss at step 5950:   4.9\n",
      "Average loss at step 5960:   7.5\n",
      "Average loss at step 5970:  11.0\n",
      "Average loss at step 5980:   3.5\n",
      "Average loss at step 5990:   5.1\n",
      "Average loss at step 6000:   5.6\n",
      "Average loss at step 6010:   3.0\n",
      "Average loss at step 6020:   5.9\n",
      "Average loss at step 6030:   2.7\n",
      "Average loss at step 6040:   1.2\n",
      "Average loss at step 6050:   3.0\n",
      "Average loss at step 6060:   2.2\n",
      "Average loss at step 6070:   7.2\n",
      "Average loss at step 6080:   4.5\n",
      "Average loss at step 6090:   5.7\n",
      "Average loss at step 6100:   1.6\n",
      "Average loss at step 6110:   5.8\n",
      "Average loss at step 6120:   4.8\n",
      "Average loss at step 6130:   7.0\n",
      "Average loss at step 6140:   5.8\n",
      "Average loss at step 6150:   3.2\n",
      "Average loss at step 6160:   4.1\n",
      "Average loss at step 6170:   5.1\n",
      "Average loss at step 6180:  10.0\n",
      "Average loss at step 6190:   4.0\n",
      "Average loss at step 6200:   6.7\n",
      "Average loss at step 6210:   3.4\n",
      "Average loss at step 6220:   3.2\n",
      "Average loss at step 6230:   0.8\n",
      "Average loss at step 6240:   4.6\n",
      "Average loss at step 6250:   7.6\n",
      "Average loss at step 6260:   1.6\n",
      "Average loss at step 6270:  11.9\n",
      "Average loss at step 6280:   7.9\n",
      "Average loss at step 6290:   3.9\n",
      "Average loss at step 6300:   2.3\n",
      "Average loss at step 6310:   4.6\n",
      "Average loss at step 6320:   5.7\n",
      "Average loss at step 6330:   2.6\n",
      "Average loss at step 6340:   3.3\n",
      "Average loss at step 6350:   1.8\n",
      "Average loss at step 6360:   3.6\n",
      "Average loss at step 6370:   6.4\n",
      "Average loss at step 6380:   4.0\n",
      "Average loss at step 6390:   7.7\n",
      "Average loss at step 6400:   4.7\n",
      "Average loss at step 6410:   5.6\n",
      "Average loss at step 6420:   7.1\n",
      "Average loss at step 6430:   1.4\n",
      "Average loss at step 6440:  10.2\n",
      "Average loss at step 6450:   4.7\n",
      "Average loss at step 6460:   1.9\n",
      "Average loss at step 6470:   2.6\n",
      "Average loss at step 6480:   5.7\n",
      "Average loss at step 6490:   3.9\n",
      "Average loss at step 6500:   2.9\n",
      "Average loss at step 6510:   6.5\n",
      "Average loss at step 6520:   2.7\n",
      "Average loss at step 6530:   2.7\n",
      "Average loss at step 6540:   3.3\n",
      "Average loss at step 6550:   4.0\n",
      "Average loss at step 6560:   1.8\n",
      "Average loss at step 6570:   3.0\n",
      "Average loss at step 6580:   4.1\n",
      "Average loss at step 6590:   5.6\n",
      "Average loss at step 6600:   8.5\n",
      "Average loss at step 6610:   3.3\n",
      "Average loss at step 6620:   5.4\n",
      "Average loss at step 6630:   3.4\n",
      "Average loss at step 6640:   8.5\n",
      "Average loss at step 6650:   8.1\n",
      "Average loss at step 6660:   4.7\n",
      "Average loss at step 6670:   4.9\n",
      "Average loss at step 6680:   5.2\n",
      "Average loss at step 6690:   2.7\n",
      "Average loss at step 6700:   2.8\n",
      "Average loss at step 6710:   5.1\n",
      "Average loss at step 6720:   3.3\n",
      "Average loss at step 6730:   5.1\n",
      "Average loss at step 6740:   3.9\n",
      "Average loss at step 6750:   2.1\n",
      "Average loss at step 6760:   7.6\n",
      "Average loss at step 6770:   4.5\n",
      "Average loss at step 6780:   6.8\n",
      "Average loss at step 6790:   4.7\n",
      "Average loss at step 6800:   4.2\n",
      "Average loss at step 6810:   4.2\n",
      "Average loss at step 6820:   5.3\n",
      "Average loss at step 6830:   5.2\n",
      "Average loss at step 6840:   6.7\n",
      "Average loss at step 6850:   1.9\n",
      "Average loss at step 6860:   2.8\n",
      "Average loss at step 6870:   3.6\n",
      "Average loss at step 6880:   3.1\n",
      "Average loss at step 6890:   4.1\n",
      "Average loss at step 6900:   2.3\n",
      "Average loss at step 6910:   3.6\n",
      "Average loss at step 6920:   3.1\n",
      "Average loss at step 6930:   2.6\n",
      "Average loss at step 6940:   1.7\n",
      "Average loss at step 6950:   5.3\n",
      "Average loss at step 6960:   3.3\n",
      "Average loss at step 6970:   3.2\n",
      "Average loss at step 6980:   6.6\n",
      "Average loss at step 6990:   2.6\n",
      "Average loss at step 7000:   6.0\n",
      "Average loss at step 7010:   3.8\n",
      "Average loss at step 7020:   2.8\n",
      "Average loss at step 7030:   3.4\n",
      "Average loss at step 7040:   3.6\n",
      "Average loss at step 7050:   1.9\n",
      "Average loss at step 7060:   3.9\n",
      "Average loss at step 7070:   2.9\n",
      "Average loss at step 7080:   2.7\n",
      "Average loss at step 7090:   3.6\n",
      "Average loss at step 7100:   7.2\n",
      "Average loss at step 7110:   4.7\n",
      "Average loss at step 7120:   1.7\n",
      "Average loss at step 7130:   4.1\n",
      "Average loss at step 7140:   2.7\n",
      "Average loss at step 7150:   5.4\n",
      "Average loss at step 7160:   3.0\n",
      "Average loss at step 7170:   6.3\n",
      "Average loss at step 7180:   1.8\n",
      "Average loss at step 7190:   2.6\n",
      "Average loss at step 7200:   2.4\n",
      "Average loss at step 7210:   9.5\n",
      "Average loss at step 7220:   4.6\n",
      "Average loss at step 7230:   5.9\n",
      "Average loss at step 7240:   2.4\n",
      "Average loss at step 7250:   5.0\n",
      "Average loss at step 7260:   4.7\n",
      "Average loss at step 7270:   2.6\n",
      "Average loss at step 7280:   5.8\n",
      "Average loss at step 7290:   2.2\n",
      "Average loss at step 7300:   2.5\n",
      "Average loss at step 7310:   4.8\n",
      "Average loss at step 7320:   2.4\n",
      "Average loss at step 7330:   3.8\n",
      "Average loss at step 7340:   5.9\n",
      "Average loss at step 7350:   2.8\n",
      "Average loss at step 7360:   4.9\n",
      "Average loss at step 7370:   3.5\n",
      "Average loss at step 7380:   2.8\n",
      "Average loss at step 7390:   6.3\n",
      "Average loss at step 7400:   2.2\n",
      "Average loss at step 7410:   7.0\n",
      "Average loss at step 7420:   6.3\n",
      "Average loss at step 7430:   7.5\n",
      "Average loss at step 7440:   4.3\n",
      "Average loss at step 7450:   8.1\n",
      "Average loss at step 7460:   8.1\n",
      "Average loss at step 7470:   5.5\n",
      "Average loss at step 7480:   4.3\n",
      "Average loss at step 7490:   7.2\n",
      "Average loss at step 7500:   3.5\n",
      "Average loss at step 7510:   7.5\n",
      "Average loss at step 7520:   2.6\n",
      "Average loss at step 7530:   6.4\n",
      "Average loss at step 7540:   2.8\n",
      "Average loss at step 7550:   5.7\n",
      "Average loss at step 7560:   7.0\n",
      "Average loss at step 7570:   5.4\n",
      "Average loss at step 7580:   1.3\n",
      "Average loss at step 7590:   4.6\n",
      "Average loss at step 7600:   3.6\n",
      "Average loss at step 7610:   5.0\n",
      "Average loss at step 7620:   3.5\n",
      "Average loss at step 7630:   4.7\n",
      "Average loss at step 7640:   3.5\n",
      "Average loss at step 7650:   2.1\n",
      "Average loss at step 7660:   3.9\n",
      "Average loss at step 7670:   4.7\n",
      "Average loss at step 7680:   4.2\n",
      "Average loss at step 7690:   3.3\n",
      "Average loss at step 7700:   2.1\n",
      "Average loss at step 7710:   0.6\n",
      "Average loss at step 7720:   3.3\n",
      "Average loss at step 7730:   4.9\n",
      "Average loss at step 7740:   5.1\n",
      "Average loss at step 7750:   5.7\n",
      "Average loss at step 7760:   4.5\n",
      "Average loss at step 7770:   5.2\n",
      "Average loss at step 7780:   3.4\n",
      "Average loss at step 7790:   3.5\n",
      "Average loss at step 7800:   5.4\n",
      "Average loss at step 7810:   5.2\n",
      "Average loss at step 7820:   2.1\n",
      "Average loss at step 7830:   5.7\n",
      "Average loss at step 7840:   2.5\n",
      "Average loss at step 7850:   1.6\n",
      "Average loss at step 7860:   1.6\n",
      "Average loss at step 7870:   2.4\n",
      "Average loss at step 7880:   2.7\n",
      "Average loss at step 7890:   2.1\n",
      "Average loss at step 7900:   3.6\n",
      "Average loss at step 7910:   3.4\n",
      "Average loss at step 7920:   1.9\n",
      "Average loss at step 7930:   2.7\n",
      "Average loss at step 7940:   2.5\n",
      "Average loss at step 7950:   4.3\n",
      "Average loss at step 7960:   4.6\n",
      "Average loss at step 7970:   2.3\n",
      "Average loss at step 7980:   2.2\n",
      "Average loss at step 7990:   3.6\n",
      "Average loss at step 8000:   4.2\n",
      "Average loss at step 8010:   4.8\n",
      "Average loss at step 8020:   2.9\n",
      "Average loss at step 8030:   5.1\n",
      "Average loss at step 8040:   4.3\n",
      "Average loss at step 8050:   4.8\n",
      "Average loss at step 8060:   4.2\n",
      "Average loss at step 8070:   6.4\n",
      "Average loss at step 8080:   4.9\n",
      "Average loss at step 8090:   9.1\n",
      "Average loss at step 8100:   6.4\n",
      "Average loss at step 8110:   3.7\n",
      "Average loss at step 8120:   5.0\n",
      "Average loss at step 8130:   0.9\n",
      "Average loss at step 8140:   1.5\n",
      "Average loss at step 8150:   1.6\n",
      "Average loss at step 8160:   2.4\n",
      "Average loss at step 8170:   2.4\n",
      "Average loss at step 8180:   5.9\n",
      "Average loss at step 8190:   4.2\n",
      "Average loss at step 8200:  10.4\n",
      "Average loss at step 8210:   6.2\n",
      "Average loss at step 8220:   2.8\n",
      "Average loss at step 8230:   2.1\n",
      "Average loss at step 8240:   3.8\n",
      "Average loss at step 8250:   4.8\n",
      "Average loss at step 8260:   1.3\n",
      "Average loss at step 8270:   6.1\n",
      "Average loss at step 8280:   5.1\n",
      "Average loss at step 8290:   3.9\n",
      "Average loss at step 8300:   7.3\n",
      "Average loss at step 8310:   2.8\n",
      "Average loss at step 8320:   5.3\n",
      "Average loss at step 8330:   5.4\n",
      "Average loss at step 8340:   2.7\n",
      "Average loss at step 8350:   6.1\n",
      "Average loss at step 8360:   0.8\n",
      "Average loss at step 8370:   4.5\n",
      "Average loss at step 8380:   2.5\n",
      "Average loss at step 8390:   2.8\n",
      "Average loss at step 8400:   2.9\n",
      "Average loss at step 8410:   1.4\n",
      "Average loss at step 8420:   2.0\n",
      "Average loss at step 8430:   2.4\n",
      "Average loss at step 8440:   3.2\n",
      "Average loss at step 8450:   6.4\n",
      "Average loss at step 8460:   4.0\n",
      "Average loss at step 8470:   3.0\n",
      "Average loss at step 8480:   2.8\n",
      "Average loss at step 8490:   4.0\n",
      "Average loss at step 8500:   6.5\n",
      "Average loss at step 8510:   0.5\n",
      "Average loss at step 8520:   2.6\n",
      "Average loss at step 8530:   4.2\n",
      "Average loss at step 8540:   3.8\n",
      "Average loss at step 8550:   6.2\n",
      "Average loss at step 8560:   3.1\n",
      "Average loss at step 8570:   4.5\n",
      "Average loss at step 8580:   4.2\n",
      "Average loss at step 8590:   4.3\n",
      "Optimization Finished!\n",
      "Total time: 1477.5036838054657 seconds\n",
      "Accuracy 0.9672\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "N_CLASSES = 10\n",
    "\n",
    "# Step 2: Define paramaters for the model\n",
    "LEARNING_RATE = 0.01\n",
    "BATCH_SIZE = 64\n",
    "SKIP_STEP = 10\n",
    "DROPOUT = 0.80\n",
    "N_EPOCHS = 10\n",
    "\n",
    "\n",
    "with tf.name_scope('data'):\n",
    "    X = tf.placeholder(tf.float32, [None, 784], name=\"X_placeholder\")\n",
    "    Y = tf.placeholder(tf.float32, [None, 10], name=\"Y_placeholder\")\n",
    "\n",
    "dropout = tf.placeholder(tf.float32, name='dropout')\n",
    "\n",
    "\n",
    "global_step = tf.Variable(0, dtype=tf.int32, trainable=False, name='global_step')\n",
    "\n",
    "\n",
    "\n",
    "with tf.variable_scope('conv1') as scope:\n",
    "    \n",
    "    images = tf.reshape(X, shape=[-1, 28, 28, 1])\n",
    "    \n",
    "    \n",
    "    \n",
    "    kernel = tf.get_variable('kernel', shape=[5, 5, 1, 32], initializer=tf.truncated_normal_initializer())\n",
    "    \n",
    "   \n",
    "    \n",
    "    biases = tf.get_variable('biases', shape=[32], initializer=tf.constant_initializer(0.0))\n",
    "    \n",
    "   \n",
    "    conv = tf.nn.conv2d(images, kernel, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    \n",
    "   \n",
    "    conv1 = tf.nn.relu(conv + biases, name='conv1')\n",
    "    \n",
    "    \n",
    "    \n",
    "with tf.variable_scope('pool1') as scope:\n",
    "   \n",
    "    \n",
    "    pool1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], name='pool1', strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "    \n",
    "\n",
    "with tf.variable_scope('conv2') as scope:\n",
    "\n",
    "    kernel = tf.get_variable('kernels', [5, 5, 32, 64], \n",
    "                        initializer=tf.truncated_normal_initializer())\n",
    "    biases = tf.get_variable('biases', [64],\n",
    "                        initializer=tf.random_normal_initializer())\n",
    "    conv = tf.nn.conv2d(pool1, kernel, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    conv2 = tf.nn.relu(conv + biases, name=scope.name)\n",
    "\n",
    "\n",
    "with tf.variable_scope('pool2') as scope:\n",
    "\n",
    "    pool2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1],\n",
    "                            padding='SAME')\n",
    "\n",
    "   \n",
    "    \n",
    "\n",
    "with tf.variable_scope('fc') as scope:\n",
    "\n",
    "    input_features = 7 * 7 * 64\n",
    "    \n",
    "   \n",
    "    w = tf.get_variable('weights', shape=[input_features, 1024], initializer=tf.truncated_normal_initializer())\n",
    "    b = tf.get_variable('bias', shape=[1024], initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "    pool2 = tf.reshape(pool2, [-1, input_features])\n",
    "\n",
    "    fc = tf.nn.relu(tf.matmul(pool2, w) + b, name='relu')\n",
    "    \n",
    "   \n",
    "    fc = tf.nn.dropout(fc, dropout, name='relu_dropout')\n",
    "\n",
    "with tf.variable_scope('softmax_linear') as scope:\n",
    "    \n",
    "    w = tf.get_variable('weights', shape=[1024, N_CLASSES], initializer=tf.truncated_normal_initializer())\n",
    "    b = tf.get_variable('bias', shape=[N_CLASSES], initializer=tf.constant_initializer(0.0))\n",
    "    \n",
    "    logits = tf.matmul(fc, w) + b\n",
    "\n",
    "\n",
    "    \n",
    "with tf.name_scope('loss'):\n",
    "   \n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=Y, logits=logits), name='loss')\n",
    "\n",
    "\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE).minimize(loss, global_step=global_step)\n",
    "\n",
    "with tf.name_scope('summary'):\n",
    "    tf.summary.scalar('loss', loss)\n",
    "    summary_op = tf.summary.merge_all()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    writer = tf.summary.FileWriter('./my_graph/mnist', sess.graph)\n",
    "   \n",
    "    \n",
    "    initial_step = global_step.eval()\n",
    "\n",
    "    start_time = time.time()\n",
    "    n_batches = int(mnist.train.num_examples / BATCH_SIZE)\n",
    "\n",
    "    total_loss = 0.0\n",
    "    for index in range(initial_step, n_batches * N_EPOCHS): # train the model n_epochs times\n",
    "        X_batch, Y_batch = mnist.train.next_batch(BATCH_SIZE)\n",
    "        _, loss_batch, summary = sess.run([optimizer, loss, summary_op], \n",
    "                                feed_dict={X: X_batch, Y:Y_batch, dropout: DROPOUT}) \n",
    "        total_loss += loss_batch\n",
    "        writer.add_summary(summary, global_step=index)\n",
    "        if (index + 1) % SKIP_STEP == 0:\n",
    "            print('Average loss at step {}: {:5.1f}'.format(index + 1, total_loss / SKIP_STEP))\n",
    "            total_loss = 0.0\n",
    "            saver.save(sess, '../checkpoints/convnet_mnist/mnist-convnet', index)\n",
    "    \n",
    "    print(\"Optimization Finished!\") # should be around 0.35 after 25 epochs\n",
    "    print(\"Total time: {0} seconds\".format(time.time() - start_time))\n",
    "    \n",
    "    # test the model\n",
    "    n_batches = int(mnist.test.num_examples/BATCH_SIZE)\n",
    "    total_correct_preds = 0\n",
    "    for i in range(n_batches):\n",
    "        X_batch, Y_batch = mnist.test.next_batch(BATCH_SIZE)\n",
    "        _, loss_batch, logits_batch = sess.run([optimizer, loss, logits], \n",
    "                                        feed_dict={X: X_batch, Y:Y_batch, dropout: DROPOUT}) \n",
    "        preds = tf.nn.softmax(logits_batch)\n",
    "        correct_preds = tf.equal(tf.argmax(preds, 1), tf.argmax(Y_batch, 1))\n",
    "        accuracy = tf.reduce_sum(tf.cast(correct_preds, tf.float32))\n",
    "        total_correct_preds += sess.run(accuracy)   \n",
    "    \n",
    "    print(\"Accuracy {0}\".format(total_correct_preds/mnist.test.num_examples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (10 epochs) two conv nets then one fc with relu and dropout -> Accuracy 0.9665 - 0.9672 - 0.97"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((55000, 784), (10000, 784))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.images.shape, mnist.test.images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 10: 209184.9\n",
      "Average loss at step 20: 16662.1\n",
      "Average loss at step 30: 3337.3\n",
      "Average loss at step 40: 1497.0\n",
      "Average loss at step 50: 845.5\n",
      "Average loss at step 60: 599.0\n",
      "Average loss at step 70: 460.2\n",
      "Average loss at step 80: 358.0\n",
      "Average loss at step 90: 282.3\n",
      "Average loss at step 100: 249.6\n",
      "Average loss at step 110: 190.0\n",
      "Average loss at step 120: 174.3\n",
      "Average loss at step 130: 144.9\n",
      "Average loss at step 140: 127.8\n",
      "Average loss at step 150: 102.1\n",
      "Average loss at step 160: 120.0\n",
      "Average loss at step 170:  87.0\n",
      "Average loss at step 180:  89.7\n",
      "Average loss at step 190:  78.6\n",
      "Average loss at step 200:  67.3\n",
      "Average loss at step 210:  51.7\n",
      "Average loss at step 220:  55.4\n",
      "Average loss at step 230:  53.6\n",
      "Average loss at step 240:  60.7\n",
      "Average loss at step 250:  51.2\n",
      "Average loss at step 260:  42.5\n",
      "Average loss at step 270:  49.7\n",
      "Average loss at step 280:  42.5\n",
      "Average loss at step 290:  33.2\n",
      "Average loss at step 300:  34.6\n",
      "Average loss at step 310:  34.9\n",
      "Average loss at step 320:  32.5\n",
      "Average loss at step 330:  24.4\n",
      "Average loss at step 340:  23.6\n",
      "Average loss at step 350:  33.5\n",
      "Average loss at step 360:  21.1\n",
      "Average loss at step 370:  31.2\n",
      "Average loss at step 380:  24.8\n",
      "Average loss at step 390:  23.6\n",
      "Average loss at step 400:  21.3\n",
      "Average loss at step 410:  19.5\n",
      "Average loss at step 420:  15.9\n",
      "Average loss at step 430:  16.5\n",
      "Average loss at step 440:  14.2\n",
      "Average loss at step 450:  20.4\n",
      "Average loss at step 460:  21.0\n",
      "Average loss at step 470:  18.7\n",
      "Average loss at step 480:  16.1\n",
      "Average loss at step 490:  12.5\n",
      "Average loss at step 500:  13.1\n",
      "Average loss at step 510:  22.3\n",
      "Average loss at step 520:  11.9\n",
      "Average loss at step 530:  14.6\n",
      "Average loss at step 540:  19.2\n",
      "Average loss at step 550:  12.1\n",
      "Average loss at step 560:  12.7\n",
      "Average loss at step 570:  12.0\n",
      "Average loss at step 580:  10.9\n",
      "Average loss at step 590:  10.3\n",
      "Average loss at step 600:  11.9\n",
      "Average loss at step 610:  13.4\n",
      "Average loss at step 620:  10.4\n",
      "Average loss at step 630:   6.9\n",
      "Average loss at step 640:   9.6\n",
      "Average loss at step 650:  12.4\n",
      "Average loss at step 660:   9.1\n",
      "Average loss at step 670:   6.1\n",
      "Average loss at step 680:  11.1\n",
      "Average loss at step 690:  10.0\n",
      "Average loss at step 700:   9.6\n",
      "Average loss at step 710:  11.1\n",
      "Average loss at step 720:   6.3\n",
      "Average loss at step 730:   8.4\n",
      "Average loss at step 740:   9.7\n",
      "Average loss at step 750:   8.4\n",
      "Average loss at step 760:   7.2\n",
      "Average loss at step 770:   8.7\n",
      "Average loss at step 780:   5.6\n",
      "Average loss at step 790:   4.6\n",
      "Average loss at step 800:   7.9\n",
      "Average loss at step 810:   7.8\n",
      "Average loss at step 820:   6.7\n",
      "Average loss at step 830:   6.0\n",
      "Average loss at step 840:   4.2\n",
      "Average loss at step 850:   9.7\n",
      "Average loss at step 860:   6.8\n",
      "Average loss at step 870:   5.8\n",
      "Average loss at step 880:   7.2\n",
      "Average loss at step 890:   5.9\n",
      "Average loss at step 900:   6.0\n",
      "Average loss at step 910:   7.8\n",
      "Average loss at step 920:   5.6\n",
      "Average loss at step 930:   6.9\n",
      "Average loss at step 940:   5.2\n",
      "Average loss at step 950:   6.2\n",
      "Average loss at step 960:   4.7\n",
      "Average loss at step 970:   7.1\n",
      "Average loss at step 980:   6.3\n",
      "Average loss at step 990:   6.7\n",
      "Average loss at step 1000:   6.9\n",
      "Average loss at step 1010:   4.7\n",
      "Average loss at step 1020:   5.5\n",
      "Average loss at step 1030:   4.8\n",
      "Average loss at step 1040:   5.7\n",
      "Average loss at step 1050:   5.2\n",
      "Average loss at step 1060:   5.1\n",
      "Average loss at step 1070:   6.0\n",
      "Average loss at step 1080:   4.9\n",
      "Average loss at step 1090:   9.6\n",
      "Average loss at step 1100:   5.6\n",
      "Average loss at step 1110:   4.8\n",
      "Average loss at step 1120:   3.7\n",
      "Average loss at step 1130:   4.0\n",
      "Average loss at step 1140:   4.1\n",
      "Average loss at step 1150:   4.2\n",
      "Average loss at step 1160:   4.8\n",
      "Average loss at step 1170:   3.3\n",
      "Average loss at step 1180:   4.8\n",
      "Average loss at step 1190:   5.0\n",
      "Average loss at step 1200:   4.1\n",
      "Average loss at step 1210:   2.9\n",
      "Average loss at step 1220:   4.7\n",
      "Average loss at step 1230:   2.8\n",
      "Average loss at step 1240:   5.6\n",
      "Average loss at step 1250:   4.7\n",
      "Average loss at step 1260:   5.0\n",
      "Average loss at step 1270:   4.8\n",
      "Average loss at step 1280:   3.6\n",
      "Average loss at step 1290:   4.2\n",
      "Average loss at step 1300:   5.2\n",
      "Average loss at step 1310:   4.8\n",
      "Average loss at step 1320:   5.1\n",
      "Average loss at step 1330:   4.0\n",
      "Average loss at step 1340:   3.8\n",
      "Average loss at step 1350:   3.9\n",
      "Average loss at step 1360:   3.1\n",
      "Average loss at step 1370:   5.4\n",
      "Average loss at step 1380:   6.3\n",
      "Average loss at step 1390:   3.8\n",
      "Average loss at step 1400:   6.4\n",
      "Average loss at step 1410:   4.3\n",
      "Average loss at step 1420:   4.0\n",
      "Average loss at step 1430:   3.7\n",
      "Average loss at step 1440:   3.3\n",
      "Average loss at step 1450:   5.1\n",
      "Average loss at step 1460:   4.8\n",
      "Average loss at step 1470:   3.6\n",
      "Average loss at step 1480:   4.5\n",
      "Average loss at step 1490:   5.3\n",
      "Average loss at step 1500:   4.3\n",
      "Average loss at step 1510:   4.0\n",
      "Average loss at step 1520:   4.3\n",
      "Average loss at step 1530:   3.6\n",
      "Average loss at step 1540:   3.6\n",
      "Average loss at step 1550:   3.4\n",
      "Average loss at step 1560:   3.4\n",
      "Average loss at step 1570:   6.1\n",
      "Average loss at step 1580:   2.8\n",
      "Average loss at step 1590:   3.2\n",
      "Average loss at step 1600:   5.7\n",
      "Average loss at step 1610:   3.4\n",
      "Average loss at step 1620:   2.9\n",
      "Average loss at step 1630:   2.8\n",
      "Average loss at step 1640:   3.3\n",
      "Average loss at step 1650:   2.9\n",
      "Average loss at step 1660:   2.8\n",
      "Average loss at step 1670:   3.7\n",
      "Average loss at step 1680:   2.4\n",
      "Average loss at step 1690:   3.0\n",
      "Average loss at step 1700:   2.6\n",
      "Average loss at step 1710:   3.9\n",
      "Average loss at step 1720:   3.1\n",
      "Average loss at step 1730:   3.7\n",
      "Average loss at step 1740:   3.8\n",
      "Average loss at step 1750:   3.4\n",
      "Average loss at step 1760:   4.3\n",
      "Average loss at step 1770:   2.5\n",
      "Average loss at step 1780:   3.4\n",
      "Average loss at step 1790:   2.7\n",
      "Average loss at step 1800:   2.9\n",
      "Average loss at step 1810:   3.8\n",
      "Average loss at step 1820:   2.5\n",
      "Average loss at step 1830:   2.6\n",
      "Average loss at step 1840:   3.6\n",
      "Average loss at step 1850:   3.3\n",
      "Average loss at step 1860:   3.5\n",
      "Average loss at step 1870:   2.8\n",
      "Average loss at step 1880:   3.0\n",
      "Average loss at step 1890:   3.2\n",
      "Average loss at step 1900:   3.1\n",
      "Average loss at step 1910:   2.8\n",
      "Average loss at step 1920:   3.1\n",
      "Average loss at step 1930:   2.6\n",
      "Average loss at step 1940:   2.8\n",
      "Average loss at step 1950:   3.3\n",
      "Average loss at step 1960:   2.5\n",
      "Average loss at step 1970:   2.4\n",
      "Average loss at step 1980:   2.7\n",
      "Average loss at step 1990:   4.0\n",
      "Average loss at step 2000:   3.0\n",
      "Average loss at step 2010:   2.6\n",
      "Average loss at step 2020:   2.9\n",
      "Average loss at step 2030:   3.3\n",
      "Average loss at step 2040:   4.0\n",
      "Average loss at step 2050:   2.9\n",
      "Average loss at step 2060:   2.8\n",
      "Average loss at step 2070:   3.0\n",
      "Average loss at step 2080:   2.7\n",
      "Average loss at step 2090:   4.2\n",
      "Average loss at step 2100:   2.7\n",
      "Average loss at step 2110:   4.3\n",
      "Average loss at step 2120:   2.7\n",
      "Average loss at step 2130:   3.3\n",
      "Average loss at step 2140:   2.9\n",
      "Average loss at step 2150:   2.8\n",
      "Average loss at step 2160:   3.0\n",
      "Average loss at step 2170:   2.8\n",
      "Average loss at step 2180:   3.5\n",
      "Average loss at step 2190:   3.6\n",
      "Average loss at step 2200:   3.1\n",
      "Average loss at step 2210:   2.9\n",
      "Average loss at step 2220:   3.0\n",
      "Average loss at step 2230:   2.8\n",
      "Average loss at step 2240:   2.7\n",
      "Average loss at step 2250:   2.8\n",
      "Average loss at step 2260:   3.1\n",
      "Average loss at step 2270:   2.7\n",
      "Average loss at step 2280:   3.0\n",
      "Average loss at step 2290:   3.0\n",
      "Average loss at step 2300:   2.6\n",
      "Average loss at step 2310:   4.0\n",
      "Average loss at step 2320:   3.2\n",
      "Average loss at step 2330:   3.0\n",
      "Average loss at step 2340:   4.2\n",
      "Average loss at step 2350:   2.7\n",
      "Average loss at step 2360:   3.1\n",
      "Average loss at step 2370:   2.7\n",
      "Average loss at step 2380:   2.5\n",
      "Average loss at step 2390:   2.3\n",
      "Average loss at step 2400:   2.8\n",
      "Average loss at step 2410:   3.2\n",
      "Average loss at step 2420:   2.8\n",
      "Average loss at step 2430:   3.3\n",
      "Average loss at step 2440:   2.4\n",
      "Average loss at step 2450:   3.2\n",
      "Average loss at step 2460:   4.1\n",
      "Average loss at step 2470:   2.4\n",
      "Average loss at step 2480:   3.1\n",
      "Average loss at step 2490:   2.6\n",
      "Average loss at step 2500:   2.3\n",
      "Average loss at step 2510:   3.0\n",
      "Average loss at step 2520:   3.1\n",
      "Average loss at step 2530:   2.9\n",
      "Average loss at step 2540:   2.5\n",
      "Average loss at step 2550:   3.0\n",
      "Average loss at step 2560:   2.6\n",
      "Average loss at step 2570:   2.3\n",
      "Average loss at step 2580:   2.8\n",
      "Average loss at step 2590:   2.9\n",
      "Average loss at step 2600:   3.2\n",
      "Average loss at step 2610:   2.5\n",
      "Average loss at step 2620:   2.3\n",
      "Average loss at step 2630:   2.4\n",
      "Average loss at step 2640:   2.6\n",
      "Average loss at step 2650:   3.3\n",
      "Average loss at step 2660:   3.6\n",
      "Average loss at step 2670:   3.0\n",
      "Average loss at step 2680:   2.4\n",
      "Average loss at step 2690:   3.1\n",
      "Average loss at step 2700:   2.9\n",
      "Average loss at step 2710:   2.5\n",
      "Average loss at step 2720:   3.0\n",
      "Average loss at step 2730:   2.5\n",
      "Average loss at step 2740:   2.4\n",
      "Average loss at step 2750:   2.9\n",
      "Average loss at step 2760:   2.9\n",
      "Average loss at step 2770:   3.2\n",
      "Average loss at step 2780:   2.5\n",
      "Average loss at step 2790:   2.3\n",
      "Average loss at step 2800:   3.0\n",
      "Average loss at step 2810:   3.2\n",
      "Average loss at step 2820:   3.0\n",
      "Average loss at step 2830:   2.8\n",
      "Average loss at step 2840:   2.6\n",
      "Average loss at step 2850:   2.5\n",
      "Average loss at step 2860:   2.4\n",
      "Average loss at step 2870:   2.3\n",
      "Average loss at step 2880:   2.3\n",
      "Average loss at step 2890:   2.6\n",
      "Average loss at step 2900:   2.4\n",
      "Average loss at step 2910:   2.3\n",
      "Average loss at step 2920:   2.8\n",
      "Average loss at step 2930:   3.4\n",
      "Average loss at step 2940:   2.6\n",
      "Average loss at step 2950:   2.3\n",
      "Average loss at step 2960:   4.0\n",
      "Average loss at step 2970:   2.7\n",
      "Average loss at step 2980:   2.3\n",
      "Average loss at step 2990:   3.2\n",
      "Average loss at step 3000:   2.3\n",
      "Average loss at step 3010:   2.5\n",
      "Average loss at step 3020:   3.1\n",
      "Average loss at step 3030:   2.3\n",
      "Average loss at step 3040:   2.6\n",
      "Average loss at step 3050:   2.6\n",
      "Average loss at step 3060:   2.3\n",
      "Average loss at step 3070:   2.5\n",
      "Average loss at step 3080:   2.6\n",
      "Average loss at step 3090:   3.2\n",
      "Average loss at step 3100:   2.3\n",
      "Average loss at step 3110:   2.9\n",
      "Average loss at step 3120:   2.4\n",
      "Average loss at step 3130:   2.6\n",
      "Average loss at step 3140:   3.0\n",
      "Average loss at step 3150:   2.3\n",
      "Average loss at step 3160:   2.9\n",
      "Average loss at step 3170:   2.4\n",
      "Average loss at step 3180:   2.9\n",
      "Average loss at step 3190:   2.4\n",
      "Average loss at step 3200:   2.7\n",
      "Average loss at step 3210:   3.0\n",
      "Average loss at step 3220:   2.3\n",
      "Average loss at step 3230:   2.8\n",
      "Average loss at step 3240:   3.0\n",
      "Average loss at step 3250:   2.3\n",
      "Average loss at step 3260:   2.3\n",
      "Average loss at step 3270:   2.5\n",
      "Average loss at step 3280:   2.4\n",
      "Average loss at step 3290:   3.0\n",
      "Average loss at step 3300:   2.3\n",
      "Average loss at step 3310:   2.3\n",
      "Average loss at step 3320:   2.4\n",
      "Average loss at step 3330:   2.5\n",
      "Average loss at step 3340:   2.3\n",
      "Average loss at step 3350:   3.1\n",
      "Average loss at step 3360:   2.7\n",
      "Average loss at step 3370:   3.1\n",
      "Average loss at step 3380:   2.4\n",
      "Average loss at step 3390:   2.9\n",
      "Average loss at step 3400:   3.6\n",
      "Average loss at step 3410:   3.3\n",
      "Average loss at step 3420:   2.8\n",
      "Average loss at step 3430:   2.5\n",
      "Average loss at step 3440:   2.3\n",
      "Average loss at step 3450:   2.4\n",
      "Average loss at step 3460:   2.3\n",
      "Average loss at step 3470:   2.4\n",
      "Average loss at step 3480:   2.8\n",
      "Average loss at step 3490:   2.7\n",
      "Average loss at step 3500:   2.3\n",
      "Average loss at step 3510:   2.3\n",
      "Average loss at step 3520:   2.4\n",
      "Average loss at step 3530:   2.5\n",
      "Average loss at step 3540:   2.4\n",
      "Average loss at step 3550:   3.3\n",
      "Average loss at step 3560:   2.3\n",
      "Average loss at step 3570:   2.3\n",
      "Average loss at step 3580:   3.4\n",
      "Average loss at step 3590:   2.3\n",
      "Average loss at step 3600:   2.3\n",
      "Average loss at step 3610:   3.0\n",
      "Average loss at step 3620:   2.3\n",
      "Average loss at step 3630:   2.9\n",
      "Average loss at step 3640:   2.9\n",
      "Average loss at step 3650:   2.3\n",
      "Average loss at step 3660:   2.3\n",
      "Average loss at step 3670:   2.9\n",
      "Average loss at step 3680:   2.3\n",
      "Average loss at step 3690:   2.4\n",
      "Average loss at step 3700:   2.3\n",
      "Average loss at step 3710:   2.5\n",
      "Average loss at step 3720:   4.2\n",
      "Average loss at step 3730:   2.3\n",
      "Average loss at step 3740:   2.5\n",
      "Average loss at step 3750:   2.3\n",
      "Average loss at step 3760:   2.3\n",
      "Average loss at step 3770:   2.5\n",
      "Average loss at step 3780:   2.5\n",
      "Average loss at step 3790:   2.8\n",
      "Average loss at step 3800:   2.8\n",
      "Average loss at step 3810:   2.3\n",
      "Average loss at step 3820:   2.3\n",
      "Average loss at step 3830:   2.3\n",
      "Average loss at step 3840:   2.9\n",
      "Average loss at step 3850:   2.6\n",
      "Average loss at step 3860:   2.4\n",
      "Average loss at step 3870:   2.3\n",
      "Average loss at step 3880:   2.3\n",
      "Average loss at step 3890:   2.4\n",
      "Average loss at step 3900:   2.3\n",
      "Average loss at step 3910:   2.4\n",
      "Average loss at step 3920:   2.9\n",
      "Average loss at step 3930:   2.3\n",
      "Average loss at step 3940:   2.9\n",
      "Average loss at step 3950:   2.3\n",
      "Average loss at step 3960:   2.9\n",
      "Average loss at step 3970:   2.6\n",
      "Average loss at step 3980:   2.3\n",
      "Average loss at step 3990:   2.3\n",
      "Average loss at step 4000:   2.4\n",
      "Average loss at step 4010:   2.5\n",
      "Average loss at step 4020:   2.7\n",
      "Average loss at step 4030:   2.4\n",
      "Average loss at step 4040:   2.4\n",
      "Average loss at step 4050:   2.5\n",
      "Average loss at step 4060:   2.3\n",
      "Average loss at step 4070:   2.4\n",
      "Average loss at step 4080:   2.3\n",
      "Average loss at step 4090:   2.4\n",
      "Average loss at step 4100:   2.6\n",
      "Average loss at step 4110:   2.7\n",
      "Average loss at step 4120:   2.4\n",
      "Average loss at step 4130:   2.8\n",
      "Average loss at step 4140:   2.3\n",
      "Average loss at step 4150:   3.0\n",
      "Average loss at step 4160:   2.3\n",
      "Average loss at step 4170:   2.5\n",
      "Average loss at step 4180:   2.3\n",
      "Average loss at step 4190:   2.3\n",
      "Average loss at step 4200:   2.5\n",
      "Average loss at step 4210:   2.4\n",
      "Average loss at step 4220:   2.3\n",
      "Average loss at step 4230:   2.3\n",
      "Average loss at step 4240:   2.9\n",
      "Average loss at step 4250:   2.6\n",
      "Average loss at step 4260:   2.6\n",
      "Average loss at step 4270:   2.3\n",
      "Average loss at step 4280:   2.3\n",
      "Average loss at step 4290:   2.5\n",
      "Average loss at step 4300:   2.7\n",
      "Average loss at step 4310:   2.7\n",
      "Average loss at step 4320:   2.9\n",
      "Average loss at step 4330:   2.5\n",
      "Average loss at step 4340:   2.5\n",
      "Average loss at step 4350:   2.4\n",
      "Average loss at step 4360:   2.3\n",
      "Average loss at step 4370:   3.5\n",
      "Average loss at step 4380:   2.4\n",
      "Average loss at step 4390:   2.3\n",
      "Average loss at step 4400:   2.7\n",
      "Average loss at step 4410:   2.5\n",
      "Average loss at step 4420:   2.9\n",
      "Average loss at step 4430:   3.2\n",
      "Average loss at step 4440:   2.3\n",
      "Average loss at step 4450:   2.3\n",
      "Average loss at step 4460:   2.5\n",
      "Average loss at step 4470:   2.3\n",
      "Average loss at step 4480:   2.7\n",
      "Average loss at step 4490:   2.3\n",
      "Average loss at step 4500:   2.3\n",
      "Average loss at step 4510:   2.3\n",
      "Average loss at step 4520:   3.2\n",
      "Average loss at step 4530:   2.6\n",
      "Average loss at step 4540:   2.3\n",
      "Average loss at step 4550:   2.4\n",
      "Average loss at step 4560:   2.3\n",
      "Average loss at step 4570:   2.4\n",
      "Average loss at step 4580:   2.4\n",
      "Average loss at step 4590:   2.3\n",
      "Average loss at step 4600:   2.3\n",
      "Average loss at step 4610:   2.4\n",
      "Average loss at step 4620:   2.6\n",
      "Average loss at step 4630:   2.3\n",
      "Average loss at step 4640:   2.3\n",
      "Average loss at step 4650:   2.6\n",
      "Average loss at step 4660:   2.3\n",
      "Average loss at step 4670:   2.4\n",
      "Average loss at step 4680:   2.3\n",
      "Average loss at step 4690:   2.3\n",
      "Average loss at step 4700:   2.3\n",
      "Average loss at step 4710:   2.3\n",
      "Average loss at step 4720:   2.5\n",
      "Average loss at step 4730:   2.4\n",
      "Average loss at step 4740:   2.4\n",
      "Average loss at step 4750:   2.4\n",
      "Average loss at step 4760:   2.7\n",
      "Average loss at step 4770:   2.7\n",
      "Average loss at step 4780:   2.3\n",
      "Average loss at step 4790:   2.4\n",
      "Average loss at step 4800:   2.3\n",
      "Average loss at step 4810:   2.6\n",
      "Average loss at step 4820:   2.3\n",
      "Average loss at step 4830:   2.5\n",
      "Average loss at step 4840:   2.4\n",
      "Average loss at step 4850:   2.3\n",
      "Average loss at step 4860:   2.6\n",
      "Average loss at step 4870:   2.3\n",
      "Average loss at step 4880:   2.3\n",
      "Average loss at step 4890:   2.3\n",
      "Average loss at step 4900:   2.4\n",
      "Average loss at step 4910:   2.3\n",
      "Average loss at step 4920:   2.4\n",
      "Average loss at step 4930:   2.3\n",
      "Average loss at step 4940:   2.5\n",
      "Average loss at step 4950:   2.3\n",
      "Average loss at step 4960:   2.3\n",
      "Average loss at step 4970:   2.3\n",
      "Average loss at step 4980:   2.3\n",
      "Average loss at step 4990:   2.3\n",
      "Average loss at step 5000:   2.3\n",
      "Average loss at step 5010:   2.3\n",
      "Average loss at step 5020:   2.3\n",
      "Average loss at step 5030:   2.3\n",
      "Average loss at step 5040:   2.3\n",
      "Average loss at step 5050:   2.3\n",
      "Average loss at step 5060:   2.8\n",
      "Average loss at step 5070:   2.3\n",
      "Average loss at step 5080:   2.3\n",
      "Average loss at step 5090:   2.3\n",
      "Average loss at step 5100:   2.3\n",
      "Average loss at step 5110:   2.3\n",
      "Average loss at step 5120:   2.3\n",
      "Average loss at step 5130:   2.3\n",
      "Average loss at step 5140:   2.3\n",
      "Average loss at step 5150:   2.4\n",
      "Average loss at step 5160:   2.3\n",
      "Average loss at step 5170:   2.4\n",
      "Average loss at step 5180:   3.1\n",
      "Average loss at step 5190:   2.3\n",
      "Average loss at step 5200:   2.3\n",
      "Average loss at step 5210:   2.3\n",
      "Average loss at step 5220:   2.3\n",
      "Average loss at step 5230:   2.3\n",
      "Average loss at step 5240:   2.4\n",
      "Average loss at step 5250:   2.3\n",
      "Average loss at step 5260:   2.3\n",
      "Average loss at step 5270:   2.3\n",
      "Average loss at step 5280:   2.3\n",
      "Average loss at step 5290:   2.3\n",
      "Average loss at step 5300:   2.7\n",
      "Average loss at step 5310:   2.3\n",
      "Average loss at step 5320:   2.3\n",
      "Average loss at step 5330:   2.3\n",
      "Average loss at step 5340:   2.3\n",
      "Average loss at step 5350:   2.4\n",
      "Average loss at step 5360:   2.3\n",
      "Average loss at step 5370:   2.6\n",
      "Average loss at step 5380:   2.6\n",
      "Average loss at step 5390:   2.6\n",
      "Average loss at step 5400:   2.4\n",
      "Average loss at step 5410:   2.3\n",
      "Average loss at step 5420:   2.3\n",
      "Average loss at step 5430:   2.3\n",
      "Average loss at step 5440:   2.3\n",
      "Average loss at step 5450:   2.5\n",
      "Average loss at step 5460:   2.3\n",
      "Average loss at step 5470:   2.4\n",
      "Average loss at step 5480:   2.3\n",
      "Average loss at step 5490:   2.3\n",
      "Average loss at step 5500:   2.3\n",
      "Average loss at step 5510:   2.3\n",
      "Average loss at step 5520:   2.4\n",
      "Average loss at step 5530:   2.3\n",
      "Average loss at step 5540:   2.3\n",
      "Average loss at step 5550:   2.3\n",
      "Average loss at step 5560:   2.4\n",
      "Average loss at step 5570:   2.3\n",
      "Average loss at step 5580:   2.3\n",
      "Average loss at step 5590:   2.3\n",
      "Average loss at step 5600:   2.8\n",
      "Average loss at step 5610:   2.4\n",
      "Average loss at step 5620:   2.3\n",
      "Average loss at step 5630:   2.4\n",
      "Average loss at step 5640:   2.4\n",
      "Average loss at step 5650:   2.6\n",
      "Average loss at step 5660:   2.3\n",
      "Average loss at step 5670:   2.3\n",
      "Average loss at step 5680:   2.4\n",
      "Average loss at step 5690:   3.0\n",
      "Average loss at step 5700:   2.3\n",
      "Average loss at step 5710:   2.3\n",
      "Average loss at step 5720:   2.3\n",
      "Average loss at step 5730:   2.3\n",
      "Average loss at step 5740:   2.3\n",
      "Average loss at step 5750:   2.3\n",
      "Average loss at step 5760:   2.3\n",
      "Average loss at step 5770:   2.3\n",
      "Average loss at step 5780:   2.3\n",
      "Average loss at step 5790:   2.3\n",
      "Average loss at step 5800:   2.4\n",
      "Average loss at step 5810:   2.3\n",
      "Average loss at step 5820:   2.3\n",
      "Average loss at step 5830:   2.8\n",
      "Average loss at step 5840:   2.3\n",
      "Average loss at step 5850:   2.3\n",
      "Average loss at step 5860:   2.3\n",
      "Average loss at step 5870:   2.3\n",
      "Average loss at step 5880:   2.3\n",
      "Average loss at step 5890:   2.4\n",
      "Average loss at step 5900:   2.3\n",
      "Average loss at step 5910:   2.3\n",
      "Average loss at step 5920:   2.3\n",
      "Average loss at step 5930:   2.3\n",
      "Average loss at step 5940:   2.6\n",
      "Average loss at step 5950:   2.3\n",
      "Average loss at step 5960:   2.3\n",
      "Average loss at step 5970:   2.3\n",
      "Average loss at step 5980:   2.3\n",
      "Average loss at step 5990:   2.3\n",
      "Average loss at step 6000:   2.3\n",
      "Average loss at step 6010:   2.3\n",
      "Average loss at step 6020:   2.3\n",
      "Average loss at step 6030:   2.4\n",
      "Average loss at step 6040:   2.3\n",
      "Average loss at step 6050:   2.3\n",
      "Average loss at step 6060:   2.3\n",
      "Average loss at step 6070:   2.3\n",
      "Average loss at step 6080:   2.3\n",
      "Average loss at step 6090:   2.3\n",
      "Average loss at step 6100:   2.3\n",
      "Average loss at step 6110:   2.3\n",
      "Average loss at step 6120:   2.3\n",
      "Average loss at step 6130:   2.4\n",
      "Average loss at step 6140:   2.3\n",
      "Average loss at step 6150:   2.3\n",
      "Average loss at step 6160:   2.3\n",
      "Average loss at step 6170:   2.3\n",
      "Average loss at step 6180:   2.3\n",
      "Average loss at step 6190:   2.3\n",
      "Average loss at step 6200:   2.4\n",
      "Average loss at step 6210:   2.3\n",
      "Average loss at step 6220:   2.3\n",
      "Average loss at step 6230:   2.3\n",
      "Average loss at step 6240:   2.9\n",
      "Average loss at step 6250:   2.3\n",
      "Average loss at step 6260:   2.3\n",
      "Average loss at step 6270:   2.3\n",
      "Average loss at step 6280:   2.3\n",
      "Average loss at step 6290:   2.5\n",
      "Average loss at step 6300:   2.3\n",
      "Average loss at step 6310:   2.5\n",
      "Average loss at step 6320:   2.3\n",
      "Average loss at step 6330:   2.5\n",
      "Average loss at step 6340:   2.3\n",
      "Average loss at step 6350:   2.8\n",
      "Average loss at step 6360:   2.3\n",
      "Average loss at step 6370:   2.3\n",
      "Average loss at step 6380:   2.4\n",
      "Average loss at step 6390:   2.3\n",
      "Average loss at step 6400:   2.3\n",
      "Average loss at step 6410:   2.3\n",
      "Average loss at step 6420:   2.3\n",
      "Average loss at step 6430:   2.3\n",
      "Average loss at step 6440:   2.3\n",
      "Average loss at step 6450:   2.3\n",
      "Average loss at step 6460:   2.3\n",
      "Average loss at step 6470:   2.3\n",
      "Average loss at step 6480:   2.4\n",
      "Average loss at step 6490:   2.3\n",
      "Average loss at step 6500:   2.3\n",
      "Average loss at step 6510:   2.3\n",
      "Average loss at step 6520:   2.4\n",
      "Average loss at step 6530:   2.3\n",
      "Average loss at step 6540:   2.3\n",
      "Average loss at step 6550:   2.3\n",
      "Average loss at step 6560:   2.3\n",
      "Average loss at step 6570:   2.3\n",
      "Average loss at step 6580:   2.3\n",
      "Average loss at step 6590:   2.5\n",
      "Average loss at step 6600:   2.3\n",
      "Average loss at step 6610:   2.3\n",
      "Average loss at step 6620:   2.3\n",
      "Average loss at step 6630:   2.3\n",
      "Average loss at step 6640:   2.3\n",
      "Average loss at step 6650:   2.3\n",
      "Average loss at step 6660:   2.3\n",
      "Average loss at step 6670:   2.3\n",
      "Average loss at step 6680:   2.3\n",
      "Average loss at step 6690:   2.3\n",
      "Average loss at step 6700:   2.3\n",
      "Average loss at step 6710:   2.4\n",
      "Average loss at step 6720:   2.3\n",
      "Average loss at step 6730:   2.4\n",
      "Average loss at step 6740:   2.5\n",
      "Average loss at step 6750:   2.3\n",
      "Average loss at step 6760:   2.3\n",
      "Average loss at step 6770:   2.3\n",
      "Average loss at step 6780:   2.3\n",
      "Average loss at step 6790:   2.3\n",
      "Average loss at step 6800:   2.3\n",
      "Average loss at step 6810:   2.3\n",
      "Average loss at step 6820:   2.3\n",
      "Average loss at step 6830:   2.3\n",
      "Average loss at step 6840:   2.3\n",
      "Average loss at step 6850:   2.3\n",
      "Average loss at step 6860:   2.3\n",
      "Average loss at step 6870:   2.3\n",
      "Average loss at step 6880:   2.3\n",
      "Average loss at step 6890:   2.3\n",
      "Average loss at step 6900:   2.3\n",
      "Average loss at step 6910:   2.3\n",
      "Average loss at step 6920:   2.3\n",
      "Average loss at step 6930:   2.3\n",
      "Average loss at step 6940:   2.3\n",
      "Average loss at step 6950:   2.3\n",
      "Average loss at step 6960:   2.3\n",
      "Average loss at step 6970:   2.3\n",
      "Average loss at step 6980:   2.3\n",
      "Average loss at step 6990:   2.3\n",
      "Average loss at step 7000:   2.4\n",
      "Average loss at step 7010:   2.3\n",
      "Average loss at step 7020:   2.3\n",
      "Average loss at step 7030:   2.3\n",
      "Average loss at step 7040:   2.3\n",
      "Average loss at step 7050:   2.3\n",
      "Average loss at step 7060:   2.3\n",
      "Average loss at step 7070:   2.3\n",
      "Average loss at step 7080:   2.3\n",
      "Average loss at step 7090:   2.3\n",
      "Average loss at step 7100:   2.3\n",
      "Average loss at step 7110:   2.4\n",
      "Average loss at step 7120:   2.3\n",
      "Average loss at step 7130:   2.3\n",
      "Average loss at step 7140:   2.6\n",
      "Average loss at step 7150:   2.9\n",
      "Average loss at step 7160:   2.3\n",
      "Average loss at step 7170:   2.3\n",
      "Average loss at step 7180:   2.3\n",
      "Average loss at step 7190:   2.3\n",
      "Average loss at step 7200:   2.3\n",
      "Average loss at step 7210:   2.3\n",
      "Average loss at step 7220:   2.3\n",
      "Average loss at step 7230:   2.3\n",
      "Average loss at step 7240:   2.3\n",
      "Average loss at step 7250:   2.3\n",
      "Average loss at step 7260:   2.3\n",
      "Average loss at step 7270:   2.3\n",
      "Average loss at step 7280:   2.3\n",
      "Average loss at step 7290:   2.6\n",
      "Average loss at step 7300:   2.3\n",
      "Average loss at step 7310:   2.5\n",
      "Average loss at step 7320:   2.3\n",
      "Average loss at step 7330:   2.6\n",
      "Average loss at step 7340:   2.3\n",
      "Average loss at step 7350:   2.3\n",
      "Average loss at step 7360:   2.3\n",
      "Average loss at step 7370:   2.3\n",
      "Average loss at step 7380:   2.3\n",
      "Average loss at step 7390:   2.7\n",
      "Average loss at step 7400:   2.3\n",
      "Average loss at step 7410:   2.4\n",
      "Average loss at step 7420:   2.3\n",
      "Average loss at step 7430:   2.3\n",
      "Average loss at step 7440:   2.3\n",
      "Average loss at step 7450:   2.3\n",
      "Average loss at step 7460:   2.3\n",
      "Average loss at step 7470:   2.3\n",
      "Average loss at step 7480:   2.3\n",
      "Average loss at step 7490:   2.3\n",
      "Average loss at step 7500:   2.3\n",
      "Average loss at step 7510:   2.3\n",
      "Average loss at step 7520:   2.3\n",
      "Average loss at step 7530:   2.3\n",
      "Average loss at step 7540:   2.3\n",
      "Average loss at step 7550:   2.3\n",
      "Average loss at step 7560:   2.3\n",
      "Average loss at step 7570:   2.3\n",
      "Average loss at step 7580:   2.3\n",
      "Average loss at step 7590:   2.3\n",
      "Average loss at step 7600:   2.3\n",
      "Average loss at step 7610:   2.3\n",
      "Average loss at step 7620:   2.3\n",
      "Average loss at step 7630:   2.3\n",
      "Average loss at step 7640:   2.4\n",
      "Average loss at step 7650:   2.3\n",
      "Average loss at step 7660:   2.3\n",
      "Average loss at step 7670:   2.3\n",
      "Average loss at step 7680:   2.3\n",
      "Average loss at step 7690:   2.4\n",
      "Average loss at step 7700:   2.4\n",
      "Average loss at step 7710:   2.3\n",
      "Average loss at step 7720:   2.3\n",
      "Average loss at step 7730:   2.3\n",
      "Average loss at step 7740:   2.3\n",
      "Average loss at step 7750:   2.3\n",
      "Average loss at step 7760:   2.3\n",
      "Average loss at step 7770:   2.5\n",
      "Average loss at step 7780:   2.3\n",
      "Average loss at step 7790:   2.3\n",
      "Average loss at step 7800:   2.3\n",
      "Average loss at step 7810:   2.3\n",
      "Average loss at step 7820:   2.3\n",
      "Average loss at step 7830:   2.3\n",
      "Average loss at step 7840:   2.3\n",
      "Average loss at step 7850:   2.3\n",
      "Average loss at step 7860:   2.3\n",
      "Average loss at step 7870:   2.3\n",
      "Average loss at step 7880:   2.3\n",
      "Average loss at step 7890:   2.3\n",
      "Average loss at step 7900:   2.3\n",
      "Average loss at step 7910:   2.3\n",
      "Average loss at step 7920:   2.3\n",
      "Average loss at step 7930:   2.3\n",
      "Average loss at step 7940:   2.3\n",
      "Average loss at step 7950:   2.3\n",
      "Average loss at step 7960:   2.3\n",
      "Average loss at step 7970:   2.3\n",
      "Average loss at step 7980:   2.3\n",
      "Average loss at step 7990:   2.3\n",
      "Average loss at step 8000:   2.3\n",
      "Average loss at step 8010:   2.3\n",
      "Average loss at step 8020:   2.3\n",
      "Average loss at step 8030:   2.3\n",
      "Average loss at step 8040:   2.3\n",
      "Average loss at step 8050:   2.3\n",
      "Average loss at step 8060:   2.3\n",
      "Average loss at step 8070:   2.3\n",
      "Average loss at step 8080:   2.3\n",
      "Average loss at step 8090:   2.3\n",
      "Average loss at step 8100:   2.3\n",
      "Average loss at step 8110:   2.3\n",
      "Average loss at step 8120:   2.3\n",
      "Average loss at step 8130:   2.3\n",
      "Average loss at step 8140:   2.3\n",
      "Average loss at step 8150:   2.3\n",
      "Average loss at step 8160:   2.3\n",
      "Average loss at step 8170:   2.3\n",
      "Average loss at step 8180:   2.3\n",
      "Average loss at step 8190:   2.3\n",
      "Average loss at step 8200:   2.4\n",
      "Average loss at step 8210:   2.3\n",
      "Average loss at step 8220:   2.3\n",
      "Average loss at step 8230:   2.3\n",
      "Average loss at step 8240:   2.3\n",
      "Average loss at step 8250:   2.3\n",
      "Average loss at step 8260:   2.3\n",
      "Average loss at step 8270:   2.3\n",
      "Average loss at step 8280:   2.3\n",
      "Average loss at step 8290:   2.4\n",
      "Average loss at step 8300:   2.3\n",
      "Average loss at step 8310:   2.3\n",
      "Average loss at step 8320:   2.3\n",
      "Average loss at step 8330:   2.3\n",
      "Average loss at step 8340:   2.3\n",
      "Average loss at step 8350:   2.3\n",
      "Average loss at step 8360:   2.3\n",
      "Average loss at step 8370:   2.3\n",
      "Average loss at step 8380:   2.3\n",
      "Average loss at step 8390:   2.3\n",
      "Average loss at step 8400:   2.3\n",
      "Average loss at step 8410:   2.3\n",
      "Average loss at step 8420:   2.3\n",
      "Average loss at step 8430:   2.3\n",
      "Average loss at step 8440:   2.3\n",
      "Average loss at step 8450:   2.4\n",
      "Average loss at step 8460:   2.3\n",
      "Average loss at step 8470:   2.3\n",
      "Average loss at step 8480:   2.3\n",
      "Average loss at step 8490:   2.3\n",
      "Average loss at step 8500:   2.3\n",
      "Average loss at step 8510:   2.3\n",
      "Average loss at step 8520:   2.3\n",
      "Average loss at step 8530:   2.3\n",
      "Average loss at step 8540:   2.3\n",
      "Average loss at step 8550:   2.3\n",
      "Average loss at step 8560:   2.3\n",
      "Average loss at step 8570:   2.3\n",
      "Average loss at step 8580:   2.3\n",
      "Average loss at step 8590:   2.3\n",
      "Optimization Finished!\n",
      "Total time: 1919.5923478603363 seconds\n",
      "Accuracy 0.1115\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "N_CLASSES = 10\n",
    "\n",
    "# Step 2: Define paramaters for the model\n",
    "LEARNING_RATE = 0.01\n",
    "BATCH_SIZE = 64\n",
    "SKIP_STEP = 10\n",
    "DROPOUT = 0.80\n",
    "N_EPOCHS = 10\n",
    "\n",
    "\n",
    "with tf.name_scope('data'):\n",
    "    X = tf.placeholder(tf.float32, [None, 784], name=\"X_placeholder\")\n",
    "    Y = tf.placeholder(tf.float32, [None, 10], name=\"Y_placeholder\")\n",
    "\n",
    "dropout = tf.placeholder(tf.float32, name='dropout')\n",
    "\n",
    "global_step = tf.Variable(0, dtype=tf.int32, trainable=False, name='global_step')\n",
    "\n",
    "\n",
    "with tf.variable_scope('fc0') as scope:\n",
    "    \n",
    "    w = tf.get_variable('weights', shape=[784, 1024], initializer=tf.truncated_normal_initializer())\n",
    "    b = tf.get_variable('bias', shape=[1024], initializer=tf.constant_initializer(0.0))\n",
    "    \n",
    "    fc0 = tf.cos(tf.add(tf.matmul(X, w), b))\n",
    "    \n",
    "\n",
    "with tf.variable_scope('conv1') as scope:\n",
    "    \n",
    "    images = tf.reshape(fc0, shape=[-1, 32, 32, 1])\n",
    "    \n",
    "    \n",
    "    kernel = tf.get_variable('kernel', shape=[5, 5, 1, 32], initializer=tf.truncated_normal_initializer())\n",
    "    \n",
    "   \n",
    "    \n",
    "    biases = tf.get_variable('biases', shape=[32], initializer=tf.constant_initializer(0.0))\n",
    "    \n",
    "   \n",
    "    conv = tf.nn.conv2d(images, kernel, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    \n",
    "   \n",
    "    conv1 = tf.nn.relu(conv + biases, name='conv1')\n",
    "    \n",
    "    \n",
    "    \n",
    "with tf.variable_scope('pool1') as scope:\n",
    "   \n",
    "    \n",
    "    pool1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], name='pool1', strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "    \n",
    "\n",
    "with tf.variable_scope('conv2') as scope:\n",
    "\n",
    "    kernel = tf.get_variable('kernels', [5, 5, 32, 64], \n",
    "                        initializer=tf.truncated_normal_initializer())\n",
    "    biases = tf.get_variable('biases', [64],\n",
    "                        initializer=tf.random_normal_initializer())\n",
    "    conv = tf.nn.conv2d(pool1, kernel, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    conv2 = tf.nn.relu(conv + biases, name=scope.name)\n",
    "\n",
    "\n",
    "with tf.variable_scope('pool2') as scope:\n",
    "\n",
    "    pool2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1],\n",
    "                            padding='SAME')\n",
    "\n",
    "   \n",
    "    \n",
    "\n",
    "with tf.variable_scope('fc') as scope:\n",
    "\n",
    "    input_features = 8 * 8 * 64\n",
    "    \n",
    "   \n",
    "    w = tf.get_variable('weights', shape=[input_features, 1024], initializer=tf.truncated_normal_initializer())\n",
    "    b = tf.get_variable('bias', shape=[1024], initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "    pool2 = tf.reshape(pool2, [-1, input_features])\n",
    "\n",
    "    fc = tf.nn.relu(tf.matmul(pool2, w) + b, name='relu')\n",
    "    \n",
    "   \n",
    "    fc = tf.nn.dropout(fc, dropout, name='relu_dropout')\n",
    "\n",
    "with tf.variable_scope('softmax_linear') as scope:\n",
    "    \n",
    "    w = tf.get_variable('weights', shape=[1024, N_CLASSES], initializer=tf.truncated_normal_initializer())\n",
    "    b = tf.get_variable('bias', shape=[N_CLASSES], initializer=tf.constant_initializer(0.0))\n",
    "    \n",
    "    logits = tf.matmul(fc, w) + b\n",
    "\n",
    "\n",
    "    \n",
    "with tf.name_scope('loss'):\n",
    "   \n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=Y, logits=logits), name='loss')\n",
    "\n",
    "\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE).minimize(loss, global_step=global_step)\n",
    "\n",
    "with tf.name_scope('summary'):\n",
    "    tf.summary.scalar('loss', loss)\n",
    "    summary_op = tf.summary.merge_all()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    writer = tf.summary.FileWriter('../my_graph/mnist2', sess.graph)\n",
    "   \n",
    "    \n",
    "    initial_step = global_step.eval()\n",
    "\n",
    "    start_time = time.time()\n",
    "    n_batches = int(mnist.train.num_examples / BATCH_SIZE)\n",
    "\n",
    "    total_loss = 0.0\n",
    "    for index in range(initial_step, n_batches * N_EPOCHS): # train the model n_epochs times\n",
    "        X_batch, Y_batch = mnist.train.next_batch(BATCH_SIZE)\n",
    "        _, loss_batch, summary = sess.run([optimizer, loss, summary_op], \n",
    "                                feed_dict={X: X_batch, Y:Y_batch, dropout: DROPOUT}) \n",
    "        total_loss += loss_batch\n",
    "        writer.add_summary(summary, global_step=index)\n",
    "        if (index + 1) % SKIP_STEP == 0:\n",
    "            print('Average loss at step {}: {:5.1f}'.format(index + 1, total_loss / SKIP_STEP))\n",
    "            total_loss = 0.0\n",
    "            saver.save(sess, '../checkpoints/convnet_mnist2/mnist-convnet', index)\n",
    "    \n",
    "    print(\"Optimization Finished!\") # should be around 0.35 after 25 epochs\n",
    "    print(\"Total time: {0} seconds\".format(time.time() - start_time))\n",
    "    \n",
    "    # test the model\n",
    "    n_batches = int(mnist.test.num_examples/BATCH_SIZE)\n",
    "    total_correct_preds = 0\n",
    "    for i in range(n_batches):\n",
    "        X_batch, Y_batch = mnist.test.next_batch(BATCH_SIZE)\n",
    "        _, loss_batch, logits_batch = sess.run([optimizer, loss, logits], \n",
    "                                        feed_dict={X: X_batch, Y:Y_batch, dropout: DROPOUT}) \n",
    "        preds = tf.nn.softmax(logits_batch)\n",
    "        correct_preds = tf.equal(tf.argmax(preds, 1), tf.argmax(Y_batch, 1))\n",
    "        accuracy = tf.reduce_sum(tf.cast(correct_preds, tf.float32))\n",
    "        total_correct_preds += sess.run(accuracy)   \n",
    "    \n",
    "    print(\"Accuracy {0}\".format(total_correct_preds/mnist.test.num_examples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# (10 epochs) one fc with cosine then 2 conv nets then 1 fc with dropout  -> Accuracy 0.1115 :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 10: 1366590.3\n",
      "Average loss at step 20: 80393.4\n",
      "Average loss at step 30: 15991.7\n",
      "Average loss at step 40: 9279.1\n",
      "Average loss at step 50: 6295.8\n",
      "Average loss at step 60: 4541.7\n",
      "Average loss at step 70: 4650.9\n",
      "Average loss at step 80: 3912.3\n",
      "Average loss at step 90: 3518.4\n",
      "Average loss at step 100: 3338.1\n",
      "Average loss at step 110: 2680.4\n",
      "Average loss at step 120: 2334.6\n",
      "Average loss at step 130: 2413.0\n",
      "Average loss at step 140: 2412.8\n",
      "Average loss at step 150: 2226.1\n",
      "Average loss at step 160: 2196.5\n",
      "Average loss at step 170: 1962.1\n",
      "Average loss at step 180: 1892.0\n",
      "Average loss at step 190: 1497.8\n",
      "Average loss at step 200: 1620.8\n",
      "Average loss at step 210: 1621.5\n",
      "Average loss at step 220: 1291.9\n",
      "Average loss at step 230: 1322.8\n",
      "Average loss at step 240: 1289.1\n",
      "Average loss at step 250: 1153.8\n",
      "Average loss at step 260: 1096.3\n",
      "Average loss at step 270: 1088.2\n",
      "Average loss at step 280: 1038.0\n",
      "Average loss at step 290: 985.9\n",
      "Average loss at step 300: 875.0\n",
      "Average loss at step 310: 962.8\n",
      "Average loss at step 320: 844.6\n",
      "Average loss at step 330: 768.9\n",
      "Average loss at step 340: 641.9\n",
      "Average loss at step 350: 746.5\n",
      "Average loss at step 360: 581.4\n",
      "Average loss at step 370: 596.8\n",
      "Average loss at step 380: 429.0\n",
      "Average loss at step 390: 465.4\n",
      "Average loss at step 400: 506.2\n",
      "Average loss at step 410: 417.7\n",
      "Average loss at step 420: 382.3\n",
      "Average loss at step 430: 378.3\n",
      "Average loss at step 440: 258.3\n",
      "Average loss at step 450: 328.5\n",
      "Average loss at step 460: 237.5\n",
      "Average loss at step 470: 253.1\n",
      "Average loss at step 480: 220.2\n",
      "Average loss at step 490: 202.3\n",
      "Average loss at step 500: 152.8\n",
      "Average loss at step 510: 147.6\n",
      "Average loss at step 520: 122.6\n",
      "Average loss at step 530: 110.4\n",
      "Average loss at step 540:  96.4\n",
      "Average loss at step 550: 125.2\n",
      "Average loss at step 560:  83.6\n",
      "Average loss at step 570: 104.6\n",
      "Average loss at step 580:  74.1\n",
      "Average loss at step 590:  52.9\n",
      "Average loss at step 600:  90.0\n",
      "Average loss at step 610:  70.1\n",
      "Average loss at step 620:  66.8\n",
      "Average loss at step 630:  54.0\n",
      "Average loss at step 640:  46.9\n",
      "Average loss at step 650:  35.4\n",
      "Average loss at step 660:  52.5\n",
      "Average loss at step 670:  37.0\n",
      "Average loss at step 680:  51.0\n",
      "Average loss at step 690:  44.4\n",
      "Average loss at step 700:  37.7\n",
      "Average loss at step 710:  29.9\n",
      "Average loss at step 720:  31.9\n",
      "Average loss at step 730:  36.2\n",
      "Average loss at step 740:  49.1\n",
      "Average loss at step 750:  27.0\n",
      "Average loss at step 760:  37.9\n",
      "Average loss at step 770:  26.3\n",
      "Average loss at step 780:  21.4\n",
      "Average loss at step 790:  26.9\n",
      "Average loss at step 800:  14.9\n",
      "Average loss at step 810:  21.0\n",
      "Average loss at step 820:  15.5\n",
      "Average loss at step 830:  21.1\n",
      "Average loss at step 840:  22.9\n",
      "Average loss at step 850:  20.7\n",
      "Average loss at step 860:  14.9\n",
      "Average loss at step 870:  16.3\n",
      "Average loss at step 880:  16.7\n",
      "Average loss at step 890:  10.5\n",
      "Average loss at step 900:  27.0\n",
      "Average loss at step 910:  19.8\n",
      "Average loss at step 920:  23.0\n",
      "Average loss at step 930:  12.2\n",
      "Average loss at step 940:  18.6\n",
      "Average loss at step 950:   6.7\n",
      "Average loss at step 960:  17.4\n",
      "Average loss at step 970:  14.3\n",
      "Average loss at step 980:  13.1\n",
      "Average loss at step 990:  10.0\n",
      "Average loss at step 1000:  19.5\n",
      "Average loss at step 1010:  20.5\n",
      "Average loss at step 1020:   7.3\n",
      "Average loss at step 1030:  26.9\n",
      "Average loss at step 1040:   9.8\n",
      "Average loss at step 1050:   9.4\n",
      "Average loss at step 1060:  13.4\n",
      "Average loss at step 1070:  11.2\n",
      "Average loss at step 1080:   6.9\n",
      "Average loss at step 1090:   7.3\n",
      "Average loss at step 1100:   8.1\n",
      "Average loss at step 1110:   7.2\n",
      "Average loss at step 1120:  18.8\n",
      "Average loss at step 1130:   7.7\n",
      "Average loss at step 1140:  11.1\n",
      "Average loss at step 1150:   6.3\n",
      "Average loss at step 1160:   4.0\n",
      "Average loss at step 1170:   8.0\n",
      "Average loss at step 1180:   8.6\n",
      "Average loss at step 1190:   8.0\n",
      "Average loss at step 1200:   6.4\n",
      "Average loss at step 1210:   8.1\n",
      "Average loss at step 1220:  10.8\n",
      "Average loss at step 1230:   5.7\n",
      "Average loss at step 1240:   5.4\n",
      "Average loss at step 1250:  10.6\n",
      "Average loss at step 1260:   3.9\n",
      "Average loss at step 1270:  11.5\n",
      "Average loss at step 1280:   5.6\n",
      "Average loss at step 1290:   8.5\n",
      "Average loss at step 1300:   4.8\n",
      "Average loss at step 1310:   4.7\n",
      "Average loss at step 1320:   4.6\n",
      "Average loss at step 1330:   5.9\n",
      "Average loss at step 1340:   8.2\n",
      "Average loss at step 1350:   8.0\n",
      "Average loss at step 1360:   9.5\n",
      "Average loss at step 1370:   4.9\n",
      "Average loss at step 1380:   6.4\n",
      "Average loss at step 1390:   3.4\n",
      "Average loss at step 1400:   6.1\n",
      "Average loss at step 1410:  15.3\n",
      "Average loss at step 1420:   4.0\n",
      "Average loss at step 1430:   4.2\n",
      "Average loss at step 1440:   4.5\n",
      "Average loss at step 1450:   4.7\n",
      "Average loss at step 1460:   7.7\n",
      "Average loss at step 1470:   4.6\n",
      "Average loss at step 1480:   6.2\n",
      "Average loss at step 1490:   2.3\n",
      "Average loss at step 1500:   4.5\n",
      "Average loss at step 1510:   5.1\n",
      "Average loss at step 1520:   2.6\n",
      "Average loss at step 1530:   9.3\n",
      "Average loss at step 1540:   4.3\n",
      "Average loss at step 1550:   3.6\n",
      "Average loss at step 1560:   3.3\n",
      "Average loss at step 1570:   2.9\n",
      "Average loss at step 1580:   4.7\n",
      "Average loss at step 1590:   5.4\n",
      "Average loss at step 1600:   6.3\n",
      "Average loss at step 1610:   3.7\n",
      "Average loss at step 1620:   4.3\n",
      "Average loss at step 1630:   6.5\n",
      "Average loss at step 1640:   4.1\n",
      "Average loss at step 1650:   3.6\n",
      "Average loss at step 1660:   7.8\n",
      "Average loss at step 1670:   5.0\n",
      "Average loss at step 1680:   2.9\n",
      "Average loss at step 1690:   2.5\n",
      "Average loss at step 1700:   3.2\n",
      "Average loss at step 1710:   5.2\n",
      "Optimization Finished!\n",
      "Total time: 385.8700170516968 seconds\n",
      "Accuracy 0.1285\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "N_CLASSES = 10\n",
    "\n",
    "# Step 2: Define paramaters for the model\n",
    "LEARNING_RATE = 0.01\n",
    "BATCH_SIZE = 64\n",
    "SKIP_STEP = 10\n",
    "DROPOUT = 0.80\n",
    "N_EPOCHS = 2\n",
    "\n",
    "\n",
    "with tf.name_scope('data'):\n",
    "    X = tf.placeholder(tf.float32, [None, 784], name=\"X_placeholder\")\n",
    "    Y = tf.placeholder(tf.float32, [None, 10], name=\"Y_placeholder\")\n",
    "\n",
    "dropout = tf.placeholder(tf.float32, name='dropout')\n",
    "\n",
    "global_step = tf.Variable(0, dtype=tf.int32, trainable=False, name='global_step')\n",
    "\n",
    "\n",
    "with tf.variable_scope('fc0') as scope:\n",
    "    \n",
    "    w = tf.get_variable('weights', shape=[784, 1024], initializer=tf.truncated_normal_initializer())\n",
    "    b = tf.get_variable('bias', shape=[1024], initializer=tf.constant_initializer(0.0))\n",
    "    \n",
    "    fc0 = tf.nn.relu(tf.add(tf.matmul(X, w), b))\n",
    "    \n",
    "\n",
    "with tf.variable_scope('conv1') as scope:\n",
    "    \n",
    "    images = tf.reshape(fc0, shape=[-1, 32, 32, 1])\n",
    "    \n",
    "    \n",
    "    kernel = tf.get_variable('kernel', shape=[5, 5, 1, 32], initializer=tf.truncated_normal_initializer())\n",
    "    \n",
    "   \n",
    "    \n",
    "    biases = tf.get_variable('biases', shape=[32], initializer=tf.constant_initializer(0.0))\n",
    "    \n",
    "   \n",
    "    conv = tf.nn.conv2d(images, kernel, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    \n",
    "   \n",
    "    conv1 = tf.nn.relu(conv + biases, name='conv1')\n",
    "    \n",
    "    \n",
    "    \n",
    "with tf.variable_scope('pool1') as scope:\n",
    "   \n",
    "    \n",
    "    pool1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], name='pool1', strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "    \n",
    "\n",
    "with tf.variable_scope('conv2') as scope:\n",
    "\n",
    "    kernel = tf.get_variable('kernels', [5, 5, 32, 64], \n",
    "                        initializer=tf.truncated_normal_initializer())\n",
    "    biases = tf.get_variable('biases', [64],\n",
    "                        initializer=tf.random_normal_initializer())\n",
    "    conv = tf.nn.conv2d(pool1, kernel, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    conv2 = tf.nn.relu(conv + biases, name=scope.name)\n",
    "\n",
    "\n",
    "with tf.variable_scope('pool2') as scope:\n",
    "\n",
    "    pool2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1],\n",
    "                            padding='SAME')\n",
    "\n",
    "   \n",
    "    \n",
    "\n",
    "with tf.variable_scope('fc') as scope:\n",
    "\n",
    "    input_features = 8 * 8 * 64\n",
    "    \n",
    "   \n",
    "    w = tf.get_variable('weights', shape=[input_features, 1024], initializer=tf.truncated_normal_initializer())\n",
    "    b = tf.get_variable('bias', shape=[1024], initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "    pool2 = tf.reshape(pool2, [-1, input_features])\n",
    "\n",
    "    fc = tf.nn.relu(tf.matmul(pool2, w) + b, name='relu')\n",
    "    \n",
    "   \n",
    "    fc = tf.nn.dropout(fc, dropout, name='relu_dropout')\n",
    "\n",
    "with tf.variable_scope('softmax_linear') as scope:\n",
    "    \n",
    "    w = tf.get_variable('weights', shape=[1024, N_CLASSES], initializer=tf.truncated_normal_initializer())\n",
    "    b = tf.get_variable('bias', shape=[N_CLASSES], initializer=tf.constant_initializer(0.0))\n",
    "    \n",
    "    logits = tf.matmul(fc, w) + b\n",
    "\n",
    "\n",
    "    \n",
    "with tf.name_scope('loss'):\n",
    "   \n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=Y, logits=logits), name='loss')\n",
    "\n",
    "\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE).minimize(loss, global_step=global_step)\n",
    "\n",
    "with tf.name_scope('summary'):\n",
    "    tf.summary.scalar('loss', loss)\n",
    "    summary_op = tf.summary.merge_all()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    writer = tf.summary.FileWriter('../my_graph/mnist3', sess.graph)\n",
    "   \n",
    "    \n",
    "    initial_step = global_step.eval()\n",
    "\n",
    "    start_time = time.time()\n",
    "    n_batches = int(mnist.train.num_examples / BATCH_SIZE)\n",
    "\n",
    "    total_loss = 0.0\n",
    "    for index in range(initial_step, n_batches * N_EPOCHS): # train the model n_epochs times\n",
    "        X_batch, Y_batch = mnist.train.next_batch(BATCH_SIZE)\n",
    "        _, loss_batch, summary = sess.run([optimizer, loss, summary_op], \n",
    "                                feed_dict={X: X_batch, Y:Y_batch, dropout: DROPOUT}) \n",
    "        total_loss += loss_batch\n",
    "        writer.add_summary(summary, global_step=index)\n",
    "        if (index + 1) % SKIP_STEP == 0:\n",
    "            print('Average loss at step {}: {:5.1f}'.format(index + 1, total_loss / SKIP_STEP))\n",
    "            total_loss = 0.0\n",
    "            saver.save(sess, '../checkpoints/convnet_mnist3/mnist-convnet', index)\n",
    "    \n",
    "    print(\"Optimization Finished!\") # should be around 0.35 after 25 epochs\n",
    "    print(\"Total time: {0} seconds\".format(time.time() - start_time))\n",
    "    \n",
    "    # test the model\n",
    "    n_batches = int(mnist.test.num_examples/BATCH_SIZE)\n",
    "    total_correct_preds = 0\n",
    "    for i in range(n_batches):\n",
    "        X_batch, Y_batch = mnist.test.next_batch(BATCH_SIZE)\n",
    "        _, loss_batch, logits_batch = sess.run([optimizer, loss, logits], \n",
    "                                        feed_dict={X: X_batch, Y:Y_batch, dropout: DROPOUT}) \n",
    "        preds = tf.nn.softmax(logits_batch)\n",
    "        correct_preds = tf.equal(tf.argmax(preds, 1), tf.argmax(Y_batch, 1))\n",
    "        accuracy = tf.reduce_sum(tf.cast(correct_preds, tf.float32))\n",
    "        total_correct_preds += sess.run(accuracy)   \n",
    "    \n",
    "    print(\"Accuracy {0}\".format(total_correct_preds/mnist.test.num_examples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# (2 epochs) one fc with cosine then 2 conv nets then 1 fc with dropout  -> Accuracy 0.1285 :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
