{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INCREASE MNIST MODEL TO REACH AT LEAST 97%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting ../data/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting ../data/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../data/mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"../data/mnist\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 10: 49462.9\n",
      "Average loss at step 20: 6585.9\n",
      "Average loss at step 30: 1948.9\n",
      "Average loss at step 40: 1246.3\n",
      "Average loss at step 50: 848.3\n",
      "Average loss at step 60: 555.2\n",
      "Average loss at step 70: 460.3\n",
      "Average loss at step 80: 434.5\n",
      "Average loss at step 90: 376.0\n",
      "Average loss at step 100: 298.7\n",
      "Average loss at step 110: 288.6\n",
      "Average loss at step 120: 290.7\n",
      "Average loss at step 130: 201.3\n",
      "Average loss at step 140: 140.2\n",
      "Average loss at step 150: 186.7\n",
      "Average loss at step 160: 167.9\n",
      "Average loss at step 170: 128.4\n",
      "Average loss at step 180: 188.6\n",
      "Average loss at step 190: 195.5\n",
      "Average loss at step 200: 117.7\n",
      "Average loss at step 210: 140.7\n",
      "Average loss at step 220: 167.2\n",
      "Average loss at step 230: 188.0\n",
      "Average loss at step 240: 140.5\n",
      "Average loss at step 250: 161.8\n",
      "Average loss at step 260: 141.8\n",
      "Average loss at step 270: 169.2\n",
      "Average loss at step 280: 130.6\n",
      "Average loss at step 290: 149.6\n",
      "Average loss at step 300: 118.7\n",
      "Average loss at step 310: 111.7\n",
      "Average loss at step 320:  69.3\n",
      "Average loss at step 330:  84.4\n",
      "Average loss at step 340: 112.4\n",
      "Average loss at step 350:  82.9\n",
      "Average loss at step 360: 126.4\n",
      "Average loss at step 370:  86.0\n",
      "Average loss at step 380:  61.7\n",
      "Average loss at step 390:  89.3\n",
      "Average loss at step 400:  99.3\n",
      "Average loss at step 410:  70.2\n",
      "Average loss at step 420: 125.5\n",
      "Average loss at step 430:  96.8\n",
      "Average loss at step 440: 106.0\n",
      "Average loss at step 450:  71.0\n",
      "Average loss at step 460:  43.0\n",
      "Average loss at step 470:  78.5\n",
      "Average loss at step 480:  51.5\n",
      "Average loss at step 490:  77.7\n",
      "Average loss at step 500:  66.5\n",
      "Average loss at step 510:  88.5\n",
      "Average loss at step 520:  29.2\n",
      "Average loss at step 530:  92.1\n",
      "Average loss at step 540:  56.0\n",
      "Average loss at step 550:  46.6\n",
      "Average loss at step 560:  62.2\n",
      "Average loss at step 570:  73.8\n",
      "Average loss at step 580:  45.1\n",
      "Average loss at step 590:  75.6\n",
      "Average loss at step 600:  55.5\n",
      "Average loss at step 610:  85.1\n",
      "Average loss at step 620:  34.3\n",
      "Average loss at step 630:  65.0\n",
      "Average loss at step 640:  82.2\n",
      "Average loss at step 650:  61.6\n",
      "Average loss at step 660:  52.2\n",
      "Average loss at step 670:  67.5\n",
      "Average loss at step 680:  40.3\n",
      "Average loss at step 690:  71.2\n",
      "Average loss at step 700:  44.9\n",
      "Average loss at step 710:  48.2\n",
      "Average loss at step 720:  41.5\n",
      "Average loss at step 730:  51.7\n",
      "Average loss at step 740:  53.0\n",
      "Average loss at step 750:  63.7\n",
      "Average loss at step 760:  66.9\n",
      "Average loss at step 770:  33.8\n",
      "Average loss at step 780:  75.9\n",
      "Average loss at step 790:  56.9\n",
      "Average loss at step 800:  58.4\n",
      "Average loss at step 810:  58.4\n",
      "Average loss at step 820:  57.1\n",
      "Average loss at step 830:  49.5\n",
      "Average loss at step 840:  47.1\n",
      "Average loss at step 850:  35.3\n",
      "Average loss at step 860:  32.2\n",
      "Average loss at step 870:  38.8\n",
      "Average loss at step 880:  25.0\n",
      "Average loss at step 890:  40.9\n",
      "Average loss at step 900:  32.9\n",
      "Average loss at step 910:  37.1\n",
      "Average loss at step 920:  37.9\n",
      "Average loss at step 930:  25.9\n",
      "Average loss at step 940:  32.9\n",
      "Average loss at step 950:  30.3\n",
      "Average loss at step 960:  57.5\n",
      "Average loss at step 970:  55.0\n",
      "Average loss at step 980:  27.1\n",
      "Average loss at step 990:  36.3\n",
      "Average loss at step 1000:  29.8\n",
      "Average loss at step 1010:  37.7\n",
      "Average loss at step 1020:  59.4\n",
      "Average loss at step 1030:  27.8\n",
      "Average loss at step 1040:  30.7\n",
      "Average loss at step 1050:  18.9\n",
      "Average loss at step 1060:  40.3\n",
      "Average loss at step 1070:  36.7\n",
      "Average loss at step 1080:  25.0\n",
      "Average loss at step 1090:  20.0\n",
      "Average loss at step 1100:  38.8\n",
      "Average loss at step 1110:  47.2\n",
      "Average loss at step 1120:  30.6\n",
      "Average loss at step 1130:  24.6\n",
      "Average loss at step 1140:  33.5\n",
      "Average loss at step 1150:  47.5\n",
      "Average loss at step 1160:  36.8\n",
      "Average loss at step 1170:  41.5\n",
      "Average loss at step 1180:  36.7\n",
      "Average loss at step 1190:  35.3\n",
      "Average loss at step 1200:  37.2\n",
      "Average loss at step 1210:  35.4\n",
      "Average loss at step 1220:  29.5\n",
      "Average loss at step 1230:  31.6\n",
      "Average loss at step 1240:  39.6\n",
      "Average loss at step 1250:  42.3\n",
      "Average loss at step 1260:  30.3\n",
      "Average loss at step 1270:  18.5\n",
      "Average loss at step 1280:  12.5\n",
      "Average loss at step 1290:  44.7\n",
      "Average loss at step 1300:  44.3\n",
      "Average loss at step 1310:  29.0\n",
      "Average loss at step 1320:  33.6\n",
      "Average loss at step 1330:  26.6\n",
      "Average loss at step 1340:  27.7\n",
      "Average loss at step 1350:  28.8\n",
      "Average loss at step 1360:  31.3\n",
      "Average loss at step 1370:  31.7\n",
      "Average loss at step 1380:  14.3\n",
      "Average loss at step 1390:  22.8\n",
      "Average loss at step 1400:  29.9\n",
      "Average loss at step 1410:  30.3\n",
      "Average loss at step 1420:  31.0\n",
      "Average loss at step 1430:  35.4\n",
      "Average loss at step 1440:  37.7\n",
      "Average loss at step 1450:  26.6\n",
      "Average loss at step 1460:  25.0\n",
      "Average loss at step 1470:  20.0\n",
      "Average loss at step 1480:   7.3\n",
      "Average loss at step 1490:  32.8\n",
      "Average loss at step 1500:  33.1\n",
      "Average loss at step 1510:  36.1\n",
      "Average loss at step 1520:  41.3\n",
      "Average loss at step 1530:  25.3\n",
      "Average loss at step 1540:  21.5\n",
      "Average loss at step 1550:  36.8\n",
      "Average loss at step 1560:  28.2\n",
      "Average loss at step 1570:  14.1\n",
      "Average loss at step 1580:  43.1\n",
      "Average loss at step 1590:  30.9\n",
      "Average loss at step 1600:  25.5\n",
      "Average loss at step 1610:  31.2\n",
      "Average loss at step 1620:  26.6\n",
      "Average loss at step 1630:  28.6\n",
      "Average loss at step 1640:  35.3\n",
      "Average loss at step 1650:  14.5\n",
      "Average loss at step 1660:  19.8\n",
      "Average loss at step 1670:  13.6\n",
      "Average loss at step 1680:  20.9\n",
      "Average loss at step 1690:  18.8\n",
      "Average loss at step 1700:  18.8\n",
      "Average loss at step 1710:  17.3\n",
      "Average loss at step 1720:  12.4\n",
      "Average loss at step 1730:   9.1\n",
      "Average loss at step 1740:  13.7\n",
      "Average loss at step 1750:  17.3\n",
      "Average loss at step 1760:  10.4\n",
      "Average loss at step 1770:   5.3\n",
      "Average loss at step 1780:  11.8\n",
      "Average loss at step 1790:  17.4\n",
      "Average loss at step 1800:  14.8\n",
      "Average loss at step 1810:   8.6\n",
      "Average loss at step 1820:  14.3\n",
      "Average loss at step 1830:  11.4\n",
      "Average loss at step 1840:   7.9\n",
      "Average loss at step 1850:  17.2\n",
      "Average loss at step 1860:  12.1\n",
      "Average loss at step 1870:  18.5\n",
      "Average loss at step 1880:  24.8\n",
      "Average loss at step 1890:   9.8\n",
      "Average loss at step 1900:   9.4\n",
      "Average loss at step 1910:  15.6\n",
      "Average loss at step 1920:  19.6\n",
      "Average loss at step 1930:  16.6\n",
      "Average loss at step 1940:   8.5\n",
      "Average loss at step 1950:  14.6\n",
      "Average loss at step 1960:  15.5\n",
      "Average loss at step 1970:  27.5\n",
      "Average loss at step 1980:  13.3\n",
      "Average loss at step 1990:  29.1\n",
      "Average loss at step 2000:  22.2\n",
      "Average loss at step 2010:  16.9\n",
      "Average loss at step 2020:  21.7\n",
      "Average loss at step 2030:  16.4\n",
      "Average loss at step 2040:  26.2\n",
      "Average loss at step 2050:  27.9\n",
      "Average loss at step 2060:  16.5\n",
      "Average loss at step 2070:  37.1\n",
      "Average loss at step 2080:  12.9\n",
      "Average loss at step 2090:  17.6\n",
      "Average loss at step 2100:  17.9\n",
      "Average loss at step 2110:  21.5\n",
      "Average loss at step 2120:  12.2\n",
      "Average loss at step 2130:  24.2\n",
      "Average loss at step 2140:  16.4\n",
      "Average loss at step 2150:  21.4\n",
      "Average loss at step 2160:  17.9\n",
      "Average loss at step 2170:  27.6\n",
      "Average loss at step 2180:  15.8\n",
      "Average loss at step 2190:  24.6\n",
      "Average loss at step 2200:  13.3\n",
      "Average loss at step 2210:  23.1\n",
      "Average loss at step 2220:  26.5\n",
      "Average loss at step 2230:  20.8\n",
      "Average loss at step 2240:  18.2\n",
      "Average loss at step 2250:  13.5\n",
      "Average loss at step 2260:  22.2\n",
      "Average loss at step 2270:  18.6\n",
      "Average loss at step 2280:   4.3\n",
      "Average loss at step 2290:  11.3\n",
      "Average loss at step 2300:  14.8\n",
      "Average loss at step 2310:  11.2\n",
      "Average loss at step 2320:  23.3\n",
      "Average loss at step 2330:   5.2\n",
      "Average loss at step 2340:  19.5\n",
      "Average loss at step 2350:  10.2\n",
      "Average loss at step 2360:  21.9\n",
      "Average loss at step 2370:  15.8\n",
      "Average loss at step 2380:  35.4\n",
      "Average loss at step 2390:  17.6\n",
      "Average loss at step 2400:  23.5\n",
      "Average loss at step 2410:  13.7\n",
      "Average loss at step 2420:  17.8\n",
      "Average loss at step 2430:  24.8\n",
      "Average loss at step 2440:  13.1\n",
      "Average loss at step 2450:  18.6\n",
      "Average loss at step 2460:  15.4\n",
      "Average loss at step 2470:  19.0\n",
      "Average loss at step 2480:  22.7\n",
      "Average loss at step 2490:  25.3\n",
      "Average loss at step 2500:  19.4\n",
      "Average loss at step 2510:  21.7\n",
      "Average loss at step 2520:  24.9\n",
      "Average loss at step 2530:  10.6\n",
      "Average loss at step 2540:  12.2\n",
      "Average loss at step 2550:   6.3\n",
      "Average loss at step 2560:  12.9\n",
      "Average loss at step 2570:   7.4\n",
      "Average loss at step 2580:  12.2\n",
      "Average loss at step 2590:  10.0\n",
      "Average loss at step 2600:  10.3\n",
      "Average loss at step 2610:  13.1\n",
      "Average loss at step 2620:   9.6\n",
      "Average loss at step 2630:   9.0\n",
      "Average loss at step 2640:   8.1\n",
      "Average loss at step 2650:  10.1\n",
      "Average loss at step 2660:  11.7\n",
      "Average loss at step 2670:   7.4\n",
      "Average loss at step 2680:   9.9\n",
      "Average loss at step 2690:  10.8\n",
      "Average loss at step 2700:  19.9\n",
      "Average loss at step 2710:  13.7\n",
      "Average loss at step 2720:  24.6\n",
      "Average loss at step 2730:  15.2\n",
      "Average loss at step 2740:  13.2\n",
      "Average loss at step 2750:   8.7\n",
      "Average loss at step 2760:  23.4\n",
      "Average loss at step 2770:  14.8\n",
      "Average loss at step 2780:  26.6\n",
      "Average loss at step 2790:  13.7\n",
      "Average loss at step 2800:  13.8\n",
      "Average loss at step 2810:  21.8\n",
      "Average loss at step 2820:  18.4\n",
      "Average loss at step 2830:   8.6\n",
      "Average loss at step 2840:   6.5\n",
      "Average loss at step 2850:   1.4\n",
      "Average loss at step 2860:  16.0\n",
      "Average loss at step 2870:   9.4\n",
      "Average loss at step 2880:  11.4\n",
      "Average loss at step 2890:   6.8\n",
      "Average loss at step 2900:  12.8\n",
      "Average loss at step 2910:  16.7\n",
      "Average loss at step 2920:  12.0\n",
      "Average loss at step 2930:  15.8\n",
      "Average loss at step 2940:   6.3\n",
      "Average loss at step 2950:  15.6\n",
      "Average loss at step 2960:  10.1\n",
      "Average loss at step 2970:   9.2\n",
      "Average loss at step 2980:  15.3\n",
      "Average loss at step 2990:  16.1\n",
      "Average loss at step 3000:  13.7\n",
      "Average loss at step 3010:  16.9\n",
      "Average loss at step 3020:  11.9\n",
      "Average loss at step 3030:  14.9\n",
      "Average loss at step 3040:  18.5\n",
      "Average loss at step 3050:  16.6\n",
      "Average loss at step 3060:  26.0\n",
      "Average loss at step 3070:  12.2\n",
      "Average loss at step 3080:  21.4\n",
      "Average loss at step 3090:  13.9\n",
      "Average loss at step 3100:  11.6\n",
      "Average loss at step 3110:  16.5\n",
      "Average loss at step 3120:   2.9\n",
      "Average loss at step 3130:  10.9\n",
      "Average loss at step 3140:  19.5\n",
      "Average loss at step 3150:   9.8\n",
      "Average loss at step 3160:   8.3\n",
      "Average loss at step 3170:  20.1\n",
      "Average loss at step 3180:  13.0\n",
      "Average loss at step 3190:  22.7\n",
      "Average loss at step 3200:  12.6\n",
      "Average loss at step 3210:  14.5\n",
      "Average loss at step 3220:   8.1\n",
      "Average loss at step 3230:  13.8\n",
      "Average loss at step 3240:  10.4\n",
      "Average loss at step 3250:  18.0\n",
      "Average loss at step 3260:  20.3\n",
      "Average loss at step 3270:  12.7\n",
      "Average loss at step 3280:  22.6\n",
      "Average loss at step 3290:  13.3\n",
      "Average loss at step 3300:  14.4\n",
      "Average loss at step 3310:  11.0\n",
      "Average loss at step 3320:  20.5\n",
      "Average loss at step 3330:  19.2\n",
      "Average loss at step 3340:  16.3\n",
      "Average loss at step 3350:  17.1\n",
      "Average loss at step 3360:  13.0\n",
      "Average loss at step 3370:   7.8\n",
      "Average loss at step 3380:   6.8\n",
      "Average loss at step 3390:  10.0\n",
      "Average loss at step 3400:  10.6\n",
      "Average loss at step 3410:   5.2\n",
      "Average loss at step 3420:   5.9\n",
      "Average loss at step 3430:   1.4\n",
      "Average loss at step 3440:   8.5\n",
      "Average loss at step 3450:   4.4\n",
      "Average loss at step 3460:  10.0\n",
      "Average loss at step 3470:  11.9\n",
      "Average loss at step 3480:   9.0\n",
      "Average loss at step 3490:   3.0\n",
      "Average loss at step 3500:   4.8\n",
      "Average loss at step 3510:   1.6\n",
      "Average loss at step 3520:   2.6\n",
      "Average loss at step 3530:   3.4\n",
      "Average loss at step 3540:   4.4\n",
      "Average loss at step 3550:   4.7\n",
      "Average loss at step 3560:   4.2\n",
      "Average loss at step 3570:   5.3\n",
      "Average loss at step 3580:   9.3\n",
      "Average loss at step 3590:   3.7\n",
      "Average loss at step 3600:   7.4\n",
      "Average loss at step 3610:   4.9\n",
      "Average loss at step 3620:   6.5\n",
      "Average loss at step 3630:   7.3\n",
      "Average loss at step 3640:   7.0\n",
      "Average loss at step 3650:   3.7\n",
      "Average loss at step 3660:  12.1\n",
      "Average loss at step 3670:  11.7\n",
      "Average loss at step 3680:   4.6\n",
      "Average loss at step 3690:   7.3\n",
      "Average loss at step 3700:  10.0\n",
      "Average loss at step 3710:   4.2\n",
      "Average loss at step 3720:   9.3\n",
      "Average loss at step 3730:  10.0\n",
      "Average loss at step 3740:   7.5\n",
      "Average loss at step 3750:   4.2\n",
      "Average loss at step 3760:   4.8\n",
      "Average loss at step 3770:  13.8\n",
      "Average loss at step 3780:   8.3\n",
      "Average loss at step 3790:   5.8\n",
      "Average loss at step 3800:   5.2\n",
      "Average loss at step 3810:   5.3\n",
      "Average loss at step 3820:   8.4\n",
      "Average loss at step 3830:   8.5\n",
      "Average loss at step 3840:   5.3\n",
      "Average loss at step 3850:   5.9\n",
      "Average loss at step 3860:  11.4\n",
      "Average loss at step 3870:   6.8\n",
      "Average loss at step 3880:  15.3\n",
      "Average loss at step 3890:   9.1\n",
      "Average loss at step 3900:   5.7\n",
      "Average loss at step 3910:   6.1\n",
      "Average loss at step 3920:   3.9\n",
      "Average loss at step 3930:   5.9\n",
      "Average loss at step 3940:   5.0\n",
      "Average loss at step 3950:   9.5\n",
      "Average loss at step 3960:   4.4\n",
      "Average loss at step 3970:   9.6\n",
      "Average loss at step 3980:  13.1\n",
      "Average loss at step 3990:   6.0\n",
      "Average loss at step 4000:   5.9\n",
      "Average loss at step 4010:   4.6\n",
      "Average loss at step 4020:  13.6\n",
      "Average loss at step 4030:   6.7\n",
      "Average loss at step 4040:   4.0\n",
      "Average loss at step 4050:  10.0\n",
      "Average loss at step 4060:   5.7\n",
      "Average loss at step 4070:   8.7\n",
      "Average loss at step 4080:  10.0\n",
      "Average loss at step 4090:   6.8\n",
      "Average loss at step 4100:  14.3\n",
      "Average loss at step 4110:   7.8\n",
      "Average loss at step 4120:  16.0\n",
      "Average loss at step 4130:  13.7\n",
      "Average loss at step 4140:   3.9\n",
      "Average loss at step 4150:  10.4\n",
      "Average loss at step 4160:   6.7\n",
      "Average loss at step 4170:  10.7\n",
      "Average loss at step 4180:  11.6\n",
      "Average loss at step 4190:   4.6\n",
      "Average loss at step 4200:  11.8\n",
      "Average loss at step 4210:  10.8\n",
      "Average loss at step 4220:  10.9\n",
      "Average loss at step 4230:  12.7\n",
      "Average loss at step 4240:   9.0\n",
      "Average loss at step 4250:   8.5\n",
      "Average loss at step 4260:   9.6\n",
      "Average loss at step 4270:   4.2\n",
      "Average loss at step 4280:   9.8\n",
      "Average loss at step 4290:   4.7\n",
      "Average loss at step 4300:   2.4\n",
      "Average loss at step 4310:   5.8\n",
      "Average loss at step 4320:   6.7\n",
      "Average loss at step 4330:   5.4\n",
      "Average loss at step 4340:   3.4\n",
      "Average loss at step 4350:  14.6\n",
      "Average loss at step 4360:   9.2\n",
      "Average loss at step 4370:   7.6\n",
      "Average loss at step 4380:  12.5\n",
      "Average loss at step 4390:  15.6\n",
      "Average loss at step 4400:   7.2\n",
      "Average loss at step 4410:   7.1\n",
      "Average loss at step 4420:   4.3\n",
      "Average loss at step 4430:  13.9\n",
      "Average loss at step 4440:  12.6\n",
      "Average loss at step 4450:   3.4\n",
      "Average loss at step 4460:  12.4\n",
      "Average loss at step 4470:  10.3\n",
      "Average loss at step 4480:  10.0\n",
      "Average loss at step 4490:  13.5\n",
      "Average loss at step 4500:   5.1\n",
      "Average loss at step 4510:   5.8\n",
      "Average loss at step 4520:   8.5\n",
      "Average loss at step 4530:  11.9\n",
      "Average loss at step 4540:   2.3\n",
      "Average loss at step 4550:   8.3\n",
      "Average loss at step 4560:   9.3\n",
      "Average loss at step 4570:  11.4\n",
      "Average loss at step 4580:   4.1\n",
      "Average loss at step 4590:   4.9\n",
      "Average loss at step 4600:   2.9\n",
      "Average loss at step 4610:   5.3\n",
      "Average loss at step 4620:   7.6\n",
      "Average loss at step 4630:   6.5\n",
      "Average loss at step 4640:   3.5\n",
      "Average loss at step 4650:   6.0\n",
      "Average loss at step 4660:   6.9\n",
      "Average loss at step 4670:   6.9\n",
      "Average loss at step 4680:  19.4\n",
      "Average loss at step 4690:   5.7\n",
      "Average loss at step 4700:  14.1\n",
      "Average loss at step 4710:  12.3\n",
      "Average loss at step 4720:  12.0\n",
      "Average loss at step 4730:   7.9\n",
      "Average loss at step 4740:  21.7\n",
      "Average loss at step 4750:  13.6\n",
      "Average loss at step 4760:   3.7\n",
      "Average loss at step 4770:   8.8\n",
      "Average loss at step 4780:   7.1\n",
      "Average loss at step 4790:  12.0\n",
      "Average loss at step 4800:  10.2\n",
      "Average loss at step 4810:   6.1\n",
      "Average loss at step 4820:   9.5\n",
      "Average loss at step 4830:  10.8\n",
      "Average loss at step 4840:  15.3\n",
      "Average loss at step 4850:   5.0\n",
      "Average loss at step 4860:  12.8\n",
      "Average loss at step 4870:  16.3\n",
      "Average loss at step 4880:  12.7\n",
      "Average loss at step 4890:  10.0\n",
      "Average loss at step 4900:   9.0\n",
      "Average loss at step 4910:  11.9\n",
      "Average loss at step 4920:   7.8\n",
      "Average loss at step 4930:   8.5\n",
      "Average loss at step 4940:   7.5\n",
      "Average loss at step 4950:   6.0\n",
      "Average loss at step 4960:   4.2\n",
      "Average loss at step 4970:   5.5\n",
      "Average loss at step 4980:   6.8\n",
      "Average loss at step 4990:   6.4\n",
      "Average loss at step 5000:   5.6\n",
      "Average loss at step 5010:  10.7\n",
      "Average loss at step 5020:   8.1\n",
      "Average loss at step 5030:  10.1\n",
      "Average loss at step 5040:   8.2\n",
      "Average loss at step 5050:  15.2\n",
      "Average loss at step 5060:  11.9\n",
      "Average loss at step 5070:  13.0\n",
      "Average loss at step 5080:  11.1\n",
      "Average loss at step 5090:   7.2\n",
      "Average loss at step 5100:   9.2\n",
      "Average loss at step 5110:   8.0\n",
      "Average loss at step 5120:  11.0\n",
      "Average loss at step 5130:   4.9\n",
      "Average loss at step 5140:  11.1\n",
      "Average loss at step 5150:   2.9\n",
      "Average loss at step 5160:   9.1\n",
      "Average loss at step 5170:   2.0\n",
      "Average loss at step 5180:   3.1\n",
      "Average loss at step 5190:   2.4\n",
      "Average loss at step 5200:   3.7\n",
      "Average loss at step 5210:   0.9\n",
      "Average loss at step 5220:   4.8\n",
      "Average loss at step 5230:   6.9\n",
      "Average loss at step 5240:   5.1\n",
      "Average loss at step 5250:   5.7\n",
      "Average loss at step 5260:   4.3\n",
      "Average loss at step 5270:   3.0\n",
      "Average loss at step 5280:   6.7\n",
      "Average loss at step 5290:   7.7\n",
      "Average loss at step 5300:   4.8\n",
      "Average loss at step 5310:   5.6\n",
      "Average loss at step 5320:   2.3\n",
      "Average loss at step 5330:   4.1\n",
      "Average loss at step 5340:   4.4\n",
      "Average loss at step 5350:  10.2\n",
      "Average loss at step 5360:   6.1\n",
      "Average loss at step 5370:  11.5\n",
      "Average loss at step 5380:   5.0\n",
      "Average loss at step 5390:   5.0\n",
      "Average loss at step 5400:   5.4\n",
      "Average loss at step 5410:  12.9\n",
      "Average loss at step 5420:   4.6\n",
      "Average loss at step 5430:   8.4\n",
      "Average loss at step 5440:   6.6\n",
      "Average loss at step 5450:   4.9\n",
      "Average loss at step 5460:   9.3\n",
      "Average loss at step 5470:   2.7\n",
      "Average loss at step 5480:   9.3\n",
      "Average loss at step 5490:   9.3\n",
      "Average loss at step 5500:   0.8\n",
      "Average loss at step 5510:   4.5\n",
      "Average loss at step 5520:   7.1\n",
      "Average loss at step 5530:   6.0\n",
      "Average loss at step 5540:   7.5\n",
      "Average loss at step 5550:   7.7\n",
      "Average loss at step 5560:   4.2\n",
      "Average loss at step 5570:   2.5\n",
      "Average loss at step 5580:   6.7\n",
      "Average loss at step 5590:   5.5\n",
      "Average loss at step 5600:   3.6\n",
      "Average loss at step 5610:   7.4\n",
      "Average loss at step 5620:  10.6\n",
      "Average loss at step 5630:   3.8\n",
      "Average loss at step 5640:   2.6\n",
      "Average loss at step 5650:   6.0\n",
      "Average loss at step 5660:   3.0\n",
      "Average loss at step 5670:   3.1\n",
      "Average loss at step 5680:   5.1\n",
      "Average loss at step 5690:   7.0\n",
      "Average loss at step 5700:   5.8\n",
      "Average loss at step 5710:   4.4\n",
      "Average loss at step 5720:   8.4\n",
      "Average loss at step 5730:  11.9\n",
      "Average loss at step 5740:   6.2\n",
      "Average loss at step 5750:   3.8\n",
      "Average loss at step 5760:   6.4\n",
      "Average loss at step 5770:   6.0\n",
      "Average loss at step 5780:   9.7\n",
      "Average loss at step 5790:   7.3\n",
      "Average loss at step 5800:   5.0\n",
      "Average loss at step 5810:  10.6\n",
      "Average loss at step 5820:   3.8\n",
      "Average loss at step 5830:   9.4\n",
      "Average loss at step 5840:   7.3\n",
      "Average loss at step 5850:   5.8\n",
      "Average loss at step 5860:   4.8\n",
      "Average loss at step 5870:   7.1\n",
      "Average loss at step 5880:   2.2\n",
      "Average loss at step 5890:   6.3\n",
      "Average loss at step 5900:   5.2\n",
      "Average loss at step 5910:   4.2\n",
      "Average loss at step 5920:   6.9\n",
      "Average loss at step 5930:   3.9\n",
      "Average loss at step 5940:   4.2\n",
      "Average loss at step 5950:   4.9\n",
      "Average loss at step 5960:   7.5\n",
      "Average loss at step 5970:  11.0\n",
      "Average loss at step 5980:   3.5\n",
      "Average loss at step 5990:   5.1\n",
      "Average loss at step 6000:   5.6\n",
      "Average loss at step 6010:   3.0\n",
      "Average loss at step 6020:   5.9\n",
      "Average loss at step 6030:   2.7\n",
      "Average loss at step 6040:   1.2\n",
      "Average loss at step 6050:   3.0\n",
      "Average loss at step 6060:   2.2\n",
      "Average loss at step 6070:   7.2\n",
      "Average loss at step 6080:   4.5\n",
      "Average loss at step 6090:   5.7\n",
      "Average loss at step 6100:   1.6\n",
      "Average loss at step 6110:   5.8\n",
      "Average loss at step 6120:   4.8\n",
      "Average loss at step 6130:   7.0\n",
      "Average loss at step 6140:   5.8\n",
      "Average loss at step 6150:   3.2\n",
      "Average loss at step 6160:   4.1\n",
      "Average loss at step 6170:   5.1\n",
      "Average loss at step 6180:  10.0\n",
      "Average loss at step 6190:   4.0\n",
      "Average loss at step 6200:   6.7\n",
      "Average loss at step 6210:   3.4\n",
      "Average loss at step 6220:   3.2\n",
      "Average loss at step 6230:   0.8\n",
      "Average loss at step 6240:   4.6\n",
      "Average loss at step 6250:   7.6\n",
      "Average loss at step 6260:   1.6\n",
      "Average loss at step 6270:  11.9\n",
      "Average loss at step 6280:   7.9\n",
      "Average loss at step 6290:   3.9\n",
      "Average loss at step 6300:   2.3\n",
      "Average loss at step 6310:   4.6\n",
      "Average loss at step 6320:   5.7\n",
      "Average loss at step 6330:   2.6\n",
      "Average loss at step 6340:   3.3\n",
      "Average loss at step 6350:   1.8\n",
      "Average loss at step 6360:   3.6\n",
      "Average loss at step 6370:   6.4\n",
      "Average loss at step 6380:   4.0\n",
      "Average loss at step 6390:   7.7\n",
      "Average loss at step 6400:   4.7\n",
      "Average loss at step 6410:   5.6\n",
      "Average loss at step 6420:   7.1\n",
      "Average loss at step 6430:   1.4\n",
      "Average loss at step 6440:  10.2\n",
      "Average loss at step 6450:   4.7\n",
      "Average loss at step 6460:   1.9\n",
      "Average loss at step 6470:   2.6\n",
      "Average loss at step 6480:   5.7\n",
      "Average loss at step 6490:   3.9\n",
      "Average loss at step 6500:   2.9\n",
      "Average loss at step 6510:   6.5\n",
      "Average loss at step 6520:   2.7\n",
      "Average loss at step 6530:   2.7\n",
      "Average loss at step 6540:   3.3\n",
      "Average loss at step 6550:   4.0\n",
      "Average loss at step 6560:   1.8\n",
      "Average loss at step 6570:   3.0\n",
      "Average loss at step 6580:   4.1\n",
      "Average loss at step 6590:   5.6\n",
      "Average loss at step 6600:   8.5\n",
      "Average loss at step 6610:   3.3\n",
      "Average loss at step 6620:   5.4\n",
      "Average loss at step 6630:   3.4\n",
      "Average loss at step 6640:   8.5\n",
      "Average loss at step 6650:   8.1\n",
      "Average loss at step 6660:   4.7\n",
      "Average loss at step 6670:   4.9\n",
      "Average loss at step 6680:   5.2\n",
      "Average loss at step 6690:   2.7\n",
      "Average loss at step 6700:   2.8\n",
      "Average loss at step 6710:   5.1\n",
      "Average loss at step 6720:   3.3\n",
      "Average loss at step 6730:   5.1\n",
      "Average loss at step 6740:   3.9\n",
      "Average loss at step 6750:   2.1\n",
      "Average loss at step 6760:   7.6\n",
      "Average loss at step 6770:   4.5\n",
      "Average loss at step 6780:   6.8\n",
      "Average loss at step 6790:   4.7\n",
      "Average loss at step 6800:   4.2\n",
      "Average loss at step 6810:   4.2\n",
      "Average loss at step 6820:   5.3\n",
      "Average loss at step 6830:   5.2\n",
      "Average loss at step 6840:   6.7\n",
      "Average loss at step 6850:   1.9\n",
      "Average loss at step 6860:   2.8\n",
      "Average loss at step 6870:   3.6\n",
      "Average loss at step 6880:   3.1\n",
      "Average loss at step 6890:   4.1\n",
      "Average loss at step 6900:   2.3\n",
      "Average loss at step 6910:   3.6\n",
      "Average loss at step 6920:   3.1\n",
      "Average loss at step 6930:   2.6\n",
      "Average loss at step 6940:   1.7\n",
      "Average loss at step 6950:   5.3\n",
      "Average loss at step 6960:   3.3\n",
      "Average loss at step 6970:   3.2\n",
      "Average loss at step 6980:   6.6\n",
      "Average loss at step 6990:   2.6\n",
      "Average loss at step 7000:   6.0\n",
      "Average loss at step 7010:   3.8\n",
      "Average loss at step 7020:   2.8\n",
      "Average loss at step 7030:   3.4\n",
      "Average loss at step 7040:   3.6\n",
      "Average loss at step 7050:   1.9\n",
      "Average loss at step 7060:   3.9\n",
      "Average loss at step 7070:   2.9\n",
      "Average loss at step 7080:   2.7\n",
      "Average loss at step 7090:   3.6\n",
      "Average loss at step 7100:   7.2\n",
      "Average loss at step 7110:   4.7\n",
      "Average loss at step 7120:   1.7\n",
      "Average loss at step 7130:   4.1\n",
      "Average loss at step 7140:   2.7\n",
      "Average loss at step 7150:   5.4\n",
      "Average loss at step 7160:   3.0\n",
      "Average loss at step 7170:   6.3\n",
      "Average loss at step 7180:   1.8\n",
      "Average loss at step 7190:   2.6\n",
      "Average loss at step 7200:   2.4\n",
      "Average loss at step 7210:   9.5\n",
      "Average loss at step 7220:   4.6\n",
      "Average loss at step 7230:   5.9\n",
      "Average loss at step 7240:   2.4\n",
      "Average loss at step 7250:   5.0\n",
      "Average loss at step 7260:   4.7\n",
      "Average loss at step 7270:   2.6\n",
      "Average loss at step 7280:   5.8\n",
      "Average loss at step 7290:   2.2\n",
      "Average loss at step 7300:   2.5\n",
      "Average loss at step 7310:   4.8\n",
      "Average loss at step 7320:   2.4\n",
      "Average loss at step 7330:   3.8\n",
      "Average loss at step 7340:   5.9\n",
      "Average loss at step 7350:   2.8\n",
      "Average loss at step 7360:   4.9\n",
      "Average loss at step 7370:   3.5\n",
      "Average loss at step 7380:   2.8\n",
      "Average loss at step 7390:   6.3\n",
      "Average loss at step 7400:   2.2\n",
      "Average loss at step 7410:   7.0\n",
      "Average loss at step 7420:   6.3\n",
      "Average loss at step 7430:   7.5\n",
      "Average loss at step 7440:   4.3\n",
      "Average loss at step 7450:   8.1\n",
      "Average loss at step 7460:   8.1\n",
      "Average loss at step 7470:   5.5\n",
      "Average loss at step 7480:   4.3\n",
      "Average loss at step 7490:   7.2\n",
      "Average loss at step 7500:   3.5\n",
      "Average loss at step 7510:   7.5\n",
      "Average loss at step 7520:   2.6\n",
      "Average loss at step 7530:   6.4\n",
      "Average loss at step 7540:   2.8\n",
      "Average loss at step 7550:   5.7\n",
      "Average loss at step 7560:   7.0\n",
      "Average loss at step 7570:   5.4\n",
      "Average loss at step 7580:   1.3\n",
      "Average loss at step 7590:   4.6\n",
      "Average loss at step 7600:   3.6\n",
      "Average loss at step 7610:   5.0\n",
      "Average loss at step 7620:   3.5\n",
      "Average loss at step 7630:   4.7\n",
      "Average loss at step 7640:   3.5\n",
      "Average loss at step 7650:   2.1\n",
      "Average loss at step 7660:   3.9\n",
      "Average loss at step 7670:   4.7\n",
      "Average loss at step 7680:   4.2\n",
      "Average loss at step 7690:   3.3\n",
      "Average loss at step 7700:   2.1\n",
      "Average loss at step 7710:   0.6\n",
      "Average loss at step 7720:   3.3\n",
      "Average loss at step 7730:   4.9\n",
      "Average loss at step 7740:   5.1\n",
      "Average loss at step 7750:   5.7\n",
      "Average loss at step 7760:   4.5\n",
      "Average loss at step 7770:   5.2\n",
      "Average loss at step 7780:   3.4\n",
      "Average loss at step 7790:   3.5\n",
      "Average loss at step 7800:   5.4\n",
      "Average loss at step 7810:   5.2\n",
      "Average loss at step 7820:   2.1\n",
      "Average loss at step 7830:   5.7\n",
      "Average loss at step 7840:   2.5\n",
      "Average loss at step 7850:   1.6\n",
      "Average loss at step 7860:   1.6\n",
      "Average loss at step 7870:   2.4\n",
      "Average loss at step 7880:   2.7\n",
      "Average loss at step 7890:   2.1\n",
      "Average loss at step 7900:   3.6\n",
      "Average loss at step 7910:   3.4\n",
      "Average loss at step 7920:   1.9\n",
      "Average loss at step 7930:   2.7\n",
      "Average loss at step 7940:   2.5\n",
      "Average loss at step 7950:   4.3\n",
      "Average loss at step 7960:   4.6\n",
      "Average loss at step 7970:   2.3\n",
      "Average loss at step 7980:   2.2\n",
      "Average loss at step 7990:   3.6\n",
      "Average loss at step 8000:   4.2\n",
      "Average loss at step 8010:   4.8\n",
      "Average loss at step 8020:   2.9\n",
      "Average loss at step 8030:   5.1\n",
      "Average loss at step 8040:   4.3\n",
      "Average loss at step 8050:   4.8\n",
      "Average loss at step 8060:   4.2\n",
      "Average loss at step 8070:   6.4\n",
      "Average loss at step 8080:   4.9\n",
      "Average loss at step 8090:   9.1\n",
      "Average loss at step 8100:   6.4\n",
      "Average loss at step 8110:   3.7\n",
      "Average loss at step 8120:   5.0\n",
      "Average loss at step 8130:   0.9\n",
      "Average loss at step 8140:   1.5\n",
      "Average loss at step 8150:   1.6\n",
      "Average loss at step 8160:   2.4\n",
      "Average loss at step 8170:   2.4\n",
      "Average loss at step 8180:   5.9\n",
      "Average loss at step 8190:   4.2\n",
      "Average loss at step 8200:  10.4\n",
      "Average loss at step 8210:   6.2\n",
      "Average loss at step 8220:   2.8\n",
      "Average loss at step 8230:   2.1\n",
      "Average loss at step 8240:   3.8\n",
      "Average loss at step 8250:   4.8\n",
      "Average loss at step 8260:   1.3\n",
      "Average loss at step 8270:   6.1\n",
      "Average loss at step 8280:   5.1\n",
      "Average loss at step 8290:   3.9\n",
      "Average loss at step 8300:   7.3\n",
      "Average loss at step 8310:   2.8\n",
      "Average loss at step 8320:   5.3\n",
      "Average loss at step 8330:   5.4\n",
      "Average loss at step 8340:   2.7\n",
      "Average loss at step 8350:   6.1\n",
      "Average loss at step 8360:   0.8\n",
      "Average loss at step 8370:   4.5\n",
      "Average loss at step 8380:   2.5\n",
      "Average loss at step 8390:   2.8\n",
      "Average loss at step 8400:   2.9\n",
      "Average loss at step 8410:   1.4\n",
      "Average loss at step 8420:   2.0\n",
      "Average loss at step 8430:   2.4\n",
      "Average loss at step 8440:   3.2\n",
      "Average loss at step 8450:   6.4\n",
      "Average loss at step 8460:   4.0\n",
      "Average loss at step 8470:   3.0\n",
      "Average loss at step 8480:   2.8\n",
      "Average loss at step 8490:   4.0\n",
      "Average loss at step 8500:   6.5\n",
      "Average loss at step 8510:   0.5\n",
      "Average loss at step 8520:   2.6\n",
      "Average loss at step 8530:   4.2\n",
      "Average loss at step 8540:   3.8\n",
      "Average loss at step 8550:   6.2\n",
      "Average loss at step 8560:   3.1\n",
      "Average loss at step 8570:   4.5\n",
      "Average loss at step 8580:   4.2\n",
      "Average loss at step 8590:   4.3\n",
      "Optimization Finished!\n",
      "Total time: 1477.5036838054657 seconds\n",
      "Accuracy 0.9672\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "N_CLASSES = 10\n",
    "\n",
    "# Step 2: Define paramaters for the model\n",
    "LEARNING_RATE = 0.01\n",
    "BATCH_SIZE = 64\n",
    "SKIP_STEP = 10\n",
    "DROPOUT = 0.80\n",
    "N_EPOCHS = 10\n",
    "\n",
    "\n",
    "with tf.name_scope('data'):\n",
    "    X = tf.placeholder(tf.float32, [None, 784], name=\"X_placeholder\")\n",
    "    Y = tf.placeholder(tf.float32, [None, 10], name=\"Y_placeholder\")\n",
    "\n",
    "dropout = tf.placeholder(tf.float32, name='dropout')\n",
    "\n",
    "\n",
    "global_step = tf.Variable(0, dtype=tf.int32, trainable=False, name='global_step')\n",
    "\n",
    "\n",
    "\n",
    "with tf.variable_scope('conv1') as scope:\n",
    "    \n",
    "    images = tf.reshape(X, shape=[-1, 28, 28, 1])\n",
    "    \n",
    "    \n",
    "    \n",
    "    kernel = tf.get_variable('kernel', shape=[5, 5, 1, 32], initializer=tf.truncated_normal_initializer())\n",
    "    \n",
    "   \n",
    "    \n",
    "    biases = tf.get_variable('biases', shape=[32], initializer=tf.constant_initializer(0.0))\n",
    "    \n",
    "   \n",
    "    conv = tf.nn.conv2d(images, kernel, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    \n",
    "   \n",
    "    conv1 = tf.nn.relu(conv + biases, name='conv1')\n",
    "    \n",
    "    \n",
    "    \n",
    "with tf.variable_scope('pool1') as scope:\n",
    "   \n",
    "    \n",
    "    pool1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], name='pool1', strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "    \n",
    "\n",
    "with tf.variable_scope('conv2') as scope:\n",
    "\n",
    "    kernel = tf.get_variable('kernels', [5, 5, 32, 64], \n",
    "                        initializer=tf.truncated_normal_initializer())\n",
    "    biases = tf.get_variable('biases', [64],\n",
    "                        initializer=tf.random_normal_initializer())\n",
    "    conv = tf.nn.conv2d(pool1, kernel, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    conv2 = tf.nn.relu(conv + biases, name=scope.name)\n",
    "\n",
    "\n",
    "with tf.variable_scope('pool2') as scope:\n",
    "\n",
    "    pool2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1],\n",
    "                            padding='SAME')\n",
    "\n",
    "   \n",
    "    \n",
    "\n",
    "with tf.variable_scope('fc') as scope:\n",
    "\n",
    "    input_features = 7 * 7 * 64\n",
    "    \n",
    "   \n",
    "    w = tf.get_variable('weights', shape=[input_features, 1024], initializer=tf.truncated_normal_initializer())\n",
    "    b = tf.get_variable('bias', shape=[1024], initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "    pool2 = tf.reshape(pool2, [-1, input_features])\n",
    "\n",
    "    fc = tf.nn.relu(tf.matmul(pool2, w) + b, name='relu')\n",
    "    \n",
    "   \n",
    "    fc = tf.nn.dropout(fc, dropout, name='relu_dropout')\n",
    "\n",
    "with tf.variable_scope('softmax_linear') as scope:\n",
    "    \n",
    "    w = tf.get_variable('weights', shape=[1024, N_CLASSES], initializer=tf.truncated_normal_initializer())\n",
    "    b = tf.get_variable('bias', shape=[N_CLASSES], initializer=tf.constant_initializer(0.0))\n",
    "    \n",
    "    logits = tf.matmul(fc, w) + b\n",
    "\n",
    "\n",
    "    \n",
    "with tf.name_scope('loss'):\n",
    "   \n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=Y, logits=logits), name='loss')\n",
    "\n",
    "\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE).minimize(loss, global_step=global_step)\n",
    "\n",
    "with tf.name_scope('summary'):\n",
    "    tf.summary.scalar('loss', loss)\n",
    "    summary_op = tf.summary.merge_all()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    writer = tf.summary.FileWriter('./my_graph/mnist', sess.graph)\n",
    "   \n",
    "    \n",
    "    initial_step = global_step.eval()\n",
    "\n",
    "    start_time = time.time()\n",
    "    n_batches = int(mnist.train.num_examples / BATCH_SIZE)\n",
    "\n",
    "    total_loss = 0.0\n",
    "    for index in range(initial_step, n_batches * N_EPOCHS): # train the model n_epochs times\n",
    "        X_batch, Y_batch = mnist.train.next_batch(BATCH_SIZE)\n",
    "        _, loss_batch, summary = sess.run([optimizer, loss, summary_op], \n",
    "                                feed_dict={X: X_batch, Y:Y_batch, dropout: DROPOUT}) \n",
    "        total_loss += loss_batch\n",
    "        writer.add_summary(summary, global_step=index)\n",
    "        if (index + 1) % SKIP_STEP == 0:\n",
    "            print('Average loss at step {}: {:5.1f}'.format(index + 1, total_loss / SKIP_STEP))\n",
    "            total_loss = 0.0\n",
    "            saver.save(sess, '../checkpoints/convnet_mnist/mnist-convnet', index)\n",
    "    \n",
    "    print(\"Optimization Finished!\") # should be around 0.35 after 25 epochs\n",
    "    print(\"Total time: {0} seconds\".format(time.time() - start_time))\n",
    "    \n",
    "    # test the model\n",
    "    n_batches = int(mnist.test.num_examples/BATCH_SIZE)\n",
    "    total_correct_preds = 0\n",
    "    for i in range(n_batches):\n",
    "        X_batch, Y_batch = mnist.test.next_batch(BATCH_SIZE)\n",
    "        _, loss_batch, logits_batch = sess.run([optimizer, loss, logits], \n",
    "                                        feed_dict={X: X_batch, Y:Y_batch, dropout: DROPOUT}) \n",
    "        preds = tf.nn.softmax(logits_batch)\n",
    "        correct_preds = tf.equal(tf.argmax(preds, 1), tf.argmax(Y_batch, 1))\n",
    "        accuracy = tf.reduce_sum(tf.cast(correct_preds, tf.float32))\n",
    "        total_correct_preds += sess.run(accuracy)   \n",
    "    \n",
    "    print(\"Accuracy {0}\".format(total_correct_preds/mnist.test.num_examples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (10 epochs) two conv nets then one fc with relu and dropout -> Accuracy 0.9665 - 0.9672 - 0.97"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((55000, 784), (10000, 784))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.images.shape, mnist.test.images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 10: 209184.9\n",
      "Average loss at step 20: 16662.1\n",
      "Average loss at step 30: 3337.3\n",
      "Average loss at step 40: 1497.0\n",
      "Average loss at step 50: 845.5\n",
      "Average loss at step 60: 599.0\n",
      "Average loss at step 70: 460.2\n",
      "Average loss at step 80: 358.0\n",
      "Average loss at step 90: 282.3\n",
      "Average loss at step 100: 249.6\n",
      "Average loss at step 110: 190.0\n",
      "Average loss at step 120: 174.3\n",
      "Average loss at step 130: 144.9\n",
      "Average loss at step 140: 127.8\n",
      "Average loss at step 150: 102.1\n",
      "Average loss at step 160: 120.0\n",
      "Average loss at step 170:  87.0\n",
      "Average loss at step 180:  89.7\n",
      "Average loss at step 190:  78.6\n",
      "Average loss at step 200:  67.3\n",
      "Average loss at step 210:  51.7\n",
      "Average loss at step 220:  55.4\n",
      "Average loss at step 230:  53.6\n",
      "Average loss at step 240:  60.7\n",
      "Average loss at step 250:  51.2\n",
      "Average loss at step 260:  42.5\n",
      "Average loss at step 270:  49.7\n",
      "Average loss at step 280:  42.5\n",
      "Average loss at step 290:  33.2\n",
      "Average loss at step 300:  34.6\n",
      "Average loss at step 310:  34.9\n",
      "Average loss at step 320:  32.5\n",
      "Average loss at step 330:  24.4\n",
      "Average loss at step 340:  23.6\n",
      "Average loss at step 350:  33.5\n",
      "Average loss at step 360:  21.1\n",
      "Average loss at step 370:  31.2\n",
      "Average loss at step 380:  24.8\n",
      "Average loss at step 390:  23.6\n",
      "Average loss at step 400:  21.3\n",
      "Average loss at step 410:  19.5\n",
      "Average loss at step 420:  15.9\n",
      "Average loss at step 430:  16.5\n",
      "Average loss at step 440:  14.2\n",
      "Average loss at step 450:  20.4\n",
      "Average loss at step 460:  21.0\n",
      "Average loss at step 470:  18.7\n",
      "Average loss at step 480:  16.1\n",
      "Average loss at step 490:  12.5\n",
      "Average loss at step 500:  13.1\n",
      "Average loss at step 510:  22.3\n",
      "Average loss at step 520:  11.9\n",
      "Average loss at step 530:  14.6\n",
      "Average loss at step 540:  19.2\n",
      "Average loss at step 550:  12.1\n",
      "Average loss at step 560:  12.7\n",
      "Average loss at step 570:  12.0\n",
      "Average loss at step 580:  10.9\n",
      "Average loss at step 590:  10.3\n",
      "Average loss at step 600:  11.9\n",
      "Average loss at step 610:  13.4\n",
      "Average loss at step 620:  10.4\n",
      "Average loss at step 630:   6.9\n",
      "Average loss at step 640:   9.6\n",
      "Average loss at step 650:  12.4\n",
      "Average loss at step 660:   9.1\n",
      "Average loss at step 670:   6.1\n",
      "Average loss at step 680:  11.1\n",
      "Average loss at step 690:  10.0\n",
      "Average loss at step 700:   9.6\n",
      "Average loss at step 710:  11.1\n",
      "Average loss at step 720:   6.3\n",
      "Average loss at step 730:   8.4\n",
      "Average loss at step 740:   9.7\n",
      "Average loss at step 750:   8.4\n",
      "Average loss at step 760:   7.2\n",
      "Average loss at step 770:   8.7\n",
      "Average loss at step 780:   5.6\n",
      "Average loss at step 790:   4.6\n",
      "Average loss at step 800:   7.9\n",
      "Average loss at step 810:   7.8\n",
      "Average loss at step 820:   6.7\n",
      "Average loss at step 830:   6.0\n",
      "Average loss at step 840:   4.2\n",
      "Average loss at step 850:   9.7\n",
      "Average loss at step 860:   6.8\n",
      "Average loss at step 870:   5.8\n",
      "Average loss at step 880:   7.2\n",
      "Average loss at step 890:   5.9\n",
      "Average loss at step 900:   6.0\n",
      "Average loss at step 910:   7.8\n",
      "Average loss at step 920:   5.6\n",
      "Average loss at step 930:   6.9\n",
      "Average loss at step 940:   5.2\n",
      "Average loss at step 950:   6.2\n",
      "Average loss at step 960:   4.7\n",
      "Average loss at step 970:   7.1\n",
      "Average loss at step 980:   6.3\n",
      "Average loss at step 990:   6.7\n",
      "Average loss at step 1000:   6.9\n",
      "Average loss at step 1010:   4.7\n",
      "Average loss at step 1020:   5.5\n",
      "Average loss at step 1030:   4.8\n",
      "Average loss at step 1040:   5.7\n",
      "Average loss at step 1050:   5.2\n",
      "Average loss at step 1060:   5.1\n",
      "Average loss at step 1070:   6.0\n",
      "Average loss at step 1080:   4.9\n",
      "Average loss at step 1090:   9.6\n",
      "Average loss at step 1100:   5.6\n",
      "Average loss at step 1110:   4.8\n",
      "Average loss at step 1120:   3.7\n",
      "Average loss at step 1130:   4.0\n",
      "Average loss at step 1140:   4.1\n",
      "Average loss at step 1150:   4.2\n",
      "Average loss at step 1160:   4.8\n",
      "Average loss at step 1170:   3.3\n",
      "Average loss at step 1180:   4.8\n",
      "Average loss at step 1190:   5.0\n",
      "Average loss at step 1200:   4.1\n",
      "Average loss at step 1210:   2.9\n",
      "Average loss at step 1220:   4.7\n",
      "Average loss at step 1230:   2.8\n",
      "Average loss at step 1240:   5.6\n",
      "Average loss at step 1250:   4.7\n",
      "Average loss at step 1260:   5.0\n",
      "Average loss at step 1270:   4.8\n",
      "Average loss at step 1280:   3.6\n",
      "Average loss at step 1290:   4.2\n",
      "Average loss at step 1300:   5.2\n",
      "Average loss at step 1310:   4.8\n",
      "Average loss at step 1320:   5.1\n",
      "Average loss at step 1330:   4.0\n",
      "Average loss at step 1340:   3.8\n",
      "Average loss at step 1350:   3.9\n",
      "Average loss at step 1360:   3.1\n",
      "Average loss at step 1370:   5.4\n",
      "Average loss at step 1380:   6.3\n",
      "Average loss at step 1390:   3.8\n",
      "Average loss at step 1400:   6.4\n",
      "Average loss at step 1410:   4.3\n",
      "Average loss at step 1420:   4.0\n",
      "Average loss at step 1430:   3.7\n",
      "Average loss at step 1440:   3.3\n",
      "Average loss at step 1450:   5.1\n",
      "Average loss at step 1460:   4.8\n",
      "Average loss at step 1470:   3.6\n",
      "Average loss at step 1480:   4.5\n",
      "Average loss at step 1490:   5.3\n",
      "Average loss at step 1500:   4.3\n",
      "Average loss at step 1510:   4.0\n",
      "Average loss at step 1520:   4.3\n",
      "Average loss at step 1530:   3.6\n",
      "Average loss at step 1540:   3.6\n",
      "Average loss at step 1550:   3.4\n",
      "Average loss at step 1560:   3.4\n",
      "Average loss at step 1570:   6.1\n",
      "Average loss at step 1580:   2.8\n",
      "Average loss at step 1590:   3.2\n",
      "Average loss at step 1600:   5.7\n",
      "Average loss at step 1610:   3.4\n",
      "Average loss at step 1620:   2.9\n",
      "Average loss at step 1630:   2.8\n",
      "Average loss at step 1640:   3.3\n",
      "Average loss at step 1650:   2.9\n",
      "Average loss at step 1660:   2.8\n",
      "Average loss at step 1670:   3.7\n",
      "Average loss at step 1680:   2.4\n",
      "Average loss at step 1690:   3.0\n",
      "Average loss at step 1700:   2.6\n",
      "Average loss at step 1710:   3.9\n",
      "Average loss at step 1720:   3.1\n",
      "Average loss at step 1730:   3.7\n",
      "Average loss at step 1740:   3.8\n",
      "Average loss at step 1750:   3.4\n",
      "Average loss at step 1760:   4.3\n",
      "Average loss at step 1770:   2.5\n",
      "Average loss at step 1780:   3.4\n",
      "Average loss at step 1790:   2.7\n",
      "Average loss at step 1800:   2.9\n",
      "Average loss at step 1810:   3.8\n",
      "Average loss at step 1820:   2.5\n",
      "Average loss at step 1830:   2.6\n",
      "Average loss at step 1840:   3.6\n",
      "Average loss at step 1850:   3.3\n",
      "Average loss at step 1860:   3.5\n",
      "Average loss at step 1870:   2.8\n",
      "Average loss at step 1880:   3.0\n",
      "Average loss at step 1890:   3.2\n",
      "Average loss at step 1900:   3.1\n",
      "Average loss at step 1910:   2.8\n",
      "Average loss at step 1920:   3.1\n",
      "Average loss at step 1930:   2.6\n",
      "Average loss at step 1940:   2.8\n",
      "Average loss at step 1950:   3.3\n",
      "Average loss at step 1960:   2.5\n",
      "Average loss at step 1970:   2.4\n",
      "Average loss at step 1980:   2.7\n",
      "Average loss at step 1990:   4.0\n",
      "Average loss at step 2000:   3.0\n",
      "Average loss at step 2010:   2.6\n",
      "Average loss at step 2020:   2.9\n",
      "Average loss at step 2030:   3.3\n",
      "Average loss at step 2040:   4.0\n",
      "Average loss at step 2050:   2.9\n",
      "Average loss at step 2060:   2.8\n",
      "Average loss at step 2070:   3.0\n",
      "Average loss at step 2080:   2.7\n",
      "Average loss at step 2090:   4.2\n",
      "Average loss at step 2100:   2.7\n",
      "Average loss at step 2110:   4.3\n",
      "Average loss at step 2120:   2.7\n",
      "Average loss at step 2130:   3.3\n",
      "Average loss at step 2140:   2.9\n",
      "Average loss at step 2150:   2.8\n",
      "Average loss at step 2160:   3.0\n",
      "Average loss at step 2170:   2.8\n",
      "Average loss at step 2180:   3.5\n",
      "Average loss at step 2190:   3.6\n",
      "Average loss at step 2200:   3.1\n",
      "Average loss at step 2210:   2.9\n",
      "Average loss at step 2220:   3.0\n",
      "Average loss at step 2230:   2.8\n",
      "Average loss at step 2240:   2.7\n",
      "Average loss at step 2250:   2.8\n",
      "Average loss at step 2260:   3.1\n",
      "Average loss at step 2270:   2.7\n",
      "Average loss at step 2280:   3.0\n",
      "Average loss at step 2290:   3.0\n",
      "Average loss at step 2300:   2.6\n",
      "Average loss at step 2310:   4.0\n",
      "Average loss at step 2320:   3.2\n",
      "Average loss at step 2330:   3.0\n",
      "Average loss at step 2340:   4.2\n",
      "Average loss at step 2350:   2.7\n",
      "Average loss at step 2360:   3.1\n",
      "Average loss at step 2370:   2.7\n",
      "Average loss at step 2380:   2.5\n",
      "Average loss at step 2390:   2.3\n",
      "Average loss at step 2400:   2.8\n",
      "Average loss at step 2410:   3.2\n",
      "Average loss at step 2420:   2.8\n",
      "Average loss at step 2430:   3.3\n",
      "Average loss at step 2440:   2.4\n",
      "Average loss at step 2450:   3.2\n",
      "Average loss at step 2460:   4.1\n",
      "Average loss at step 2470:   2.4\n",
      "Average loss at step 2480:   3.1\n",
      "Average loss at step 2490:   2.6\n",
      "Average loss at step 2500:   2.3\n",
      "Average loss at step 2510:   3.0\n",
      "Average loss at step 2520:   3.1\n",
      "Average loss at step 2530:   2.9\n",
      "Average loss at step 2540:   2.5\n",
      "Average loss at step 2550:   3.0\n",
      "Average loss at step 2560:   2.6\n",
      "Average loss at step 2570:   2.3\n",
      "Average loss at step 2580:   2.8\n",
      "Average loss at step 2590:   2.9\n",
      "Average loss at step 2600:   3.2\n",
      "Average loss at step 2610:   2.5\n",
      "Average loss at step 2620:   2.3\n",
      "Average loss at step 2630:   2.4\n",
      "Average loss at step 2640:   2.6\n",
      "Average loss at step 2650:   3.3\n",
      "Average loss at step 2660:   3.6\n",
      "Average loss at step 2670:   3.0\n",
      "Average loss at step 2680:   2.4\n",
      "Average loss at step 2690:   3.1\n",
      "Average loss at step 2700:   2.9\n",
      "Average loss at step 2710:   2.5\n",
      "Average loss at step 2720:   3.0\n",
      "Average loss at step 2730:   2.5\n",
      "Average loss at step 2740:   2.4\n",
      "Average loss at step 2750:   2.9\n",
      "Average loss at step 2760:   2.9\n",
      "Average loss at step 2770:   3.2\n",
      "Average loss at step 2780:   2.5\n",
      "Average loss at step 2790:   2.3\n",
      "Average loss at step 2800:   3.0\n",
      "Average loss at step 2810:   3.2\n",
      "Average loss at step 2820:   3.0\n",
      "Average loss at step 2830:   2.8\n",
      "Average loss at step 2840:   2.6\n",
      "Average loss at step 2850:   2.5\n",
      "Average loss at step 2860:   2.4\n",
      "Average loss at step 2870:   2.3\n",
      "Average loss at step 2880:   2.3\n",
      "Average loss at step 2890:   2.6\n",
      "Average loss at step 2900:   2.4\n",
      "Average loss at step 2910:   2.3\n",
      "Average loss at step 2920:   2.8\n",
      "Average loss at step 2930:   3.4\n",
      "Average loss at step 2940:   2.6\n",
      "Average loss at step 2950:   2.3\n",
      "Average loss at step 2960:   4.0\n",
      "Average loss at step 2970:   2.7\n",
      "Average loss at step 2980:   2.3\n",
      "Average loss at step 2990:   3.2\n",
      "Average loss at step 3000:   2.3\n",
      "Average loss at step 3010:   2.5\n",
      "Average loss at step 3020:   3.1\n",
      "Average loss at step 3030:   2.3\n",
      "Average loss at step 3040:   2.6\n",
      "Average loss at step 3050:   2.6\n",
      "Average loss at step 3060:   2.3\n",
      "Average loss at step 3070:   2.5\n",
      "Average loss at step 3080:   2.6\n",
      "Average loss at step 3090:   3.2\n",
      "Average loss at step 3100:   2.3\n",
      "Average loss at step 3110:   2.9\n",
      "Average loss at step 3120:   2.4\n",
      "Average loss at step 3130:   2.6\n",
      "Average loss at step 3140:   3.0\n",
      "Average loss at step 3150:   2.3\n",
      "Average loss at step 3160:   2.9\n",
      "Average loss at step 3170:   2.4\n",
      "Average loss at step 3180:   2.9\n",
      "Average loss at step 3190:   2.4\n",
      "Average loss at step 3200:   2.7\n",
      "Average loss at step 3210:   3.0\n",
      "Average loss at step 3220:   2.3\n",
      "Average loss at step 3230:   2.8\n",
      "Average loss at step 3240:   3.0\n",
      "Average loss at step 3250:   2.3\n",
      "Average loss at step 3260:   2.3\n",
      "Average loss at step 3270:   2.5\n",
      "Average loss at step 3280:   2.4\n",
      "Average loss at step 3290:   3.0\n",
      "Average loss at step 3300:   2.3\n",
      "Average loss at step 3310:   2.3\n",
      "Average loss at step 3320:   2.4\n",
      "Average loss at step 3330:   2.5\n",
      "Average loss at step 3340:   2.3\n",
      "Average loss at step 3350:   3.1\n",
      "Average loss at step 3360:   2.7\n",
      "Average loss at step 3370:   3.1\n",
      "Average loss at step 3380:   2.4\n",
      "Average loss at step 3390:   2.9\n",
      "Average loss at step 3400:   3.6\n",
      "Average loss at step 3410:   3.3\n",
      "Average loss at step 3420:   2.8\n",
      "Average loss at step 3430:   2.5\n",
      "Average loss at step 3440:   2.3\n",
      "Average loss at step 3450:   2.4\n",
      "Average loss at step 3460:   2.3\n",
      "Average loss at step 3470:   2.4\n",
      "Average loss at step 3480:   2.8\n",
      "Average loss at step 3490:   2.7\n",
      "Average loss at step 3500:   2.3\n",
      "Average loss at step 3510:   2.3\n",
      "Average loss at step 3520:   2.4\n",
      "Average loss at step 3530:   2.5\n",
      "Average loss at step 3540:   2.4\n",
      "Average loss at step 3550:   3.3\n",
      "Average loss at step 3560:   2.3\n",
      "Average loss at step 3570:   2.3\n",
      "Average loss at step 3580:   3.4\n",
      "Average loss at step 3590:   2.3\n",
      "Average loss at step 3600:   2.3\n",
      "Average loss at step 3610:   3.0\n",
      "Average loss at step 3620:   2.3\n",
      "Average loss at step 3630:   2.9\n",
      "Average loss at step 3640:   2.9\n",
      "Average loss at step 3650:   2.3\n",
      "Average loss at step 3660:   2.3\n",
      "Average loss at step 3670:   2.9\n",
      "Average loss at step 3680:   2.3\n",
      "Average loss at step 3690:   2.4\n",
      "Average loss at step 3700:   2.3\n",
      "Average loss at step 3710:   2.5\n",
      "Average loss at step 3720:   4.2\n",
      "Average loss at step 3730:   2.3\n",
      "Average loss at step 3740:   2.5\n",
      "Average loss at step 3750:   2.3\n",
      "Average loss at step 3760:   2.3\n",
      "Average loss at step 3770:   2.5\n",
      "Average loss at step 3780:   2.5\n",
      "Average loss at step 3790:   2.8\n",
      "Average loss at step 3800:   2.8\n",
      "Average loss at step 3810:   2.3\n",
      "Average loss at step 3820:   2.3\n",
      "Average loss at step 3830:   2.3\n",
      "Average loss at step 3840:   2.9\n",
      "Average loss at step 3850:   2.6\n",
      "Average loss at step 3860:   2.4\n",
      "Average loss at step 3870:   2.3\n",
      "Average loss at step 3880:   2.3\n",
      "Average loss at step 3890:   2.4\n",
      "Average loss at step 3900:   2.3\n",
      "Average loss at step 3910:   2.4\n",
      "Average loss at step 3920:   2.9\n",
      "Average loss at step 3930:   2.3\n",
      "Average loss at step 3940:   2.9\n",
      "Average loss at step 3950:   2.3\n",
      "Average loss at step 3960:   2.9\n",
      "Average loss at step 3970:   2.6\n",
      "Average loss at step 3980:   2.3\n",
      "Average loss at step 3990:   2.3\n",
      "Average loss at step 4000:   2.4\n",
      "Average loss at step 4010:   2.5\n",
      "Average loss at step 4020:   2.7\n",
      "Average loss at step 4030:   2.4\n",
      "Average loss at step 4040:   2.4\n",
      "Average loss at step 4050:   2.5\n",
      "Average loss at step 4060:   2.3\n",
      "Average loss at step 4070:   2.4\n",
      "Average loss at step 4080:   2.3\n",
      "Average loss at step 4090:   2.4\n",
      "Average loss at step 4100:   2.6\n",
      "Average loss at step 4110:   2.7\n",
      "Average loss at step 4120:   2.4\n",
      "Average loss at step 4130:   2.8\n",
      "Average loss at step 4140:   2.3\n",
      "Average loss at step 4150:   3.0\n",
      "Average loss at step 4160:   2.3\n",
      "Average loss at step 4170:   2.5\n",
      "Average loss at step 4180:   2.3\n",
      "Average loss at step 4190:   2.3\n",
      "Average loss at step 4200:   2.5\n",
      "Average loss at step 4210:   2.4\n",
      "Average loss at step 4220:   2.3\n",
      "Average loss at step 4230:   2.3\n",
      "Average loss at step 4240:   2.9\n",
      "Average loss at step 4250:   2.6\n",
      "Average loss at step 4260:   2.6\n",
      "Average loss at step 4270:   2.3\n",
      "Average loss at step 4280:   2.3\n",
      "Average loss at step 4290:   2.5\n",
      "Average loss at step 4300:   2.7\n",
      "Average loss at step 4310:   2.7\n",
      "Average loss at step 4320:   2.9\n",
      "Average loss at step 4330:   2.5\n",
      "Average loss at step 4340:   2.5\n",
      "Average loss at step 4350:   2.4\n",
      "Average loss at step 4360:   2.3\n",
      "Average loss at step 4370:   3.5\n",
      "Average loss at step 4380:   2.4\n",
      "Average loss at step 4390:   2.3\n",
      "Average loss at step 4400:   2.7\n",
      "Average loss at step 4410:   2.5\n",
      "Average loss at step 4420:   2.9\n",
      "Average loss at step 4430:   3.2\n",
      "Average loss at step 4440:   2.3\n",
      "Average loss at step 4450:   2.3\n",
      "Average loss at step 4460:   2.5\n",
      "Average loss at step 4470:   2.3\n",
      "Average loss at step 4480:   2.7\n",
      "Average loss at step 4490:   2.3\n",
      "Average loss at step 4500:   2.3\n",
      "Average loss at step 4510:   2.3\n",
      "Average loss at step 4520:   3.2\n",
      "Average loss at step 4530:   2.6\n",
      "Average loss at step 4540:   2.3\n",
      "Average loss at step 4550:   2.4\n",
      "Average loss at step 4560:   2.3\n",
      "Average loss at step 4570:   2.4\n",
      "Average loss at step 4580:   2.4\n",
      "Average loss at step 4590:   2.3\n",
      "Average loss at step 4600:   2.3\n",
      "Average loss at step 4610:   2.4\n",
      "Average loss at step 4620:   2.6\n",
      "Average loss at step 4630:   2.3\n",
      "Average loss at step 4640:   2.3\n",
      "Average loss at step 4650:   2.6\n",
      "Average loss at step 4660:   2.3\n",
      "Average loss at step 4670:   2.4\n",
      "Average loss at step 4680:   2.3\n",
      "Average loss at step 4690:   2.3\n",
      "Average loss at step 4700:   2.3\n",
      "Average loss at step 4710:   2.3\n",
      "Average loss at step 4720:   2.5\n",
      "Average loss at step 4730:   2.4\n",
      "Average loss at step 4740:   2.4\n",
      "Average loss at step 4750:   2.4\n",
      "Average loss at step 4760:   2.7\n",
      "Average loss at step 4770:   2.7\n",
      "Average loss at step 4780:   2.3\n",
      "Average loss at step 4790:   2.4\n",
      "Average loss at step 4800:   2.3\n",
      "Average loss at step 4810:   2.6\n",
      "Average loss at step 4820:   2.3\n",
      "Average loss at step 4830:   2.5\n",
      "Average loss at step 4840:   2.4\n",
      "Average loss at step 4850:   2.3\n",
      "Average loss at step 4860:   2.6\n",
      "Average loss at step 4870:   2.3\n",
      "Average loss at step 4880:   2.3\n",
      "Average loss at step 4890:   2.3\n",
      "Average loss at step 4900:   2.4\n",
      "Average loss at step 4910:   2.3\n",
      "Average loss at step 4920:   2.4\n",
      "Average loss at step 4930:   2.3\n",
      "Average loss at step 4940:   2.5\n",
      "Average loss at step 4950:   2.3\n",
      "Average loss at step 4960:   2.3\n",
      "Average loss at step 4970:   2.3\n",
      "Average loss at step 4980:   2.3\n",
      "Average loss at step 4990:   2.3\n",
      "Average loss at step 5000:   2.3\n",
      "Average loss at step 5010:   2.3\n",
      "Average loss at step 5020:   2.3\n",
      "Average loss at step 5030:   2.3\n",
      "Average loss at step 5040:   2.3\n",
      "Average loss at step 5050:   2.3\n",
      "Average loss at step 5060:   2.8\n",
      "Average loss at step 5070:   2.3\n",
      "Average loss at step 5080:   2.3\n",
      "Average loss at step 5090:   2.3\n",
      "Average loss at step 5100:   2.3\n",
      "Average loss at step 5110:   2.3\n",
      "Average loss at step 5120:   2.3\n",
      "Average loss at step 5130:   2.3\n",
      "Average loss at step 5140:   2.3\n",
      "Average loss at step 5150:   2.4\n",
      "Average loss at step 5160:   2.3\n",
      "Average loss at step 5170:   2.4\n",
      "Average loss at step 5180:   3.1\n",
      "Average loss at step 5190:   2.3\n",
      "Average loss at step 5200:   2.3\n",
      "Average loss at step 5210:   2.3\n",
      "Average loss at step 5220:   2.3\n",
      "Average loss at step 5230:   2.3\n",
      "Average loss at step 5240:   2.4\n",
      "Average loss at step 5250:   2.3\n",
      "Average loss at step 5260:   2.3\n",
      "Average loss at step 5270:   2.3\n",
      "Average loss at step 5280:   2.3\n",
      "Average loss at step 5290:   2.3\n",
      "Average loss at step 5300:   2.7\n",
      "Average loss at step 5310:   2.3\n",
      "Average loss at step 5320:   2.3\n",
      "Average loss at step 5330:   2.3\n",
      "Average loss at step 5340:   2.3\n",
      "Average loss at step 5350:   2.4\n",
      "Average loss at step 5360:   2.3\n",
      "Average loss at step 5370:   2.6\n",
      "Average loss at step 5380:   2.6\n",
      "Average loss at step 5390:   2.6\n",
      "Average loss at step 5400:   2.4\n",
      "Average loss at step 5410:   2.3\n",
      "Average loss at step 5420:   2.3\n",
      "Average loss at step 5430:   2.3\n",
      "Average loss at step 5440:   2.3\n",
      "Average loss at step 5450:   2.5\n",
      "Average loss at step 5460:   2.3\n",
      "Average loss at step 5470:   2.4\n",
      "Average loss at step 5480:   2.3\n",
      "Average loss at step 5490:   2.3\n",
      "Average loss at step 5500:   2.3\n",
      "Average loss at step 5510:   2.3\n",
      "Average loss at step 5520:   2.4\n",
      "Average loss at step 5530:   2.3\n",
      "Average loss at step 5540:   2.3\n",
      "Average loss at step 5550:   2.3\n",
      "Average loss at step 5560:   2.4\n",
      "Average loss at step 5570:   2.3\n",
      "Average loss at step 5580:   2.3\n",
      "Average loss at step 5590:   2.3\n",
      "Average loss at step 5600:   2.8\n",
      "Average loss at step 5610:   2.4\n",
      "Average loss at step 5620:   2.3\n",
      "Average loss at step 5630:   2.4\n",
      "Average loss at step 5640:   2.4\n",
      "Average loss at step 5650:   2.6\n",
      "Average loss at step 5660:   2.3\n",
      "Average loss at step 5670:   2.3\n",
      "Average loss at step 5680:   2.4\n",
      "Average loss at step 5690:   3.0\n",
      "Average loss at step 5700:   2.3\n",
      "Average loss at step 5710:   2.3\n",
      "Average loss at step 5720:   2.3\n",
      "Average loss at step 5730:   2.3\n",
      "Average loss at step 5740:   2.3\n",
      "Average loss at step 5750:   2.3\n",
      "Average loss at step 5760:   2.3\n",
      "Average loss at step 5770:   2.3\n",
      "Average loss at step 5780:   2.3\n",
      "Average loss at step 5790:   2.3\n",
      "Average loss at step 5800:   2.4\n",
      "Average loss at step 5810:   2.3\n",
      "Average loss at step 5820:   2.3\n",
      "Average loss at step 5830:   2.8\n",
      "Average loss at step 5840:   2.3\n",
      "Average loss at step 5850:   2.3\n",
      "Average loss at step 5860:   2.3\n",
      "Average loss at step 5870:   2.3\n",
      "Average loss at step 5880:   2.3\n",
      "Average loss at step 5890:   2.4\n",
      "Average loss at step 5900:   2.3\n",
      "Average loss at step 5910:   2.3\n",
      "Average loss at step 5920:   2.3\n",
      "Average loss at step 5930:   2.3\n",
      "Average loss at step 5940:   2.6\n",
      "Average loss at step 5950:   2.3\n",
      "Average loss at step 5960:   2.3\n",
      "Average loss at step 5970:   2.3\n",
      "Average loss at step 5980:   2.3\n",
      "Average loss at step 5990:   2.3\n",
      "Average loss at step 6000:   2.3\n",
      "Average loss at step 6010:   2.3\n",
      "Average loss at step 6020:   2.3\n",
      "Average loss at step 6030:   2.4\n",
      "Average loss at step 6040:   2.3\n",
      "Average loss at step 6050:   2.3\n",
      "Average loss at step 6060:   2.3\n",
      "Average loss at step 6070:   2.3\n",
      "Average loss at step 6080:   2.3\n",
      "Average loss at step 6090:   2.3\n",
      "Average loss at step 6100:   2.3\n",
      "Average loss at step 6110:   2.3\n",
      "Average loss at step 6120:   2.3\n",
      "Average loss at step 6130:   2.4\n",
      "Average loss at step 6140:   2.3\n",
      "Average loss at step 6150:   2.3\n",
      "Average loss at step 6160:   2.3\n",
      "Average loss at step 6170:   2.3\n",
      "Average loss at step 6180:   2.3\n",
      "Average loss at step 6190:   2.3\n",
      "Average loss at step 6200:   2.4\n",
      "Average loss at step 6210:   2.3\n",
      "Average loss at step 6220:   2.3\n",
      "Average loss at step 6230:   2.3\n",
      "Average loss at step 6240:   2.9\n",
      "Average loss at step 6250:   2.3\n",
      "Average loss at step 6260:   2.3\n",
      "Average loss at step 6270:   2.3\n",
      "Average loss at step 6280:   2.3\n",
      "Average loss at step 6290:   2.5\n",
      "Average loss at step 6300:   2.3\n",
      "Average loss at step 6310:   2.5\n",
      "Average loss at step 6320:   2.3\n",
      "Average loss at step 6330:   2.5\n",
      "Average loss at step 6340:   2.3\n",
      "Average loss at step 6350:   2.8\n",
      "Average loss at step 6360:   2.3\n",
      "Average loss at step 6370:   2.3\n",
      "Average loss at step 6380:   2.4\n",
      "Average loss at step 6390:   2.3\n",
      "Average loss at step 6400:   2.3\n",
      "Average loss at step 6410:   2.3\n",
      "Average loss at step 6420:   2.3\n",
      "Average loss at step 6430:   2.3\n",
      "Average loss at step 6440:   2.3\n",
      "Average loss at step 6450:   2.3\n",
      "Average loss at step 6460:   2.3\n",
      "Average loss at step 6470:   2.3\n",
      "Average loss at step 6480:   2.4\n",
      "Average loss at step 6490:   2.3\n",
      "Average loss at step 6500:   2.3\n",
      "Average loss at step 6510:   2.3\n",
      "Average loss at step 6520:   2.4\n",
      "Average loss at step 6530:   2.3\n",
      "Average loss at step 6540:   2.3\n",
      "Average loss at step 6550:   2.3\n",
      "Average loss at step 6560:   2.3\n",
      "Average loss at step 6570:   2.3\n",
      "Average loss at step 6580:   2.3\n",
      "Average loss at step 6590:   2.5\n",
      "Average loss at step 6600:   2.3\n",
      "Average loss at step 6610:   2.3\n",
      "Average loss at step 6620:   2.3\n",
      "Average loss at step 6630:   2.3\n",
      "Average loss at step 6640:   2.3\n",
      "Average loss at step 6650:   2.3\n",
      "Average loss at step 6660:   2.3\n",
      "Average loss at step 6670:   2.3\n",
      "Average loss at step 6680:   2.3\n",
      "Average loss at step 6690:   2.3\n",
      "Average loss at step 6700:   2.3\n",
      "Average loss at step 6710:   2.4\n",
      "Average loss at step 6720:   2.3\n",
      "Average loss at step 6730:   2.4\n",
      "Average loss at step 6740:   2.5\n",
      "Average loss at step 6750:   2.3\n",
      "Average loss at step 6760:   2.3\n",
      "Average loss at step 6770:   2.3\n",
      "Average loss at step 6780:   2.3\n",
      "Average loss at step 6790:   2.3\n",
      "Average loss at step 6800:   2.3\n",
      "Average loss at step 6810:   2.3\n",
      "Average loss at step 6820:   2.3\n",
      "Average loss at step 6830:   2.3\n",
      "Average loss at step 6840:   2.3\n",
      "Average loss at step 6850:   2.3\n",
      "Average loss at step 6860:   2.3\n",
      "Average loss at step 6870:   2.3\n",
      "Average loss at step 6880:   2.3\n",
      "Average loss at step 6890:   2.3\n",
      "Average loss at step 6900:   2.3\n",
      "Average loss at step 6910:   2.3\n",
      "Average loss at step 6920:   2.3\n",
      "Average loss at step 6930:   2.3\n",
      "Average loss at step 6940:   2.3\n",
      "Average loss at step 6950:   2.3\n",
      "Average loss at step 6960:   2.3\n",
      "Average loss at step 6970:   2.3\n",
      "Average loss at step 6980:   2.3\n",
      "Average loss at step 6990:   2.3\n",
      "Average loss at step 7000:   2.4\n",
      "Average loss at step 7010:   2.3\n",
      "Average loss at step 7020:   2.3\n",
      "Average loss at step 7030:   2.3\n",
      "Average loss at step 7040:   2.3\n",
      "Average loss at step 7050:   2.3\n",
      "Average loss at step 7060:   2.3\n",
      "Average loss at step 7070:   2.3\n",
      "Average loss at step 7080:   2.3\n",
      "Average loss at step 7090:   2.3\n",
      "Average loss at step 7100:   2.3\n",
      "Average loss at step 7110:   2.4\n",
      "Average loss at step 7120:   2.3\n",
      "Average loss at step 7130:   2.3\n",
      "Average loss at step 7140:   2.6\n",
      "Average loss at step 7150:   2.9\n",
      "Average loss at step 7160:   2.3\n",
      "Average loss at step 7170:   2.3\n",
      "Average loss at step 7180:   2.3\n",
      "Average loss at step 7190:   2.3\n",
      "Average loss at step 7200:   2.3\n",
      "Average loss at step 7210:   2.3\n",
      "Average loss at step 7220:   2.3\n",
      "Average loss at step 7230:   2.3\n",
      "Average loss at step 7240:   2.3\n",
      "Average loss at step 7250:   2.3\n",
      "Average loss at step 7260:   2.3\n",
      "Average loss at step 7270:   2.3\n",
      "Average loss at step 7280:   2.3\n",
      "Average loss at step 7290:   2.6\n",
      "Average loss at step 7300:   2.3\n",
      "Average loss at step 7310:   2.5\n",
      "Average loss at step 7320:   2.3\n",
      "Average loss at step 7330:   2.6\n",
      "Average loss at step 7340:   2.3\n",
      "Average loss at step 7350:   2.3\n",
      "Average loss at step 7360:   2.3\n",
      "Average loss at step 7370:   2.3\n",
      "Average loss at step 7380:   2.3\n",
      "Average loss at step 7390:   2.7\n",
      "Average loss at step 7400:   2.3\n",
      "Average loss at step 7410:   2.4\n",
      "Average loss at step 7420:   2.3\n",
      "Average loss at step 7430:   2.3\n",
      "Average loss at step 7440:   2.3\n",
      "Average loss at step 7450:   2.3\n",
      "Average loss at step 7460:   2.3\n",
      "Average loss at step 7470:   2.3\n",
      "Average loss at step 7480:   2.3\n",
      "Average loss at step 7490:   2.3\n",
      "Average loss at step 7500:   2.3\n",
      "Average loss at step 7510:   2.3\n",
      "Average loss at step 7520:   2.3\n",
      "Average loss at step 7530:   2.3\n",
      "Average loss at step 7540:   2.3\n",
      "Average loss at step 7550:   2.3\n",
      "Average loss at step 7560:   2.3\n",
      "Average loss at step 7570:   2.3\n",
      "Average loss at step 7580:   2.3\n",
      "Average loss at step 7590:   2.3\n",
      "Average loss at step 7600:   2.3\n",
      "Average loss at step 7610:   2.3\n",
      "Average loss at step 7620:   2.3\n",
      "Average loss at step 7630:   2.3\n",
      "Average loss at step 7640:   2.4\n",
      "Average loss at step 7650:   2.3\n",
      "Average loss at step 7660:   2.3\n",
      "Average loss at step 7670:   2.3\n",
      "Average loss at step 7680:   2.3\n",
      "Average loss at step 7690:   2.4\n",
      "Average loss at step 7700:   2.4\n",
      "Average loss at step 7710:   2.3\n",
      "Average loss at step 7720:   2.3\n",
      "Average loss at step 7730:   2.3\n",
      "Average loss at step 7740:   2.3\n",
      "Average loss at step 7750:   2.3\n",
      "Average loss at step 7760:   2.3\n",
      "Average loss at step 7770:   2.5\n",
      "Average loss at step 7780:   2.3\n",
      "Average loss at step 7790:   2.3\n",
      "Average loss at step 7800:   2.3\n",
      "Average loss at step 7810:   2.3\n",
      "Average loss at step 7820:   2.3\n",
      "Average loss at step 7830:   2.3\n",
      "Average loss at step 7840:   2.3\n",
      "Average loss at step 7850:   2.3\n",
      "Average loss at step 7860:   2.3\n",
      "Average loss at step 7870:   2.3\n",
      "Average loss at step 7880:   2.3\n",
      "Average loss at step 7890:   2.3\n",
      "Average loss at step 7900:   2.3\n",
      "Average loss at step 7910:   2.3\n",
      "Average loss at step 7920:   2.3\n",
      "Average loss at step 7930:   2.3\n",
      "Average loss at step 7940:   2.3\n",
      "Average loss at step 7950:   2.3\n",
      "Average loss at step 7960:   2.3\n",
      "Average loss at step 7970:   2.3\n",
      "Average loss at step 7980:   2.3\n",
      "Average loss at step 7990:   2.3\n",
      "Average loss at step 8000:   2.3\n",
      "Average loss at step 8010:   2.3\n",
      "Average loss at step 8020:   2.3\n",
      "Average loss at step 8030:   2.3\n",
      "Average loss at step 8040:   2.3\n",
      "Average loss at step 8050:   2.3\n",
      "Average loss at step 8060:   2.3\n",
      "Average loss at step 8070:   2.3\n",
      "Average loss at step 8080:   2.3\n",
      "Average loss at step 8090:   2.3\n",
      "Average loss at step 8100:   2.3\n",
      "Average loss at step 8110:   2.3\n",
      "Average loss at step 8120:   2.3\n",
      "Average loss at step 8130:   2.3\n",
      "Average loss at step 8140:   2.3\n",
      "Average loss at step 8150:   2.3\n",
      "Average loss at step 8160:   2.3\n",
      "Average loss at step 8170:   2.3\n",
      "Average loss at step 8180:   2.3\n",
      "Average loss at step 8190:   2.3\n",
      "Average loss at step 8200:   2.4\n",
      "Average loss at step 8210:   2.3\n",
      "Average loss at step 8220:   2.3\n",
      "Average loss at step 8230:   2.3\n",
      "Average loss at step 8240:   2.3\n",
      "Average loss at step 8250:   2.3\n",
      "Average loss at step 8260:   2.3\n",
      "Average loss at step 8270:   2.3\n",
      "Average loss at step 8280:   2.3\n",
      "Average loss at step 8290:   2.4\n",
      "Average loss at step 8300:   2.3\n",
      "Average loss at step 8310:   2.3\n",
      "Average loss at step 8320:   2.3\n",
      "Average loss at step 8330:   2.3\n",
      "Average loss at step 8340:   2.3\n",
      "Average loss at step 8350:   2.3\n",
      "Average loss at step 8360:   2.3\n",
      "Average loss at step 8370:   2.3\n",
      "Average loss at step 8380:   2.3\n",
      "Average loss at step 8390:   2.3\n",
      "Average loss at step 8400:   2.3\n",
      "Average loss at step 8410:   2.3\n",
      "Average loss at step 8420:   2.3\n",
      "Average loss at step 8430:   2.3\n",
      "Average loss at step 8440:   2.3\n",
      "Average loss at step 8450:   2.4\n",
      "Average loss at step 8460:   2.3\n",
      "Average loss at step 8470:   2.3\n",
      "Average loss at step 8480:   2.3\n",
      "Average loss at step 8490:   2.3\n",
      "Average loss at step 8500:   2.3\n",
      "Average loss at step 8510:   2.3\n",
      "Average loss at step 8520:   2.3\n",
      "Average loss at step 8530:   2.3\n",
      "Average loss at step 8540:   2.3\n",
      "Average loss at step 8550:   2.3\n",
      "Average loss at step 8560:   2.3\n",
      "Average loss at step 8570:   2.3\n",
      "Average loss at step 8580:   2.3\n",
      "Average loss at step 8590:   2.3\n",
      "Optimization Finished!\n",
      "Total time: 1919.5923478603363 seconds\n",
      "Accuracy 0.1115\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "N_CLASSES = 10\n",
    "\n",
    "# Step 2: Define paramaters for the model\n",
    "LEARNING_RATE = 0.01\n",
    "BATCH_SIZE = 64\n",
    "SKIP_STEP = 10\n",
    "DROPOUT = 0.80\n",
    "N_EPOCHS = 10\n",
    "\n",
    "\n",
    "with tf.name_scope('data'):\n",
    "    X = tf.placeholder(tf.float32, [None, 784], name=\"X_placeholder\")\n",
    "    Y = tf.placeholder(tf.float32, [None, 10], name=\"Y_placeholder\")\n",
    "\n",
    "dropout = tf.placeholder(tf.float32, name='dropout')\n",
    "\n",
    "global_step = tf.Variable(0, dtype=tf.int32, trainable=False, name='global_step')\n",
    "\n",
    "\n",
    "with tf.variable_scope('fc0') as scope:\n",
    "    \n",
    "    w = tf.get_variable('weights', shape=[784, 1024], initializer=tf.truncated_normal_initializer())\n",
    "    b = tf.get_variable('bias', shape=[1024], initializer=tf.constant_initializer(0.0))\n",
    "    \n",
    "    fc0 = tf.cos(tf.add(tf.matmul(X, w), b))\n",
    "    \n",
    "\n",
    "with tf.variable_scope('conv1') as scope:\n",
    "    \n",
    "    images = tf.reshape(fc0, shape=[-1, 32, 32, 1])\n",
    "    \n",
    "    \n",
    "    kernel = tf.get_variable('kernel', shape=[5, 5, 1, 32], initializer=tf.truncated_normal_initializer())\n",
    "    \n",
    "   \n",
    "    \n",
    "    biases = tf.get_variable('biases', shape=[32], initializer=tf.constant_initializer(0.0))\n",
    "    \n",
    "   \n",
    "    conv = tf.nn.conv2d(images, kernel, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    \n",
    "   \n",
    "    conv1 = tf.nn.relu(conv + biases, name='conv1')\n",
    "    \n",
    "    \n",
    "    \n",
    "with tf.variable_scope('pool1') as scope:\n",
    "   \n",
    "    \n",
    "    pool1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], name='pool1', strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "    \n",
    "\n",
    "with tf.variable_scope('conv2') as scope:\n",
    "\n",
    "    kernel = tf.get_variable('kernels', [5, 5, 32, 64], \n",
    "                        initializer=tf.truncated_normal_initializer())\n",
    "    biases = tf.get_variable('biases', [64],\n",
    "                        initializer=tf.random_normal_initializer())\n",
    "    conv = tf.nn.conv2d(pool1, kernel, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    conv2 = tf.nn.relu(conv + biases, name=scope.name)\n",
    "\n",
    "\n",
    "with tf.variable_scope('pool2') as scope:\n",
    "\n",
    "    pool2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1],\n",
    "                            padding='SAME')\n",
    "\n",
    "   \n",
    "    \n",
    "\n",
    "with tf.variable_scope('fc') as scope:\n",
    "\n",
    "    input_features = 8 * 8 * 64\n",
    "    \n",
    "   \n",
    "    w = tf.get_variable('weights', shape=[input_features, 1024], initializer=tf.truncated_normal_initializer())\n",
    "    b = tf.get_variable('bias', shape=[1024], initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "    pool2 = tf.reshape(pool2, [-1, input_features])\n",
    "\n",
    "    fc = tf.nn.relu(tf.matmul(pool2, w) + b, name='relu')\n",
    "    \n",
    "   \n",
    "    fc = tf.nn.dropout(fc, dropout, name='relu_dropout')\n",
    "\n",
    "with tf.variable_scope('softmax_linear') as scope:\n",
    "    \n",
    "    w = tf.get_variable('weights', shape=[1024, N_CLASSES], initializer=tf.truncated_normal_initializer())\n",
    "    b = tf.get_variable('bias', shape=[N_CLASSES], initializer=tf.constant_initializer(0.0))\n",
    "    \n",
    "    logits = tf.matmul(fc, w) + b\n",
    "\n",
    "\n",
    "    \n",
    "with tf.name_scope('loss'):\n",
    "   \n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=Y, logits=logits), name='loss')\n",
    "\n",
    "\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE).minimize(loss, global_step=global_step)\n",
    "\n",
    "with tf.name_scope('summary'):\n",
    "    tf.summary.scalar('loss', loss)\n",
    "    summary_op = tf.summary.merge_all()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    writer = tf.summary.FileWriter('../my_graph/mnist2', sess.graph)\n",
    "   \n",
    "    \n",
    "    initial_step = global_step.eval()\n",
    "\n",
    "    start_time = time.time()\n",
    "    n_batches = int(mnist.train.num_examples / BATCH_SIZE)\n",
    "\n",
    "    total_loss = 0.0\n",
    "    for index in range(initial_step, n_batches * N_EPOCHS): # train the model n_epochs times\n",
    "        X_batch, Y_batch = mnist.train.next_batch(BATCH_SIZE)\n",
    "        _, loss_batch, summary = sess.run([optimizer, loss, summary_op], \n",
    "                                feed_dict={X: X_batch, Y:Y_batch, dropout: DROPOUT}) \n",
    "        total_loss += loss_batch\n",
    "        writer.add_summary(summary, global_step=index)\n",
    "        if (index + 1) % SKIP_STEP == 0:\n",
    "            print('Average loss at step {}: {:5.1f}'.format(index + 1, total_loss / SKIP_STEP))\n",
    "            total_loss = 0.0\n",
    "            saver.save(sess, '../checkpoints/convnet_mnist2/mnist-convnet', index)\n",
    "    \n",
    "    print(\"Optimization Finished!\") # should be around 0.35 after 25 epochs\n",
    "    print(\"Total time: {0} seconds\".format(time.time() - start_time))\n",
    "    \n",
    "    # test the model\n",
    "    n_batches = int(mnist.test.num_examples/BATCH_SIZE)\n",
    "    total_correct_preds = 0\n",
    "    for i in range(n_batches):\n",
    "        X_batch, Y_batch = mnist.test.next_batch(BATCH_SIZE)\n",
    "        _, loss_batch, logits_batch = sess.run([optimizer, loss, logits], \n",
    "                                        feed_dict={X: X_batch, Y:Y_batch, dropout: DROPOUT}) \n",
    "        preds = tf.nn.softmax(logits_batch)\n",
    "        correct_preds = tf.equal(tf.argmax(preds, 1), tf.argmax(Y_batch, 1))\n",
    "        accuracy = tf.reduce_sum(tf.cast(correct_preds, tf.float32))\n",
    "        total_correct_preds += sess.run(accuracy)   \n",
    "    \n",
    "    print(\"Accuracy {0}\".format(total_correct_preds/mnist.test.num_examples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# (10 epochs) one fc with cosine then 2 conv nets then 1 fc with dropout  -> Accuracy 0.1115 :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x10667e160>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlkAAAEmCAYAAABRZIXOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQRBSwP8HifyDQaA9pysAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10667e390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "img = plt.imread('../assets/Screen Shot 2017-11-02 at 9.23.18 PM.png')\n",
    "plt.title('first->orange, second->blue')\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 10: 1366590.3\n",
      "Average loss at step 20: 80393.4\n",
      "Average loss at step 30: 15991.7\n",
      "Average loss at step 40: 9279.1\n",
      "Average loss at step 50: 6295.8\n",
      "Average loss at step 60: 4541.7\n",
      "Average loss at step 70: 4650.9\n",
      "Average loss at step 80: 3912.3\n",
      "Average loss at step 90: 3518.4\n",
      "Average loss at step 100: 3338.1\n",
      "Average loss at step 110: 2680.4\n",
      "Average loss at step 120: 2334.6\n",
      "Average loss at step 130: 2413.0\n",
      "Average loss at step 140: 2412.8\n",
      "Average loss at step 150: 2226.1\n",
      "Average loss at step 160: 2196.5\n",
      "Average loss at step 170: 1962.1\n",
      "Average loss at step 180: 1892.0\n",
      "Average loss at step 190: 1497.8\n",
      "Average loss at step 200: 1620.8\n",
      "Average loss at step 210: 1621.5\n",
      "Average loss at step 220: 1291.9\n",
      "Average loss at step 230: 1322.8\n",
      "Average loss at step 240: 1289.1\n",
      "Average loss at step 250: 1153.8\n",
      "Average loss at step 260: 1096.3\n",
      "Average loss at step 270: 1088.2\n",
      "Average loss at step 280: 1038.0\n",
      "Average loss at step 290: 985.9\n",
      "Average loss at step 300: 875.0\n",
      "Average loss at step 310: 962.8\n",
      "Average loss at step 320: 844.6\n",
      "Average loss at step 330: 768.9\n",
      "Average loss at step 340: 641.9\n",
      "Average loss at step 350: 746.5\n",
      "Average loss at step 360: 581.4\n",
      "Average loss at step 370: 596.8\n",
      "Average loss at step 380: 429.0\n",
      "Average loss at step 390: 465.4\n",
      "Average loss at step 400: 506.2\n",
      "Average loss at step 410: 417.7\n",
      "Average loss at step 420: 382.3\n",
      "Average loss at step 430: 378.3\n",
      "Average loss at step 440: 258.3\n",
      "Average loss at step 450: 328.5\n",
      "Average loss at step 460: 237.5\n",
      "Average loss at step 470: 253.1\n",
      "Average loss at step 480: 220.2\n",
      "Average loss at step 490: 202.3\n",
      "Average loss at step 500: 152.8\n",
      "Average loss at step 510: 147.6\n",
      "Average loss at step 520: 122.6\n",
      "Average loss at step 530: 110.4\n",
      "Average loss at step 540:  96.4\n",
      "Average loss at step 550: 125.2\n",
      "Average loss at step 560:  83.6\n",
      "Average loss at step 570: 104.6\n",
      "Average loss at step 580:  74.1\n",
      "Average loss at step 590:  52.9\n",
      "Average loss at step 600:  90.0\n",
      "Average loss at step 610:  70.1\n",
      "Average loss at step 620:  66.8\n",
      "Average loss at step 630:  54.0\n",
      "Average loss at step 640:  46.9\n",
      "Average loss at step 650:  35.4\n",
      "Average loss at step 660:  52.5\n",
      "Average loss at step 670:  37.0\n",
      "Average loss at step 680:  51.0\n",
      "Average loss at step 690:  44.4\n",
      "Average loss at step 700:  37.7\n",
      "Average loss at step 710:  29.9\n",
      "Average loss at step 720:  31.9\n",
      "Average loss at step 730:  36.2\n",
      "Average loss at step 740:  49.1\n",
      "Average loss at step 750:  27.0\n",
      "Average loss at step 760:  37.9\n",
      "Average loss at step 770:  26.3\n",
      "Average loss at step 780:  21.4\n",
      "Average loss at step 790:  26.9\n",
      "Average loss at step 800:  14.9\n",
      "Average loss at step 810:  21.0\n",
      "Average loss at step 820:  15.5\n",
      "Average loss at step 830:  21.1\n",
      "Average loss at step 840:  22.9\n",
      "Average loss at step 850:  20.7\n",
      "Average loss at step 860:  14.9\n",
      "Average loss at step 870:  16.3\n",
      "Average loss at step 880:  16.7\n",
      "Average loss at step 890:  10.5\n",
      "Average loss at step 900:  27.0\n",
      "Average loss at step 910:  19.8\n",
      "Average loss at step 920:  23.0\n",
      "Average loss at step 930:  12.2\n",
      "Average loss at step 940:  18.6\n",
      "Average loss at step 950:   6.7\n",
      "Average loss at step 960:  17.4\n",
      "Average loss at step 970:  14.3\n",
      "Average loss at step 980:  13.1\n",
      "Average loss at step 990:  10.0\n",
      "Average loss at step 1000:  19.5\n",
      "Average loss at step 1010:  20.5\n",
      "Average loss at step 1020:   7.3\n",
      "Average loss at step 1030:  26.9\n",
      "Average loss at step 1040:   9.8\n",
      "Average loss at step 1050:   9.4\n",
      "Average loss at step 1060:  13.4\n",
      "Average loss at step 1070:  11.2\n",
      "Average loss at step 1080:   6.9\n",
      "Average loss at step 1090:   7.3\n",
      "Average loss at step 1100:   8.1\n",
      "Average loss at step 1110:   7.2\n",
      "Average loss at step 1120:  18.8\n",
      "Average loss at step 1130:   7.7\n",
      "Average loss at step 1140:  11.1\n",
      "Average loss at step 1150:   6.3\n",
      "Average loss at step 1160:   4.0\n",
      "Average loss at step 1170:   8.0\n",
      "Average loss at step 1180:   8.6\n",
      "Average loss at step 1190:   8.0\n",
      "Average loss at step 1200:   6.4\n",
      "Average loss at step 1210:   8.1\n",
      "Average loss at step 1220:  10.8\n",
      "Average loss at step 1230:   5.7\n",
      "Average loss at step 1240:   5.4\n",
      "Average loss at step 1250:  10.6\n",
      "Average loss at step 1260:   3.9\n",
      "Average loss at step 1270:  11.5\n",
      "Average loss at step 1280:   5.6\n",
      "Average loss at step 1290:   8.5\n",
      "Average loss at step 1300:   4.8\n",
      "Average loss at step 1310:   4.7\n",
      "Average loss at step 1320:   4.6\n",
      "Average loss at step 1330:   5.9\n",
      "Average loss at step 1340:   8.2\n",
      "Average loss at step 1350:   8.0\n",
      "Average loss at step 1360:   9.5\n",
      "Average loss at step 1370:   4.9\n",
      "Average loss at step 1380:   6.4\n",
      "Average loss at step 1390:   3.4\n",
      "Average loss at step 1400:   6.1\n",
      "Average loss at step 1410:  15.3\n",
      "Average loss at step 1420:   4.0\n",
      "Average loss at step 1430:   4.2\n",
      "Average loss at step 1440:   4.5\n",
      "Average loss at step 1450:   4.7\n",
      "Average loss at step 1460:   7.7\n",
      "Average loss at step 1470:   4.6\n",
      "Average loss at step 1480:   6.2\n",
      "Average loss at step 1490:   2.3\n",
      "Average loss at step 1500:   4.5\n",
      "Average loss at step 1510:   5.1\n",
      "Average loss at step 1520:   2.6\n",
      "Average loss at step 1530:   9.3\n",
      "Average loss at step 1540:   4.3\n",
      "Average loss at step 1550:   3.6\n",
      "Average loss at step 1560:   3.3\n",
      "Average loss at step 1570:   2.9\n",
      "Average loss at step 1580:   4.7\n",
      "Average loss at step 1590:   5.4\n",
      "Average loss at step 1600:   6.3\n",
      "Average loss at step 1610:   3.7\n",
      "Average loss at step 1620:   4.3\n",
      "Average loss at step 1630:   6.5\n",
      "Average loss at step 1640:   4.1\n",
      "Average loss at step 1650:   3.6\n",
      "Average loss at step 1660:   7.8\n",
      "Average loss at step 1670:   5.0\n",
      "Average loss at step 1680:   2.9\n",
      "Average loss at step 1690:   2.5\n",
      "Average loss at step 1700:   3.2\n",
      "Average loss at step 1710:   5.2\n",
      "Optimization Finished!\n",
      "Total time: 385.8700170516968 seconds\n",
      "Accuracy 0.1285\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "N_CLASSES = 10\n",
    "\n",
    "# Step 2: Define paramaters for the model\n",
    "LEARNING_RATE = 0.01\n",
    "BATCH_SIZE = 64\n",
    "SKIP_STEP = 10\n",
    "DROPOUT = 0.80\n",
    "N_EPOCHS = 2\n",
    "\n",
    "\n",
    "with tf.name_scope('data'):\n",
    "    X = tf.placeholder(tf.float32, [None, 784], name=\"X_placeholder\")\n",
    "    Y = tf.placeholder(tf.float32, [None, 10], name=\"Y_placeholder\")\n",
    "\n",
    "dropout = tf.placeholder(tf.float32, name='dropout')\n",
    "\n",
    "global_step = tf.Variable(0, dtype=tf.int32, trainable=False, name='global_step')\n",
    "\n",
    "\n",
    "with tf.variable_scope('fc0') as scope:\n",
    "    \n",
    "    w = tf.get_variable('weights', shape=[784, 1024], initializer=tf.truncated_normal_initializer())\n",
    "    b = tf.get_variable('bias', shape=[1024], initializer=tf.constant_initializer(0.0))\n",
    "    \n",
    "    fc0 = tf.nn.relu(tf.add(tf.matmul(X, w), b))\n",
    "    \n",
    "\n",
    "with tf.variable_scope('conv1') as scope:\n",
    "    \n",
    "    images = tf.reshape(fc0, shape=[-1, 32, 32, 1])\n",
    "    \n",
    "    \n",
    "    kernel = tf.get_variable('kernel', shape=[5, 5, 1, 32], initializer=tf.truncated_normal_initializer())\n",
    "    \n",
    "   \n",
    "    \n",
    "    biases = tf.get_variable('biases', shape=[32], initializer=tf.constant_initializer(0.0))\n",
    "    \n",
    "   \n",
    "    conv = tf.nn.conv2d(images, kernel, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    \n",
    "   \n",
    "    conv1 = tf.nn.relu(conv + biases, name='conv1')\n",
    "    \n",
    "    \n",
    "    \n",
    "with tf.variable_scope('pool1') as scope:\n",
    "   \n",
    "    \n",
    "    pool1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], name='pool1', strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "    \n",
    "\n",
    "with tf.variable_scope('conv2') as scope:\n",
    "\n",
    "    kernel = tf.get_variable('kernels', [5, 5, 32, 64], \n",
    "                        initializer=tf.truncated_normal_initializer())\n",
    "    biases = tf.get_variable('biases', [64],\n",
    "                        initializer=tf.random_normal_initializer())\n",
    "    conv = tf.nn.conv2d(pool1, kernel, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    conv2 = tf.nn.relu(conv + biases, name=scope.name)\n",
    "\n",
    "\n",
    "with tf.variable_scope('pool2') as scope:\n",
    "\n",
    "    pool2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1],\n",
    "                            padding='SAME')\n",
    "\n",
    "   \n",
    "    \n",
    "\n",
    "with tf.variable_scope('fc') as scope:\n",
    "\n",
    "    input_features = 8 * 8 * 64\n",
    "    \n",
    "   \n",
    "    w = tf.get_variable('weights', shape=[input_features, 1024], initializer=tf.truncated_normal_initializer())\n",
    "    b = tf.get_variable('bias', shape=[1024], initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "    pool2 = tf.reshape(pool2, [-1, input_features])\n",
    "\n",
    "    fc = tf.nn.relu(tf.matmul(pool2, w) + b, name='relu')\n",
    "    \n",
    "   \n",
    "    fc = tf.nn.dropout(fc, dropout, name='relu_dropout')\n",
    "\n",
    "with tf.variable_scope('softmax_linear') as scope:\n",
    "    \n",
    "    w = tf.get_variable('weights', shape=[1024, N_CLASSES], initializer=tf.truncated_normal_initializer())\n",
    "    b = tf.get_variable('bias', shape=[N_CLASSES], initializer=tf.constant_initializer(0.0))\n",
    "    \n",
    "    logits = tf.matmul(fc, w) + b\n",
    "\n",
    "\n",
    "    \n",
    "with tf.name_scope('loss'):\n",
    "   \n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=Y, logits=logits), name='loss')\n",
    "\n",
    "\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE).minimize(loss, global_step=global_step)\n",
    "\n",
    "with tf.name_scope('summary'):\n",
    "    tf.summary.scalar('loss', loss)\n",
    "    summary_op = tf.summary.merge_all()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    writer = tf.summary.FileWriter('../my_graph/mnist3', sess.graph)\n",
    "   \n",
    "    \n",
    "    initial_step = global_step.eval()\n",
    "\n",
    "    start_time = time.time()\n",
    "    n_batches = int(mnist.train.num_examples / BATCH_SIZE)\n",
    "\n",
    "    total_loss = 0.0\n",
    "    for index in range(initial_step, n_batches * N_EPOCHS): # train the model n_epochs times\n",
    "        X_batch, Y_batch = mnist.train.next_batch(BATCH_SIZE)\n",
    "        _, loss_batch, summary = sess.run([optimizer, loss, summary_op], \n",
    "                                feed_dict={X: X_batch, Y:Y_batch, dropout: DROPOUT}) \n",
    "        total_loss += loss_batch\n",
    "        writer.add_summary(summary, global_step=index)\n",
    "        if (index + 1) % SKIP_STEP == 0:\n",
    "            print('Average loss at step {}: {:5.1f}'.format(index + 1, total_loss / SKIP_STEP))\n",
    "            total_loss = 0.0\n",
    "            saver.save(sess, '../checkpoints/convnet_mnist3/mnist-convnet', index)\n",
    "    \n",
    "    print(\"Optimization Finished!\") # should be around 0.35 after 25 epochs\n",
    "    print(\"Total time: {0} seconds\".format(time.time() - start_time))\n",
    "    \n",
    "    # test the model\n",
    "    n_batches = int(mnist.test.num_examples/BATCH_SIZE)\n",
    "    total_correct_preds = 0\n",
    "    for i in range(n_batches):\n",
    "        X_batch, Y_batch = mnist.test.next_batch(BATCH_SIZE)\n",
    "        _, loss_batch, logits_batch = sess.run([optimizer, loss, logits], \n",
    "                                        feed_dict={X: X_batch, Y:Y_batch, dropout: DROPOUT}) \n",
    "        preds = tf.nn.softmax(logits_batch)\n",
    "        correct_preds = tf.equal(tf.argmax(preds, 1), tf.argmax(Y_batch, 1))\n",
    "        accuracy = tf.reduce_sum(tf.cast(correct_preds, tf.float32))\n",
    "        total_correct_preds += sess.run(accuracy)   \n",
    "    \n",
    "    print(\"Accuracy {0}\".format(total_correct_preds/mnist.test.num_examples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# (2 epochs) one fc with relu then 2 conv nets then 1 fc with dropout  -> Accuracy 0.1285 :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# try resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 10: 17788824.2\n",
      "Average loss at step 20: 1083870.6\n",
      "Average loss at step 30: 197865.6\n",
      "Average loss at step 40: 94776.7\n",
      "Average loss at step 50: 63273.8\n",
      "Average loss at step 60: 50003.1\n",
      "Average loss at step 70: 37653.5\n",
      "Average loss at step 80: 35214.1\n",
      "Average loss at step 90: 31107.0\n",
      "Average loss at step 100: 26878.4\n",
      "Average loss at step 110: 23883.7\n",
      "Average loss at step 120: 22298.1\n",
      "Average loss at step 130: 20411.6\n",
      "Average loss at step 140: 15411.7\n",
      "Average loss at step 150: 15745.3\n",
      "Average loss at step 160: 13657.7\n",
      "Average loss at step 170: 11996.5\n",
      "Average loss at step 180: 9973.1\n",
      "Average loss at step 190: 9951.5\n",
      "Average loss at step 200: 8184.6\n",
      "Average loss at step 210: 7711.6\n",
      "Average loss at step 220: 5708.9\n",
      "Average loss at step 230: 4484.7\n",
      "Average loss at step 240: 3318.9\n",
      "Average loss at step 250: 3143.7\n",
      "Average loss at step 260: 2811.9\n",
      "Average loss at step 270: 2501.5\n",
      "Average loss at step 280: 1573.3\n",
      "Average loss at step 290: 1809.6\n",
      "Average loss at step 300: 1333.4\n",
      "Average loss at step 310: 1959.7\n",
      "Average loss at step 320: 1704.7\n",
      "Average loss at step 330: 1193.4\n",
      "Average loss at step 340: 1005.4\n",
      "Average loss at step 350: 1046.8\n",
      "Average loss at step 360: 721.1\n",
      "Average loss at step 370: 966.7\n",
      "Average loss at step 380: 645.1\n",
      "Average loss at step 390: 420.4\n",
      "Average loss at step 400: 455.3\n",
      "Average loss at step 410: 628.2\n",
      "Average loss at step 420: 423.6\n",
      "Average loss at step 430: 450.9\n",
      "Average loss at step 440: 627.2\n",
      "Average loss at step 450: 507.3\n",
      "Average loss at step 460: 519.4\n",
      "Average loss at step 470: 718.2\n",
      "Average loss at step 480: 466.1\n",
      "Average loss at step 490: 214.9\n",
      "Average loss at step 500: 475.1\n",
      "Average loss at step 510: 434.6\n",
      "Average loss at step 520: 170.1\n",
      "Average loss at step 530:  69.4\n",
      "Average loss at step 540: 267.4\n",
      "Average loss at step 550: 149.0\n",
      "Average loss at step 560: 248.0\n",
      "Average loss at step 570: 109.1\n",
      "Average loss at step 580: 160.4\n",
      "Average loss at step 590: 183.8\n",
      "Average loss at step 600: 166.6\n",
      "Average loss at step 610: 169.6\n",
      "Average loss at step 620: 177.6\n",
      "Average loss at step 630: 140.9\n",
      "Average loss at step 640:  74.3\n",
      "Average loss at step 650: 140.8\n",
      "Average loss at step 660: 158.8\n",
      "Average loss at step 670:  99.5\n",
      "Average loss at step 680: 104.6\n",
      "Average loss at step 690: 171.0\n",
      "Average loss at step 700: 152.0\n",
      "Average loss at step 710:  44.5\n",
      "Average loss at step 720:  97.3\n",
      "Average loss at step 730:  82.9\n",
      "Average loss at step 740:  14.9\n",
      "Average loss at step 750: 132.5\n",
      "Average loss at step 760:  59.2\n",
      "Average loss at step 770:  43.5\n",
      "Average loss at step 780:  62.8\n",
      "Average loss at step 790:  57.4\n",
      "Average loss at step 800: 117.5\n",
      "Average loss at step 810:  33.2\n",
      "Average loss at step 820: 100.1\n",
      "Average loss at step 830:  59.1\n",
      "Average loss at step 840:  23.2\n",
      "Average loss at step 850:  82.8\n",
      "Average loss at step 860:  19.7\n",
      "Average loss at step 870:  88.5\n",
      "Average loss at step 880:  18.7\n",
      "Average loss at step 890:  60.3\n",
      "Average loss at step 900:  58.4\n",
      "Average loss at step 910: 119.4\n",
      "Average loss at step 920:  15.8\n",
      "Average loss at step 930:  71.1\n",
      "Average loss at step 940:  21.9\n",
      "Average loss at step 950:  37.4\n",
      "Average loss at step 960:  57.6\n",
      "Average loss at step 970: 105.5\n",
      "Average loss at step 980:  12.8\n",
      "Average loss at step 990:  29.0\n",
      "Average loss at step 1000:  72.0\n",
      "Average loss at step 1010:  36.8\n",
      "Average loss at step 1020:  63.4\n",
      "Average loss at step 1030:  95.9\n",
      "Average loss at step 1040:  55.1\n",
      "Average loss at step 1050:  48.7\n",
      "Average loss at step 1060:   6.0\n",
      "Average loss at step 1070:  42.5\n",
      "Average loss at step 1080:  33.7\n",
      "Average loss at step 1090:  30.5\n",
      "Average loss at step 1100:  88.0\n",
      "Average loss at step 1110:   7.2\n",
      "Average loss at step 1120:   6.3\n",
      "Average loss at step 1130:  27.7\n",
      "Average loss at step 1140:  13.5\n",
      "Average loss at step 1150:   9.9\n",
      "Average loss at step 1160:   6.5\n",
      "Average loss at step 1170:  33.5\n",
      "Average loss at step 1180:   2.9\n",
      "Average loss at step 1190:  24.5\n",
      "Average loss at step 1200:  14.2\n",
      "Average loss at step 1210:  16.5\n",
      "Average loss at step 1220:  19.1\n",
      "Average loss at step 1230:  60.7\n",
      "Average loss at step 1240:  19.4\n",
      "Average loss at step 1250:  17.9\n",
      "Average loss at step 1260:  47.6\n",
      "Average loss at step 1270:   6.3\n",
      "Average loss at step 1280:   2.2\n",
      "Average loss at step 1290:  16.7\n",
      "Average loss at step 1300:  11.0\n",
      "Average loss at step 1310:  46.7\n",
      "Average loss at step 1320: 125.8\n",
      "Average loss at step 1330:  47.7\n",
      "Average loss at step 1340:  18.2\n",
      "Average loss at step 1350:  55.2\n",
      "Average loss at step 1360:  14.4\n",
      "Average loss at step 1370:  12.4\n",
      "Average loss at step 1380:  16.6\n",
      "Average loss at step 1390:  42.9\n",
      "Average loss at step 1400:   6.4\n",
      "Average loss at step 1410:  39.9\n",
      "Average loss at step 1420:   9.6\n",
      "Average loss at step 1430:   3.8\n",
      "Average loss at step 1440:   2.2\n",
      "Average loss at step 1450:   5.9\n",
      "Average loss at step 1460:  23.3\n",
      "Average loss at step 1470:  21.1\n",
      "Average loss at step 1480:   5.0\n",
      "Average loss at step 1490:  34.8\n",
      "Average loss at step 1500:   2.2\n",
      "Average loss at step 1510:  33.1\n",
      "Average loss at step 1520:  44.6\n",
      "Average loss at step 1530:  25.9\n",
      "Average loss at step 1540:   4.3\n",
      "Average loss at step 1550:  39.3\n",
      "Average loss at step 1560:  85.4\n",
      "Average loss at step 1570:   5.5\n",
      "Average loss at step 1580:   4.9\n",
      "Average loss at step 1590:   3.9\n",
      "Average loss at step 1600:   2.8\n",
      "Average loss at step 1610:  13.7\n",
      "Average loss at step 1620:  20.8\n",
      "Average loss at step 1630:  18.3\n",
      "Average loss at step 1640:  32.5\n",
      "Average loss at step 1650:   2.2\n",
      "Average loss at step 1660:   3.2\n",
      "Average loss at step 1670:   3.7\n",
      "Average loss at step 1680:   2.3\n",
      "Average loss at step 1690:   2.2\n",
      "Average loss at step 1700:   2.3\n",
      "Average loss at step 1710:   2.2\n",
      "Optimization Finished!\n",
      "Total time: 368.33568596839905 seconds\n",
      "Accuracy 0.1399\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "N_CLASSES = 10\n",
    "\n",
    "# Step 2: Define paramaters for the model\n",
    "LEARNING_RATE = 0.01\n",
    "BATCH_SIZE = 64\n",
    "SKIP_STEP = 10\n",
    "DROPOUT = 0.80\n",
    "N_EPOCHS = 2\n",
    "\n",
    "\n",
    "with tf.name_scope('data'):\n",
    "    X = tf.placeholder(tf.float32, [None, 784], name=\"X_placeholder\")\n",
    "    Y = tf.placeholder(tf.float32, [None, 10], name=\"Y_placeholder\")\n",
    "\n",
    "dropout = tf.placeholder(tf.float32, name='dropout')\n",
    "\n",
    "global_step = tf.Variable(0, dtype=tf.int32, trainable=False, name='global_step')\n",
    "\n",
    "\n",
    "with tf.variable_scope('fc0') as scope:\n",
    "    \n",
    "    w = tf.get_variable('weights', shape=[784, 1024], initializer=tf.truncated_normal_initializer())\n",
    "    b = tf.get_variable('bias', shape=[1024], initializer=tf.constant_initializer(0.0))\n",
    "    \n",
    "    fc0 = tf.nn.relu(tf.add(tf.matmul(X, w), b))\n",
    "    \n",
    "with tf.variable_scope('fc1') as scope:\n",
    "    \n",
    "    w = tf.get_variable('weights', shape=[1024, 784], initializer=tf.truncated_normal_initializer())\n",
    "    b = tf.get_variable('bias', shape=[784], initializer=tf.constant_initializer(0.0))\n",
    "    \n",
    "    fc1 = tf.add(tf.nn.relu(tf.add(tf.matmul(fc0, w), b)), X)\n",
    "    \n",
    "\n",
    "with tf.variable_scope('conv1') as scope:\n",
    "    \n",
    "    images = tf.reshape(fc1, shape=[-1, 28, 28, 1])\n",
    "    \n",
    "    \n",
    "    kernel = tf.get_variable('kernel', shape=[5, 5, 1, 32], initializer=tf.truncated_normal_initializer())\n",
    "    \n",
    "   \n",
    "    \n",
    "    biases = tf.get_variable('biases', shape=[32], initializer=tf.constant_initializer(0.0))\n",
    "    \n",
    "   \n",
    "    conv = tf.nn.conv2d(images, kernel, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    \n",
    "   \n",
    "    conv1 = tf.nn.relu(conv + biases, name='conv1')\n",
    "    \n",
    "    \n",
    "    \n",
    "with tf.variable_scope('pool1') as scope:\n",
    "   \n",
    "    \n",
    "    pool1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], name='pool1', strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "    \n",
    "\n",
    "with tf.variable_scope('conv2') as scope:\n",
    "\n",
    "    kernel = tf.get_variable('kernels', [5, 5, 32, 64], \n",
    "                        initializer=tf.truncated_normal_initializer())\n",
    "    biases = tf.get_variable('biases', [64],\n",
    "                        initializer=tf.random_normal_initializer())\n",
    "    conv = tf.nn.conv2d(pool1, kernel, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    conv2 = tf.nn.relu(conv + biases, name=scope.name)\n",
    "\n",
    "\n",
    "with tf.variable_scope('pool2') as scope:\n",
    "\n",
    "    pool2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1],\n",
    "                            padding='SAME')\n",
    "\n",
    "   \n",
    "    \n",
    "\n",
    "with tf.variable_scope('fc') as scope:\n",
    "\n",
    "    input_features = 7 * 7 * 64\n",
    "    \n",
    "   \n",
    "    w = tf.get_variable('weights', shape=[input_features, 1024], initializer=tf.truncated_normal_initializer())\n",
    "    b = tf.get_variable('bias', shape=[1024], initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "    pool2 = tf.reshape(pool2, [-1, input_features])\n",
    "\n",
    "    fc = tf.nn.relu(tf.matmul(pool2, w) + b, name='relu')\n",
    "    \n",
    "   \n",
    "    fc = tf.nn.dropout(fc, dropout, name='relu_dropout')\n",
    "\n",
    "with tf.variable_scope('softmax_linear') as scope:\n",
    "    \n",
    "    w = tf.get_variable('weights', shape=[1024, N_CLASSES], initializer=tf.truncated_normal_initializer())\n",
    "    b = tf.get_variable('bias', shape=[N_CLASSES], initializer=tf.constant_initializer(0.0))\n",
    "    \n",
    "    logits = tf.matmul(fc, w) + b\n",
    "\n",
    "\n",
    "    \n",
    "with tf.name_scope('loss'):\n",
    "   \n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=Y, logits=logits), name='loss')\n",
    "\n",
    "\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE).minimize(loss, global_step=global_step)\n",
    "\n",
    "with tf.name_scope('summary'):\n",
    "    tf.summary.scalar('loss', loss)\n",
    "    summary_op = tf.summary.merge_all()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    writer = tf.summary.FileWriter('../my_graph/mnist4', sess.graph)\n",
    "   \n",
    "    \n",
    "    initial_step = global_step.eval()\n",
    "\n",
    "    start_time = time.time()\n",
    "    n_batches = int(mnist.train.num_examples / BATCH_SIZE)\n",
    "\n",
    "    total_loss = 0.0\n",
    "    for index in range(initial_step, n_batches * N_EPOCHS): # train the model n_epochs times\n",
    "        X_batch, Y_batch = mnist.train.next_batch(BATCH_SIZE)\n",
    "        _, loss_batch, summary = sess.run([optimizer, loss, summary_op], \n",
    "                                feed_dict={X: X_batch, Y:Y_batch, dropout: DROPOUT}) \n",
    "        total_loss += loss_batch\n",
    "        writer.add_summary(summary, global_step=index)\n",
    "        if (index + 1) % SKIP_STEP == 0:\n",
    "            print('Average loss at step {}: {:5.1f}'.format(index + 1, total_loss / SKIP_STEP))\n",
    "            total_loss = 0.0\n",
    "            saver.save(sess, '../checkpoints/convnet_mnist4/mnist-convnet', index)\n",
    "    \n",
    "    print(\"Optimization Finished!\") # should be around 0.35 after 25 epochs\n",
    "    print(\"Total time: {0} seconds\".format(time.time() - start_time))\n",
    "    \n",
    "    # test the model\n",
    "    n_batches = int(mnist.test.num_examples/BATCH_SIZE)\n",
    "    total_correct_preds = 0\n",
    "    for i in range(n_batches):\n",
    "        X_batch, Y_batch = mnist.test.next_batch(BATCH_SIZE)\n",
    "        _, loss_batch, logits_batch = sess.run([optimizer, loss, logits], \n",
    "                                        feed_dict={X: X_batch, Y:Y_batch, dropout: DROPOUT}) \n",
    "        preds = tf.nn.softmax(logits_batch)\n",
    "        correct_preds = tf.equal(tf.argmax(preds, 1), tf.argmax(Y_batch, 1))\n",
    "        accuracy = tf.reduce_sum(tf.cast(correct_preds, tf.float32))\n",
    "        total_correct_preds += sess.run(accuracy)   \n",
    "    \n",
    "    print(\"Accuracy {0}\".format(total_correct_preds/mnist.test.num_examples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# (2 epochs) fc then fc with resnet with relu then 2 conv nets then 1 fc with dropout -> Accuracy 0.1399 :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
