{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "torch.set_default_tensor_type('torch.FloatTensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def squash(s, dim=-1, epsilon=1e-3):\n",
    "#     print(s.size())\n",
    "    norm_s = torch.sqrt(torch.norm(s) + epsilon)\n",
    "#     print(norm_s)\n",
    "    v = (norm_s / (1 + norm_s)) * (s / norm_s)\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://discuss.pytorch.org/t/why-softmax-function-cant-specify-the-dimension-to-operate/2637\n",
    "def softmax(input, axis=1):\n",
    "    input_size = input.size()\n",
    "    \n",
    "    trans_input = input.transpose(axis, len(input_size)-1)\n",
    "    trans_size = trans_input.size()\n",
    "\n",
    "    input_2d = trans_input.contiguous().view(-1, trans_size[-1])\n",
    "    \n",
    "    soft_max_2d = nn.functional.softmax(input_2d)\n",
    "    \n",
    "    soft_max_nd = soft_max_2d.view(*trans_size)\n",
    "    return soft_max_nd.transpose(axis, len(input_size)-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class CapsuleLayer(nn.Module):\n",
    "    def __init__(self, in_channels, capsule_dimension, num_capsules, kernel_size=None, routing=False, num_iterations=0, stride=1):\n",
    "        \n",
    "        super(CapsuleLayer, self).__init__()\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        self.capsule_dimension = capsule_dimension\n",
    "        self.num_capsules = num_capsules\n",
    "        self.kernel_size = kernel_size\n",
    "        self.routing = routing\n",
    "        self.num_iterations = num_iterations\n",
    "        self.stride = stride\n",
    "        \n",
    "        if not self.routing:\n",
    "            \n",
    "            self.conv = nn.Conv2d(self.in_channels, self.capsule_dimension * self.num_capsules,\\\n",
    "                                  self.kernel_size, self.stride)      \n",
    "            \n",
    "        \n",
    "        else:\n",
    "            \n",
    "            # todo: to be calculated later\n",
    "            self.width = 1152\n",
    "            \n",
    "            self.weights = torch.autograd.Variable(torch.from_numpy(0.01 * np.random.randn(1, self.width,\\\n",
    "                                                                                           self.num_capsules, self.capsule_dimension,\\\n",
    "                                                                                           self.in_channels)), requires_grad=True)\n",
    "            \n",
    "            \n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        if not self.routing:\n",
    "            \n",
    "            self.conv_out = self.conv(x)\n",
    "            \n",
    "            # todo: to be calculated later\n",
    "            width = 6 # (W - F + 2 * P) / S + 1\n",
    "            \n",
    "            conv2 = self.conv_out.view(-1, width * width * self.num_capsules, self.capsule_dimension)\n",
    "            \n",
    "            squash_conv = squash(conv2)\n",
    "            \n",
    "            return squash_conv\n",
    "                \n",
    "                \n",
    "        else:\n",
    "            # x -> [batch size , 1152, 8]\n",
    "             \n",
    "            batch_size = x.size()[0]\n",
    "            \n",
    "#             print(\"x\", x.size())\n",
    "#             print(\"w\", self.weights.size())\n",
    "            \n",
    "            u = x.unsqueeze(2).expand(x.size()[0], x.size()[1], self.num_capsules, x.size()[2]).unsqueeze(4).type(torch.FloatTensor)\n",
    "            \n",
    "            self.weights = self.weights.expand(batch_size, self.weights.size()[1], self.weights.size()[2],\\\n",
    "                                              self.weights.size()[3], self.weights.size()[4]).type(torch.FloatTensor)\n",
    "    \n",
    "#             print(\"w with batch\", self.weights.size())\n",
    "#             print(\"u\", u.size())\n",
    "            \n",
    "#             print(\"w type\", type(self.weights))\n",
    "#             print(\"u type\", type(u))\n",
    "                    \n",
    "            u_hat = torch.matmul(self.weights, u)\n",
    "        \n",
    "#             print(\"u_hat\", u_hat.size())\n",
    "            \n",
    "            v = None\n",
    "        \n",
    "            b = Variable(torch.zeros(batch_size, x.size()[1], self.num_capsules, 1, 1).type(torch.FloatTensor), requires_grad=True)\n",
    "            \n",
    "            for r in range(self.num_iterations):\n",
    "                \n",
    "#                 print(\"b\", b.size())\n",
    "                \n",
    "                c = softmax(b, axis=2)\n",
    "                \n",
    "#                 c = c.expand(c.size()[0], c.size()[1], c.size()[2], u_hat.size()[3], c.size()[4])\n",
    "                \n",
    "#                 print(\"c\", c.size())\n",
    "                \n",
    "#                 print(\"c numpy\", c.data.cpu().numpy().shape)\n",
    "#                 print(\"u_hat numpy\", u_hat.data.cpu().numpy().shape)\n",
    "\n",
    "#                 s_numpy = np.matrix.dot(c.data.cpu().numpy(), u_hat.data.cpu().numpy())\n",
    "                \n",
    "#                 print(\"s_numpy\", s_numpy.shape)\n",
    "                \n",
    "                s = c * u_hat\n",
    "                \n",
    "#                 print(\"s\", s.size())\n",
    "                \n",
    "                s = torch.sum(s, dim=1)\n",
    "                \n",
    "#                 print(\"s\", s.size())\n",
    "                \n",
    "#                 s = s.unsqueeze(2)\n",
    "                \n",
    "#                 print(\"s\", s.size())\n",
    "                \n",
    "                v = squash(s, dim=-2)\n",
    "                \n",
    "#                 print(\"v\", v.size())\n",
    "                                \n",
    "                temp1 = v.unsqueeze(1).expand(v.size()[0], u_hat.size()[1], v.size()[1], v.size()[2], v.size()[3])\n",
    "            \n",
    "                v = v.unsqueeze(1)\n",
    "            \n",
    "#                 print(\"v\", v.size())\n",
    "            \n",
    "                temp = torch.matmul(u_hat.transpose(-1, -2), temp1)\n",
    "                \n",
    "#                 print(\"temp\", temp.size())\n",
    "                \n",
    "#                 print(\"temp type\", type(temp))\n",
    "                \n",
    "#                 print(\"b type\", type(b))\n",
    "                \n",
    "                b = b + temp\n",
    "                \n",
    "#                 print(\"b\", b.size())\n",
    "                \n",
    "            return v\n",
    "                \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        \n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 256, 9)\n",
    "        \n",
    "        self.primary_capsule = CapsuleLayer(256, 8, 32, kernel_size=9, routing=False, num_iterations=0, stride=2)\n",
    "        \n",
    "        self.digital_capsule = CapsuleLayer(8, 16, 10, kernel_size=None, routing=True, num_iterations=5, stride=1)\n",
    "        \n",
    "        self.criterion = torch.nn.MSELoss(size_average=True)\n",
    "        \n",
    "        \n",
    "    def forward(self, x, target):\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        \n",
    "        x = nn.functional.relu(x)\n",
    "        \n",
    "        x = self.primary_capsule(x)\n",
    "        \n",
    "        x = self.digital_capsule(x)\n",
    "        \n",
    "#         print(\"x\", x.size())\n",
    "        \n",
    "        x = torch.squeeze(x)\n",
    "        \n",
    "#         print(\"x\", x.size())\n",
    "                \n",
    "        x = torch.sqrt(torch.norm(x, dim=2) + 1e-3)\n",
    "        \n",
    "        x = x.squeeze()\n",
    "        \n",
    "#         print(\"x\", x.size())\n",
    "        \n",
    "        _, y_predicted = torch.max(x, dim=1)\n",
    "        \n",
    "#         print(list(zip(y_predicted, target)))\n",
    "\n",
    "#         print(y_predicted == target)\n",
    "                \n",
    "        \n",
    "#         print(y_predicted.size(), target.size())\n",
    "                \n",
    "#         loss = torch.sum(y_predicted != target)\n",
    "\n",
    "        loss = self.criterion(y_predicted.type(torch.FloatTensor), target.type(torch.FloatTensor))\n",
    "        \n",
    "        return x, loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net (\n",
       "  (conv1): Conv2d(1, 256, kernel_size=(9, 9), stride=(1, 1))\n",
       "  (primary_capsule): CapsuleLayer (\n",
       "    (conv): Conv2d(256, 256, kernel_size=(9, 9), stride=(2, 2))\n",
       "  )\n",
       "  (digital_capsule): CapsuleLayer (\n",
       "  )\n",
       "  (criterion): MSELoss (\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "## load mnist dataset\n",
    "root = '../data'\n",
    "download = False\n",
    "trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))])\n",
    "train_set = dset.MNIST(root=root, train=True, transform=trans, download=download)\n",
    "test_set = dset.MNIST(root=root, train=False, transform=trans)\n",
    "\n",
    "batch_size = 128\n",
    "# kwargs = {'num_workers': 1}\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "                 dataset=train_set,\n",
    "                 batch_size=batch_size,\n",
    "                 shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "                dataset=test_set,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "there are no graph nodes that require computing gradients",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-61c1f94b2626>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#         if batch_idx % 100 == 0:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/apple/anaconda/lib/python3.5/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \"\"\"\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/apple/anaconda/lib/python3.5/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 98\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: there are no graph nodes that require computing gradients"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(net.parameters(), lr=0.01)\n",
    "for epoch in range(10):\n",
    "    # trainning\n",
    "    for batch_idx, (x, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        x, target = Variable(x), Variable(target)\n",
    "        _, loss = net(x, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "#         if batch_idx % 100 == 0:\n",
    "        print('==>>> epoch: {}, batch index: {}, train loss: {:.6f}'.format(epoch, batch_idx, loss.data[0]))\n",
    "#     # testing\n",
    "#     correct_cnt, ave_loss = 0, 0\n",
    "#     for batch_idx, (x, target) in enumerate(test_loader):\n",
    "#         x, target = Variable(x, volatile=True), Variable(target, volatile=True)\n",
    "#         score, loss = net(x, target)\n",
    "#         _, pred_label = torch.max(score.data, 1)\n",
    "#         correct_cnt += (pred_label == target.data).sum()\n",
    "#         ave_loss += loss.data[0]\n",
    "#     accuracy = correct_cnt*1.0/len(test_loader)/batch_size\n",
    "#     ave_loss /= len(test_loader)\n",
    "#     print('==>>> epoch: {}, test loss: {:.6f}, accuracy: {:.4f}'.format(epoch, ave_loss, accuracy))\n",
    "\n",
    "torch.save(net.state_dict(), 'capsule_net')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
