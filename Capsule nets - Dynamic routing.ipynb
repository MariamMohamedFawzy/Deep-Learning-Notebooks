{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#https://github.com/ageron/handson-ml/blob/master/extra_capsnets.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "torch.set_default_tensor_type('torch.FloatTensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def squash(s, dim=-1, epsilon=1e-3):\n",
    "#     print(s.size())\n",
    "    norm_s = torch.sqrt(torch.norm(s) + epsilon)\n",
    "#     print(norm_s)\n",
    "    v = (norm_s / (1 + norm_s)) * (s / norm_s)\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://discuss.pytorch.org/t/why-softmax-function-cant-specify-the-dimension-to-operate/2637\n",
    "def softmax(input, axis=1):\n",
    "    input_size = input.size()\n",
    "    \n",
    "    trans_input = input.transpose(axis, len(input_size)-1)\n",
    "    trans_size = trans_input.size()\n",
    "\n",
    "    input_2d = trans_input.contiguous().view(-1, trans_size[-1])\n",
    "    \n",
    "    soft_max_2d = nn.functional.softmax(input_2d)\n",
    "    \n",
    "    soft_max_nd = soft_max_2d.view(*trans_size)\n",
    "    return soft_max_nd.transpose(axis, len(input_size)-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class CapsuleLayer(nn.Module):\n",
    "    def __init__(self, in_channels, capsule_dimension, num_capsules, kernel_size=None, routing=False, num_iterations=0, stride=1):\n",
    "        \n",
    "        super(CapsuleLayer, self).__init__()\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        self.capsule_dimension = capsule_dimension\n",
    "        self.num_capsules = num_capsules\n",
    "        self.kernel_size = kernel_size\n",
    "        self.routing = routing\n",
    "        self.num_iterations = num_iterations\n",
    "        self.stride = stride\n",
    "        \n",
    "        if not self.routing:\n",
    "            \n",
    "            self.conv = nn.Conv2d(self.in_channels, self.capsule_dimension * self.num_capsules,\\\n",
    "                                  self.kernel_size, self.stride)      \n",
    "            \n",
    "        \n",
    "        else:\n",
    "            \n",
    "            # todo: to be calculated later\n",
    "            self.width = 1152\n",
    "            \n",
    "            self.weights = torch.autograd.Variable(torch.from_numpy(0.01 * np.random.randn(1, self.width,\\\n",
    "                                                                                           self.num_capsules, self.capsule_dimension,\\\n",
    "                                                                                           self.in_channels)), requires_grad=True)\n",
    "            \n",
    "            \n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        if not self.routing:\n",
    "            \n",
    "            self.conv_out = self.conv(x)\n",
    "            \n",
    "            # todo: to be calculated later\n",
    "            width = 6 # (W - F + 2 * P) / S + 1\n",
    "            \n",
    "            conv2 = self.conv_out.view(-1, width * width * self.num_capsules, self.capsule_dimension)\n",
    "            \n",
    "            squash_conv = squash(conv2)\n",
    "            \n",
    "#             print(\"squash conv\", type(squash_conv))\n",
    "            \n",
    "            return squash_conv\n",
    "                \n",
    "                \n",
    "        else:\n",
    "            # x -> [batch size , 1152, 8]\n",
    "             \n",
    "            batch_size = x.size()[0]\n",
    "            \n",
    "#             print(\"x\", x.size())\n",
    "#             print(\"w\", self.weights.size())\n",
    "            \n",
    "            u = x.unsqueeze(2).expand(x.size()[0], x.size()[1], self.num_capsules, x.size()[2]).unsqueeze(4).type(torch.FloatTensor)\n",
    "            \n",
    "            self.weights = self.weights.expand(batch_size, self.weights.size()[1], self.weights.size()[2],\\\n",
    "                                              self.weights.size()[3], self.weights.size()[4]).type(torch.FloatTensor)\n",
    "    \n",
    "#             print(\"w with batch\", self.weights.size())\n",
    "#             print(\"u\", u.size())\n",
    "            \n",
    "#             print(\"w type\", type(self.weights))\n",
    "#             print(\"u type\", type(u))\n",
    "                    \n",
    "            u_hat = torch.matmul(self.weights, u)\n",
    "        \n",
    "#             print(\"u_hat\", u_hat.size())\n",
    "            \n",
    "            v = None\n",
    "        \n",
    "            b = Variable(torch.zeros(batch_size, x.size()[1], self.num_capsules, 1, 1).type(torch.FloatTensor), requires_grad=True)\n",
    "            \n",
    "            for r in range(self.num_iterations):\n",
    "                \n",
    "#                 print(\"b\", b.size())\n",
    "                \n",
    "                c = softmax(b, axis=2)\n",
    "        \n",
    "#                 print(c.requires_grad)\n",
    "                \n",
    "#                 c = c.expand(c.size()[0], c.size()[1], c.size()[2], u_hat.size()[3], c.size()[4])\n",
    "                \n",
    "#                 print(\"c\", c.size())\n",
    "                \n",
    "#                 print(\"c numpy\", c.data.cpu().numpy().shape)\n",
    "#                 print(\"u_hat numpy\", u_hat.data.cpu().numpy().shape)\n",
    "\n",
    "#                 s_numpy = np.matrix.dot(c.data.cpu().numpy(), u_hat.data.cpu().numpy())\n",
    "                \n",
    "#                 print(\"s_numpy\", s_numpy.shape)\n",
    "                \n",
    "                s = c * u_hat\n",
    "                \n",
    "#                 print(\"s\", s.size())\n",
    "                \n",
    "                s = torch.sum(s, dim=1)\n",
    "                \n",
    "#                 print(\"s\", s.size())\n",
    "                \n",
    "#                 s = s.unsqueeze(2)\n",
    "                \n",
    "#                 print(\"s\", s.size())\n",
    "                \n",
    "                v = squash(s, dim=-2)\n",
    "                \n",
    "#                 print(\"v\", v.size())\n",
    "                                \n",
    "                temp1 = v.unsqueeze(1).expand(v.size()[0], u_hat.size()[1], v.size()[1], v.size()[2], v.size()[3])\n",
    "            \n",
    "                v = v.unsqueeze(1)\n",
    "            \n",
    "#                 print(\"v\", v.size())\n",
    "            \n",
    "                temp = torch.matmul(u_hat.transpose(-1, -2), temp1)\n",
    "                \n",
    "#                 print(\"temp\", temp.size())\n",
    "                \n",
    "#                 print(\"temp type\", type(temp))\n",
    "                \n",
    "#                 print(\"b type\", type(b))\n",
    "                \n",
    "                b = b + temp\n",
    "                \n",
    "#                 print(\"b\", b.size())\n",
    "    \n",
    "#             print(\"v\", type(v))\n",
    "\n",
    "#             print(v.requires_grad)\n",
    "                \n",
    "            return v\n",
    "                \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        \n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 256, 9)\n",
    "        \n",
    "        self.primary_capsule = CapsuleLayer(256, 8, 32, kernel_size=9, routing=False, num_iterations=0, stride=2)\n",
    "        \n",
    "        self.digital_capsule = CapsuleLayer(8, 16, 10, kernel_size=None, routing=True, num_iterations=5, stride=1)\n",
    "        \n",
    "        self.criterion = torch.nn.MSELoss(size_average=True)\n",
    "        \n",
    "        \n",
    "    def forward(self, x, target):\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        \n",
    "        x = nn.functional.relu(x)\n",
    "        \n",
    "        x = self.primary_capsule(x)\n",
    "        \n",
    "        x = self.digital_capsule(x)\n",
    "        \n",
    "        x = x.squeeze()\n",
    "        \n",
    "#         print(x.size())\n",
    "        \n",
    "#         y_predicted = torch.squeeze(x, dim=1)\n",
    "        \n",
    "        \n",
    "#         start\n",
    "                \n",
    "        x = torch.sqrt(torch.norm(x, dim=2) + 1e-3)\n",
    "        \n",
    "        x = x.squeeze()\n",
    "        \n",
    "        \n",
    "#         sof = nn.functional.softmax(x)\n",
    "                \n",
    "#         values, _ = torch.max(sof, dim=1)\n",
    "        \n",
    "#         aa = values.data.cpu().numpy()\n",
    "#         aa = aa.reshape(aa.shape[0], 1)\n",
    "        \n",
    "#         vvv = x.data.cpu().numpy() == aa\n",
    "        \n",
    "#         y_predicted_temp = vvv + 0\n",
    "        \n",
    "#         y_predicted = Variable(torch.from_numpy(y_predicted_temp), requires_grad=True).type(torch.FloatTensor)\n",
    "        \n",
    "        \n",
    "        y_predicted = x\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#         end\n",
    "        \n",
    "#         y_predicted = nn.Softmax()(x)\n",
    "        \n",
    "#         print(y_predicted)\n",
    "        \n",
    "#         print(\"y\", y_predicted.size())\n",
    "        \n",
    "        #start\n",
    "#         n_labels = 10\n",
    "        \n",
    "#         targets = target.data.cpu().numpy()\n",
    "\n",
    "#         ohm = np.zeros((targets.shape[0], n_labels))\n",
    "#         #empty one-hot matrix\n",
    "#         ohm[np.arange(targets.shape[0]), targets] = 1\n",
    "        \n",
    "#         target = Variable(torch.from_numpy(ohm), requires_grad=False).type(torch.LongTensor)\n",
    "        #end\n",
    "        \n",
    "#         print(\"target\", target.size())\n",
    "        \n",
    "#         print(target.requires_grad)\n",
    "#         print(y_predicted.requires_grad)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "#         _, y_predicted = torch.max(x, dim=1)\n",
    "        \n",
    "#         print(y_predicted.requires_grad)\n",
    "        \n",
    "#         temp3 = np.eye(10) * (y_predicted * 10)\n",
    "        \n",
    "#         print(temp3)\n",
    "        \n",
    "        \n",
    "#         print(list(zip(y_predicted, target)))\n",
    "\n",
    "#         print(y_predicted == target)\n",
    "                \n",
    "        \n",
    "#         print(y_predicted.size(), target.size())\n",
    "                \n",
    "#         loss = torch.sum(y_predicted != target)\n",
    "\n",
    "#         loss = self.criterion(y_predicted, target)\n",
    "\n",
    "#         loss = mse_loss(y_predicted, target)\n",
    "\n",
    "#         loss = nn.functional.nll_loss(nn.functional.log_softmax(y_predicted), target)\n",
    "\n",
    "#         loss = self.criterion(y_predicted, target)\n",
    "\n",
    "        target = target.type(torch.LongTensor)\n",
    "    \n",
    "#         print(target.size(), y_predicted.size())\n",
    "#         print(target.data.type())\n",
    "#         print(y_predicted.data.type())\n",
    "\n",
    "        loss_fn = nn.CrossEntropyLoss() \n",
    "        loss = loss_fn(y_predicted, target)\n",
    "\n",
    "    \n",
    "        return x, loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net (\n",
       "  (conv1): Conv2d(1, 256, kernel_size=(9, 9), stride=(1, 1))\n",
       "  (primary_capsule): CapsuleLayer (\n",
       "    (conv): Conv2d(256, 256, kernel_size=(9, 9), stride=(2, 2))\n",
       "  )\n",
       "  (digital_capsule): CapsuleLayer (\n",
       "  )\n",
       "  (criterion): MSELoss (\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "## load mnist dataset\n",
    "root = '../data'\n",
    "download = False\n",
    "trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))])\n",
    "train_set = dset.MNIST(root=root, train=True, transform=trans, download=download)\n",
    "test_set = dset.MNIST(root=root, train=False, transform=trans)\n",
    "\n",
    "batch_size = 128\n",
    "# kwargs = {'num_workers': 1}\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "                 dataset=train_set,\n",
    "                 batch_size=batch_size,\n",
    "                 shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "                dataset=test_set,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>>> epoch: 0, batch index: 0, train loss: 2.289733\n",
      "==>>> epoch: 0, batch index: 1, train loss: 2.253983\n",
      "==>>> epoch: 0, batch index: 2, train loss: 2.242553\n",
      "==>>> epoch: 0, batch index: 3, train loss: 2.219036\n",
      "==>>> epoch: 0, batch index: 4, train loss: 2.213325\n",
      "==>>> epoch: 0, batch index: 5, train loss: 2.206645\n",
      "==>>> epoch: 0, batch index: 6, train loss: 2.198196\n",
      "==>>> epoch: 0, batch index: 7, train loss: 2.204408\n",
      "==>>> epoch: 0, batch index: 8, train loss: 2.184974\n",
      "==>>> epoch: 0, batch index: 9, train loss: 2.172639\n",
      "==>>> epoch: 0, batch index: 10, train loss: 2.165656\n",
      "==>>> epoch: 0, batch index: 11, train loss: 2.163977\n",
      "==>>> epoch: 0, batch index: 12, train loss: 2.158755\n",
      "==>>> epoch: 0, batch index: 13, train loss: 2.164740\n",
      "==>>> epoch: 0, batch index: 14, train loss: 2.164521\n",
      "==>>> epoch: 0, batch index: 15, train loss: 2.148664\n",
      "==>>> epoch: 0, batch index: 16, train loss: 2.136095\n",
      "==>>> epoch: 0, batch index: 17, train loss: 2.137916\n",
      "==>>> epoch: 0, batch index: 18, train loss: 2.140372\n",
      "==>>> epoch: 0, batch index: 19, train loss: 2.140035\n",
      "==>>> epoch: 0, batch index: 20, train loss: 2.147725\n",
      "==>>> epoch: 0, batch index: 21, train loss: 2.139385\n",
      "==>>> epoch: 0, batch index: 22, train loss: 2.129932\n",
      "==>>> epoch: 0, batch index: 23, train loss: 2.122820\n",
      "==>>> epoch: 0, batch index: 24, train loss: 2.130529\n",
      "==>>> epoch: 0, batch index: 25, train loss: 2.125401\n",
      "==>>> epoch: 0, batch index: 26, train loss: 2.105772\n",
      "==>>> epoch: 0, batch index: 27, train loss: 2.117095\n",
      "==>>> epoch: 0, batch index: 28, train loss: 2.117487\n",
      "==>>> epoch: 0, batch index: 29, train loss: 2.115206\n",
      "==>>> epoch: 0, batch index: 30, train loss: 2.117260\n",
      "==>>> epoch: 0, batch index: 31, train loss: 2.098955\n",
      "==>>> epoch: 0, batch index: 32, train loss: 2.093374\n",
      "==>>> epoch: 0, batch index: 33, train loss: 2.103683\n",
      "==>>> epoch: 0, batch index: 34, train loss: 2.104678\n",
      "==>>> epoch: 0, batch index: 35, train loss: 2.111905\n",
      "==>>> epoch: 0, batch index: 36, train loss: 2.093878\n",
      "==>>> epoch: 0, batch index: 37, train loss: 2.082913\n",
      "==>>> epoch: 0, batch index: 38, train loss: 2.113271\n",
      "==>>> epoch: 0, batch index: 39, train loss: 2.083338\n",
      "==>>> epoch: 0, batch index: 40, train loss: 2.102430\n",
      "==>>> epoch: 0, batch index: 41, train loss: 2.099445\n",
      "==>>> epoch: 0, batch index: 42, train loss: 2.062312\n",
      "==>>> epoch: 0, batch index: 43, train loss: 2.088987\n",
      "==>>> epoch: 0, batch index: 44, train loss: 2.090419\n",
      "==>>> epoch: 0, batch index: 45, train loss: 2.095112\n",
      "==>>> epoch: 0, batch index: 46, train loss: 2.081264\n",
      "==>>> epoch: 0, batch index: 47, train loss: 2.074622\n",
      "==>>> epoch: 0, batch index: 48, train loss: 2.073843\n",
      "==>>> epoch: 0, batch index: 49, train loss: 2.083281\n",
      "==>>> epoch: 0, batch index: 50, train loss: 2.069584\n",
      "==>>> epoch: 0, batch index: 51, train loss: 2.066899\n",
      "==>>> epoch: 0, batch index: 52, train loss: 2.069177\n",
      "==>>> epoch: 0, batch index: 53, train loss: 2.073006\n",
      "==>>> epoch: 0, batch index: 54, train loss: 2.073566\n",
      "==>>> epoch: 0, batch index: 55, train loss: 2.050894\n",
      "==>>> epoch: 0, batch index: 56, train loss: 2.079881\n",
      "==>>> epoch: 0, batch index: 57, train loss: 2.072027\n",
      "==>>> epoch: 0, batch index: 58, train loss: 2.059759\n",
      "==>>> epoch: 0, batch index: 59, train loss: 2.065878\n",
      "==>>> epoch: 0, batch index: 60, train loss: 2.054066\n",
      "==>>> epoch: 0, batch index: 61, train loss: 2.060395\n",
      "==>>> epoch: 0, batch index: 62, train loss: 2.058383\n",
      "==>>> epoch: 0, batch index: 63, train loss: 2.072492\n",
      "==>>> epoch: 0, batch index: 64, train loss: 2.047209\n",
      "==>>> epoch: 0, batch index: 65, train loss: 2.049573\n",
      "==>>> epoch: 0, batch index: 66, train loss: 2.056910\n",
      "==>>> epoch: 0, batch index: 67, train loss: 2.049162\n",
      "==>>> epoch: 0, batch index: 68, train loss: 2.047112\n",
      "==>>> epoch: 0, batch index: 69, train loss: 2.039285\n",
      "==>>> epoch: 0, batch index: 70, train loss: 2.037331\n",
      "==>>> epoch: 0, batch index: 71, train loss: 2.054552\n",
      "==>>> epoch: 0, batch index: 72, train loss: 2.057428\n",
      "==>>> epoch: 0, batch index: 73, train loss: 2.045666\n",
      "==>>> epoch: 0, batch index: 74, train loss: 2.042265\n",
      "==>>> epoch: 0, batch index: 75, train loss: 2.040141\n",
      "==>>> epoch: 0, batch index: 76, train loss: 2.045011\n",
      "==>>> epoch: 0, batch index: 77, train loss: 2.050704\n",
      "==>>> epoch: 0, batch index: 78, train loss: 2.031040\n",
      "==>>> epoch: 0, batch index: 79, train loss: 2.026893\n",
      "==>>> epoch: 0, batch index: 80, train loss: 2.035781\n",
      "==>>> epoch: 0, batch index: 81, train loss: 2.046370\n",
      "==>>> epoch: 0, batch index: 82, train loss: 2.023517\n",
      "==>>> epoch: 0, batch index: 83, train loss: 2.043249\n",
      "==>>> epoch: 0, batch index: 84, train loss: 2.034228\n",
      "==>>> epoch: 0, batch index: 85, train loss: 2.048950\n",
      "==>>> epoch: 0, batch index: 86, train loss: 2.022650\n",
      "==>>> epoch: 0, batch index: 87, train loss: 2.023480\n",
      "==>>> epoch: 0, batch index: 88, train loss: 2.033712\n",
      "==>>> epoch: 0, batch index: 89, train loss: 2.041481\n",
      "==>>> epoch: 0, batch index: 90, train loss: 2.031900\n",
      "==>>> epoch: 0, batch index: 91, train loss: 2.032292\n",
      "==>>> epoch: 0, batch index: 92, train loss: 2.019160\n",
      "==>>> epoch: 0, batch index: 93, train loss: 2.020839\n",
      "==>>> epoch: 0, batch index: 94, train loss: 2.011956\n",
      "==>>> epoch: 0, batch index: 95, train loss: 2.031162\n",
      "==>>> epoch: 0, batch index: 96, train loss: 2.021382\n",
      "==>>> epoch: 0, batch index: 97, train loss: 2.020720\n",
      "==>>> epoch: 0, batch index: 98, train loss: 2.042783\n",
      "==>>> epoch: 0, batch index: 99, train loss: 2.034075\n",
      "==>>> epoch: 0, batch index: 100, train loss: 2.012050\n",
      "==>>> epoch: 0, batch index: 101, train loss: 2.027000\n",
      "==>>> epoch: 0, batch index: 102, train loss: 2.023548\n",
      "==>>> epoch: 0, batch index: 103, train loss: 2.023795\n",
      "==>>> epoch: 0, batch index: 104, train loss: 1.997149\n",
      "==>>> epoch: 0, batch index: 105, train loss: 2.004736\n",
      "==>>> epoch: 0, batch index: 106, train loss: 2.024624\n",
      "==>>> epoch: 0, batch index: 107, train loss: 2.012424\n",
      "==>>> epoch: 0, batch index: 108, train loss: 2.006900\n",
      "==>>> epoch: 0, batch index: 109, train loss: 2.014499\n",
      "==>>> epoch: 0, batch index: 110, train loss: 2.019734\n",
      "==>>> epoch: 0, batch index: 111, train loss: 2.005574\n",
      "==>>> epoch: 0, batch index: 112, train loss: 2.023515\n",
      "==>>> epoch: 0, batch index: 113, train loss: 2.024746\n",
      "==>>> epoch: 0, batch index: 114, train loss: 1.996260\n",
      "==>>> epoch: 0, batch index: 115, train loss: 1.992236\n",
      "==>>> epoch: 0, batch index: 116, train loss: 2.006772\n",
      "==>>> epoch: 0, batch index: 117, train loss: 2.009126\n",
      "==>>> epoch: 0, batch index: 118, train loss: 2.006037\n",
      "==>>> epoch: 0, batch index: 119, train loss: 2.015910\n",
      "==>>> epoch: 0, batch index: 120, train loss: 2.006756\n",
      "==>>> epoch: 0, batch index: 121, train loss: 2.009094\n",
      "==>>> epoch: 0, batch index: 122, train loss: 2.014461\n",
      "==>>> epoch: 0, batch index: 123, train loss: 2.006421\n",
      "==>>> epoch: 0, batch index: 124, train loss: 2.010216\n",
      "==>>> epoch: 0, batch index: 125, train loss: 2.007096\n",
      "==>>> epoch: 0, batch index: 126, train loss: 2.012232\n",
      "==>>> epoch: 0, batch index: 127, train loss: 1.986238\n",
      "==>>> epoch: 0, batch index: 128, train loss: 1.996639\n",
      "==>>> epoch: 0, batch index: 129, train loss: 1.990879\n",
      "==>>> epoch: 0, batch index: 130, train loss: 1.993578\n",
      "==>>> epoch: 0, batch index: 131, train loss: 1.994542\n",
      "==>>> epoch: 0, batch index: 132, train loss: 2.004265\n",
      "==>>> epoch: 0, batch index: 133, train loss: 2.007438\n",
      "==>>> epoch: 0, batch index: 134, train loss: 2.003461\n",
      "==>>> epoch: 0, batch index: 135, train loss: 1.996583\n",
      "==>>> epoch: 0, batch index: 136, train loss: 2.013797\n",
      "==>>> epoch: 0, batch index: 137, train loss: 1.998413\n",
      "==>>> epoch: 0, batch index: 138, train loss: 1.998539\n",
      "==>>> epoch: 0, batch index: 139, train loss: 2.006836\n",
      "==>>> epoch: 0, batch index: 140, train loss: 2.000046\n",
      "==>>> epoch: 0, batch index: 141, train loss: 1.990031\n",
      "==>>> epoch: 0, batch index: 142, train loss: 1.983802\n",
      "==>>> epoch: 0, batch index: 143, train loss: 1.984364\n",
      "==>>> epoch: 0, batch index: 144, train loss: 1.981091\n",
      "==>>> epoch: 0, batch index: 145, train loss: 2.004011\n",
      "==>>> epoch: 0, batch index: 146, train loss: 1.988133\n",
      "==>>> epoch: 0, batch index: 147, train loss: 1.991517\n",
      "==>>> epoch: 0, batch index: 148, train loss: 1.982266\n",
      "==>>> epoch: 0, batch index: 149, train loss: 1.985701\n",
      "==>>> epoch: 0, batch index: 150, train loss: 1.993534\n",
      "==>>> epoch: 0, batch index: 151, train loss: 1.983006\n",
      "==>>> epoch: 0, batch index: 152, train loss: 1.997258\n",
      "==>>> epoch: 0, batch index: 153, train loss: 2.007785\n",
      "==>>> epoch: 0, batch index: 154, train loss: 2.001989\n",
      "==>>> epoch: 0, batch index: 155, train loss: 1.999045\n",
      "==>>> epoch: 0, batch index: 156, train loss: 1.991956\n",
      "==>>> epoch: 0, batch index: 157, train loss: 1.985724\n",
      "==>>> epoch: 0, batch index: 158, train loss: 1.971930\n",
      "==>>> epoch: 0, batch index: 159, train loss: 1.986552\n",
      "==>>> epoch: 0, batch index: 160, train loss: 1.987622\n",
      "==>>> epoch: 0, batch index: 161, train loss: 1.973745\n",
      "==>>> epoch: 0, batch index: 162, train loss: 1.979913\n",
      "==>>> epoch: 0, batch index: 163, train loss: 1.981753\n",
      "==>>> epoch: 0, batch index: 164, train loss: 1.972183\n",
      "==>>> epoch: 0, batch index: 165, train loss: 1.983822\n",
      "==>>> epoch: 0, batch index: 166, train loss: 1.986858\n",
      "==>>> epoch: 0, batch index: 167, train loss: 1.984837\n",
      "==>>> epoch: 0, batch index: 168, train loss: 1.978366\n",
      "==>>> epoch: 0, batch index: 169, train loss: 1.976278\n",
      "==>>> epoch: 0, batch index: 170, train loss: 1.958805\n",
      "==>>> epoch: 0, batch index: 171, train loss: 1.976218\n",
      "==>>> epoch: 0, batch index: 172, train loss: 1.981729\n",
      "==>>> epoch: 0, batch index: 173, train loss: 1.968400\n",
      "==>>> epoch: 0, batch index: 174, train loss: 1.969805\n",
      "==>>> epoch: 0, batch index: 175, train loss: 1.969348\n",
      "==>>> epoch: 0, batch index: 176, train loss: 1.965910\n",
      "==>>> epoch: 0, batch index: 177, train loss: 1.971745\n",
      "==>>> epoch: 0, batch index: 178, train loss: 1.976922\n",
      "==>>> epoch: 0, batch index: 179, train loss: 1.994242\n",
      "==>>> epoch: 0, batch index: 180, train loss: 1.968479\n",
      "==>>> epoch: 0, batch index: 181, train loss: 1.958370\n",
      "==>>> epoch: 0, batch index: 182, train loss: 1.969820\n",
      "==>>> epoch: 0, batch index: 183, train loss: 1.968754\n",
      "==>>> epoch: 0, batch index: 184, train loss: 1.977473\n",
      "==>>> epoch: 0, batch index: 185, train loss: 1.955063\n",
      "==>>> epoch: 0, batch index: 186, train loss: 1.963853\n",
      "==>>> epoch: 0, batch index: 187, train loss: 1.970495\n",
      "==>>> epoch: 0, batch index: 188, train loss: 1.965005\n",
      "==>>> epoch: 0, batch index: 189, train loss: 1.966234\n",
      "==>>> epoch: 0, batch index: 190, train loss: 1.977179\n",
      "==>>> epoch: 0, batch index: 191, train loss: 1.951866\n",
      "==>>> epoch: 0, batch index: 192, train loss: 1.965681\n",
      "==>>> epoch: 0, batch index: 193, train loss: 1.965460\n",
      "==>>> epoch: 0, batch index: 194, train loss: 1.952183\n",
      "==>>> epoch: 0, batch index: 195, train loss: 1.954725\n",
      "==>>> epoch: 0, batch index: 196, train loss: 1.945234\n",
      "==>>> epoch: 0, batch index: 197, train loss: 1.949273\n",
      "==>>> epoch: 0, batch index: 198, train loss: 1.974091\n",
      "==>>> epoch: 0, batch index: 199, train loss: 1.954012\n",
      "==>>> epoch: 0, batch index: 200, train loss: 1.949449\n",
      "==>>> epoch: 0, batch index: 201, train loss: 1.955545\n",
      "==>>> epoch: 0, batch index: 202, train loss: 1.966398\n",
      "==>>> epoch: 0, batch index: 203, train loss: 1.945967\n",
      "==>>> epoch: 0, batch index: 204, train loss: 1.948900\n",
      "==>>> epoch: 0, batch index: 205, train loss: 1.961739\n",
      "==>>> epoch: 0, batch index: 206, train loss: 1.945311\n",
      "==>>> epoch: 0, batch index: 207, train loss: 1.939633\n",
      "==>>> epoch: 0, batch index: 208, train loss: 1.951589\n",
      "==>>> epoch: 0, batch index: 209, train loss: 1.963446\n",
      "==>>> epoch: 0, batch index: 210, train loss: 1.938113\n",
      "==>>> epoch: 0, batch index: 211, train loss: 1.947589\n",
      "==>>> epoch: 0, batch index: 212, train loss: 1.939601\n",
      "==>>> epoch: 0, batch index: 213, train loss: 1.949507\n",
      "==>>> epoch: 0, batch index: 214, train loss: 1.967769\n",
      "==>>> epoch: 0, batch index: 215, train loss: 1.927174\n",
      "==>>> epoch: 0, batch index: 216, train loss: 1.933370\n",
      "==>>> epoch: 0, batch index: 217, train loss: 1.950386\n",
      "==>>> epoch: 0, batch index: 218, train loss: 1.934768\n",
      "==>>> epoch: 0, batch index: 219, train loss: 1.948030\n",
      "==>>> epoch: 0, batch index: 220, train loss: 1.944351\n",
      "==>>> epoch: 0, batch index: 221, train loss: 1.932459\n",
      "==>>> epoch: 0, batch index: 222, train loss: 1.942932\n",
      "==>>> epoch: 0, batch index: 223, train loss: 1.958276\n",
      "==>>> epoch: 0, batch index: 224, train loss: 1.928956\n",
      "==>>> epoch: 0, batch index: 225, train loss: 1.937909\n",
      "==>>> epoch: 0, batch index: 226, train loss: 1.926653\n",
      "==>>> epoch: 0, batch index: 227, train loss: 1.919968\n",
      "==>>> epoch: 0, batch index: 228, train loss: 1.940762\n",
      "==>>> epoch: 0, batch index: 229, train loss: 1.946546\n",
      "==>>> epoch: 0, batch index: 230, train loss: 1.959163\n",
      "==>>> epoch: 0, batch index: 231, train loss: 1.935218\n",
      "==>>> epoch: 0, batch index: 232, train loss: 1.945211\n",
      "==>>> epoch: 0, batch index: 233, train loss: 1.950571\n",
      "==>>> epoch: 0, batch index: 234, train loss: 1.934966\n",
      "==>>> epoch: 0, batch index: 235, train loss: 1.948101\n",
      "==>>> epoch: 0, batch index: 236, train loss: 1.932873\n",
      "==>>> epoch: 0, batch index: 237, train loss: 1.961330\n",
      "==>>> epoch: 0, batch index: 238, train loss: 1.953360\n",
      "==>>> epoch: 0, batch index: 239, train loss: 1.951955\n",
      "==>>> epoch: 0, batch index: 240, train loss: 1.943035\n",
      "==>>> epoch: 0, batch index: 241, train loss: 1.932957\n",
      "==>>> epoch: 0, batch index: 242, train loss: 1.935348\n",
      "==>>> epoch: 0, batch index: 243, train loss: 1.941502\n",
      "==>>> epoch: 0, batch index: 244, train loss: 1.945238\n",
      "==>>> epoch: 0, batch index: 245, train loss: 1.936279\n",
      "==>>> epoch: 0, batch index: 246, train loss: 1.941302\n",
      "==>>> epoch: 0, batch index: 247, train loss: 1.930584\n",
      "==>>> epoch: 0, batch index: 248, train loss: 1.920989\n",
      "==>>> epoch: 0, batch index: 249, train loss: 1.930792\n",
      "==>>> epoch: 0, batch index: 250, train loss: 1.930779\n",
      "==>>> epoch: 0, batch index: 251, train loss: 1.923256\n",
      "==>>> epoch: 0, batch index: 252, train loss: 1.915901\n",
      "==>>> epoch: 0, batch index: 253, train loss: 1.934444\n",
      "==>>> epoch: 0, batch index: 254, train loss: 1.940130\n",
      "==>>> epoch: 0, batch index: 255, train loss: 1.941204\n",
      "==>>> epoch: 0, batch index: 256, train loss: 1.921539\n",
      "==>>> epoch: 0, batch index: 257, train loss: 1.911017\n",
      "==>>> epoch: 0, batch index: 258, train loss: 1.913794\n",
      "==>>> epoch: 0, batch index: 259, train loss: 1.921874\n",
      "==>>> epoch: 0, batch index: 260, train loss: 1.922645\n",
      "==>>> epoch: 0, batch index: 261, train loss: 1.923268\n",
      "==>>> epoch: 0, batch index: 262, train loss: 1.927472\n",
      "==>>> epoch: 0, batch index: 263, train loss: 1.918302\n",
      "==>>> epoch: 0, batch index: 264, train loss: 1.918013\n",
      "==>>> epoch: 0, batch index: 265, train loss: 1.944410\n",
      "==>>> epoch: 0, batch index: 266, train loss: 1.907410\n",
      "==>>> epoch: 0, batch index: 267, train loss: 1.935106\n",
      "==>>> epoch: 0, batch index: 268, train loss: 1.945137\n",
      "==>>> epoch: 0, batch index: 269, train loss: 1.913530\n",
      "==>>> epoch: 0, batch index: 270, train loss: 1.912577\n",
      "==>>> epoch: 0, batch index: 271, train loss: 1.929749\n",
      "==>>> epoch: 0, batch index: 272, train loss: 1.930028\n",
      "==>>> epoch: 0, batch index: 273, train loss: 1.921044\n",
      "==>>> epoch: 0, batch index: 274, train loss: 1.921269\n",
      "==>>> epoch: 0, batch index: 275, train loss: 1.914129\n",
      "==>>> epoch: 0, batch index: 276, train loss: 1.909612\n",
      "==>>> epoch: 0, batch index: 277, train loss: 1.901488\n",
      "==>>> epoch: 0, batch index: 278, train loss: 1.907217\n",
      "==>>> epoch: 0, batch index: 279, train loss: 1.910821\n",
      "==>>> epoch: 0, batch index: 280, train loss: 1.903044\n",
      "==>>> epoch: 0, batch index: 281, train loss: 1.902687\n",
      "==>>> epoch: 0, batch index: 282, train loss: 1.904794\n",
      "==>>> epoch: 0, batch index: 283, train loss: 1.931910\n",
      "==>>> epoch: 0, batch index: 284, train loss: 1.893984\n",
      "==>>> epoch: 0, batch index: 285, train loss: 1.901038\n",
      "==>>> epoch: 0, batch index: 286, train loss: 1.930165\n",
      "==>>> epoch: 0, batch index: 287, train loss: 1.907264\n",
      "==>>> epoch: 0, batch index: 288, train loss: 1.911403\n",
      "==>>> epoch: 0, batch index: 289, train loss: 1.915514\n",
      "==>>> epoch: 0, batch index: 290, train loss: 1.916942\n",
      "==>>> epoch: 0, batch index: 291, train loss: 1.904541\n",
      "==>>> epoch: 0, batch index: 292, train loss: 1.897490\n",
      "==>>> epoch: 0, batch index: 293, train loss: 1.902573\n",
      "==>>> epoch: 0, batch index: 294, train loss: 1.892059\n",
      "==>>> epoch: 0, batch index: 295, train loss: 1.901977\n",
      "==>>> epoch: 0, batch index: 296, train loss: 1.890749\n",
      "==>>> epoch: 0, batch index: 297, train loss: 1.915873\n",
      "==>>> epoch: 0, batch index: 298, train loss: 1.897432\n",
      "==>>> epoch: 0, batch index: 299, train loss: 1.902418\n",
      "==>>> epoch: 0, batch index: 300, train loss: 1.884774\n",
      "==>>> epoch: 0, batch index: 301, train loss: 1.899375\n",
      "==>>> epoch: 0, batch index: 302, train loss: 1.906127\n",
      "==>>> epoch: 0, batch index: 303, train loss: 1.894727\n",
      "==>>> epoch: 0, batch index: 304, train loss: 1.880810\n",
      "==>>> epoch: 0, batch index: 305, train loss: 1.897596\n",
      "==>>> epoch: 0, batch index: 306, train loss: 1.885521\n",
      "==>>> epoch: 0, batch index: 307, train loss: 1.898838\n",
      "==>>> epoch: 0, batch index: 308, train loss: 1.894376\n",
      "==>>> epoch: 0, batch index: 309, train loss: 1.915620\n",
      "==>>> epoch: 0, batch index: 310, train loss: 1.918734\n",
      "==>>> epoch: 0, batch index: 311, train loss: 1.906058\n",
      "==>>> epoch: 0, batch index: 312, train loss: 1.909817\n",
      "==>>> epoch: 0, batch index: 313, train loss: 1.897816\n",
      "==>>> epoch: 0, batch index: 314, train loss: 1.909220\n",
      "==>>> epoch: 0, batch index: 315, train loss: 1.912078\n",
      "==>>> epoch: 0, batch index: 316, train loss: 1.883493\n",
      "==>>> epoch: 0, batch index: 317, train loss: 1.902470\n",
      "==>>> epoch: 0, batch index: 318, train loss: 1.887483\n",
      "==>>> epoch: 0, batch index: 319, train loss: 1.881069\n",
      "==>>> epoch: 0, batch index: 320, train loss: 1.901113\n",
      "==>>> epoch: 0, batch index: 321, train loss: 1.895435\n",
      "==>>> epoch: 0, batch index: 322, train loss: 1.873815\n",
      "==>>> epoch: 0, batch index: 323, train loss: 1.863203\n",
      "==>>> epoch: 0, batch index: 324, train loss: 1.876404\n",
      "==>>> epoch: 0, batch index: 325, train loss: 1.888940\n",
      "==>>> epoch: 0, batch index: 326, train loss: 1.894080\n",
      "==>>> epoch: 0, batch index: 327, train loss: 1.881301\n",
      "==>>> epoch: 0, batch index: 328, train loss: 1.879260\n",
      "==>>> epoch: 0, batch index: 329, train loss: 1.867508\n",
      "==>>> epoch: 0, batch index: 330, train loss: 1.906886\n",
      "==>>> epoch: 0, batch index: 331, train loss: 1.861579\n",
      "==>>> epoch: 0, batch index: 332, train loss: 1.879563\n",
      "==>>> epoch: 0, batch index: 333, train loss: 1.885153\n",
      "==>>> epoch: 0, batch index: 334, train loss: 1.889074\n",
      "==>>> epoch: 0, batch index: 335, train loss: 1.875882\n",
      "==>>> epoch: 0, batch index: 336, train loss: 1.851599\n",
      "==>>> epoch: 0, batch index: 337, train loss: 1.880980\n",
      "==>>> epoch: 0, batch index: 338, train loss: 1.867119\n",
      "==>>> epoch: 0, batch index: 339, train loss: 1.858095\n",
      "==>>> epoch: 0, batch index: 340, train loss: 1.883471\n",
      "==>>> epoch: 0, batch index: 341, train loss: 1.864441\n",
      "==>>> epoch: 0, batch index: 342, train loss: 1.887106\n",
      "==>>> epoch: 0, batch index: 343, train loss: 1.853859\n",
      "==>>> epoch: 0, batch index: 344, train loss: 1.861951\n",
      "==>>> epoch: 0, batch index: 345, train loss: 1.894042\n",
      "==>>> epoch: 0, batch index: 346, train loss: 1.862052\n",
      "==>>> epoch: 0, batch index: 347, train loss: 1.852136\n",
      "==>>> epoch: 0, batch index: 348, train loss: 1.864124\n",
      "==>>> epoch: 0, batch index: 349, train loss: 1.844382\n",
      "==>>> epoch: 0, batch index: 350, train loss: 1.873924\n",
      "==>>> epoch: 0, batch index: 351, train loss: 1.862592\n",
      "==>>> epoch: 0, batch index: 352, train loss: 1.856061\n",
      "==>>> epoch: 0, batch index: 353, train loss: 1.846728\n",
      "==>>> epoch: 0, batch index: 354, train loss: 1.867991\n",
      "==>>> epoch: 0, batch index: 355, train loss: 1.865979\n",
      "==>>> epoch: 0, batch index: 356, train loss: 1.866746\n",
      "==>>> epoch: 0, batch index: 357, train loss: 1.855265\n",
      "==>>> epoch: 0, batch index: 358, train loss: 1.862116\n",
      "==>>> epoch: 0, batch index: 359, train loss: 1.844556\n",
      "==>>> epoch: 0, batch index: 360, train loss: 1.841818\n",
      "==>>> epoch: 0, batch index: 361, train loss: 1.848049\n",
      "==>>> epoch: 0, batch index: 362, train loss: 1.829693\n",
      "==>>> epoch: 0, batch index: 363, train loss: 1.846637\n",
      "==>>> epoch: 0, batch index: 364, train loss: 1.825836\n",
      "==>>> epoch: 0, batch index: 365, train loss: 1.831572\n",
      "==>>> epoch: 0, batch index: 366, train loss: 1.855093\n",
      "==>>> epoch: 0, batch index: 367, train loss: 1.842201\n",
      "==>>> epoch: 0, batch index: 368, train loss: 1.821605\n",
      "==>>> epoch: 0, batch index: 369, train loss: 1.833759\n",
      "==>>> epoch: 0, batch index: 370, train loss: 1.837616\n",
      "==>>> epoch: 0, batch index: 371, train loss: 1.826569\n",
      "==>>> epoch: 0, batch index: 372, train loss: 1.840924\n",
      "==>>> epoch: 0, batch index: 373, train loss: 1.839635\n",
      "==>>> epoch: 0, batch index: 374, train loss: 1.824898\n",
      "==>>> epoch: 0, batch index: 375, train loss: 1.861979\n",
      "==>>> epoch: 0, batch index: 376, train loss: 1.804410\n",
      "==>>> epoch: 0, batch index: 377, train loss: 1.810968\n",
      "==>>> epoch: 0, batch index: 378, train loss: 1.840912\n",
      "==>>> epoch: 0, batch index: 379, train loss: 1.846129\n",
      "==>>> epoch: 0, batch index: 380, train loss: 1.824202\n",
      "==>>> epoch: 0, batch index: 381, train loss: 1.824302\n",
      "==>>> epoch: 0, batch index: 382, train loss: 1.791883\n",
      "==>>> epoch: 0, batch index: 383, train loss: 1.807737\n",
      "==>>> epoch: 0, batch index: 384, train loss: 1.825528\n",
      "==>>> epoch: 0, batch index: 385, train loss: 1.801928\n",
      "==>>> epoch: 0, batch index: 386, train loss: 1.803005\n",
      "==>>> epoch: 0, batch index: 387, train loss: 1.801103\n",
      "==>>> epoch: 0, batch index: 388, train loss: 1.790186\n",
      "==>>> epoch: 0, batch index: 389, train loss: 1.820220\n",
      "==>>> epoch: 0, batch index: 390, train loss: 1.813097\n",
      "==>>> epoch: 0, batch index: 391, train loss: 1.817831\n",
      "==>>> epoch: 0, batch index: 392, train loss: 1.796564\n",
      "==>>> epoch: 0, batch index: 393, train loss: 1.795573\n",
      "==>>> epoch: 0, batch index: 394, train loss: 1.778679\n",
      "==>>> epoch: 0, batch index: 395, train loss: 1.815602\n",
      "==>>> epoch: 0, batch index: 396, train loss: 1.830559\n",
      "==>>> epoch: 0, batch index: 397, train loss: 1.779351\n",
      "==>>> epoch: 0, batch index: 398, train loss: 1.801232\n",
      "==>>> epoch: 0, batch index: 399, train loss: 1.776235\n",
      "==>>> epoch: 0, batch index: 400, train loss: 1.800829\n",
      "==>>> epoch: 0, batch index: 401, train loss: 1.837694\n",
      "==>>> epoch: 0, batch index: 402, train loss: 1.800148\n",
      "==>>> epoch: 0, batch index: 403, train loss: 1.791505\n",
      "==>>> epoch: 0, batch index: 404, train loss: 1.765370\n",
      "==>>> epoch: 0, batch index: 405, train loss: 1.791987\n",
      "==>>> epoch: 0, batch index: 406, train loss: 1.759160\n",
      "==>>> epoch: 0, batch index: 407, train loss: 1.784478\n",
      "==>>> epoch: 0, batch index: 408, train loss: 1.771641\n",
      "==>>> epoch: 0, batch index: 409, train loss: 1.780566\n",
      "==>>> epoch: 0, batch index: 410, train loss: 1.744883\n",
      "==>>> epoch: 0, batch index: 411, train loss: 1.764324\n",
      "==>>> epoch: 0, batch index: 412, train loss: 1.778906\n",
      "==>>> epoch: 0, batch index: 413, train loss: 1.756983\n",
      "==>>> epoch: 0, batch index: 414, train loss: 1.786454\n",
      "==>>> epoch: 0, batch index: 415, train loss: 1.820827\n",
      "==>>> epoch: 0, batch index: 416, train loss: 1.757504\n",
      "==>>> epoch: 0, batch index: 417, train loss: 1.765474\n",
      "==>>> epoch: 0, batch index: 418, train loss: 1.749932\n",
      "==>>> epoch: 0, batch index: 419, train loss: 1.789762\n",
      "==>>> epoch: 0, batch index: 420, train loss: 1.773214\n",
      "==>>> epoch: 0, batch index: 421, train loss: 1.746859\n",
      "==>>> epoch: 0, batch index: 422, train loss: 1.742604\n",
      "==>>> epoch: 0, batch index: 423, train loss: 1.734958\n",
      "==>>> epoch: 0, batch index: 424, train loss: 1.785155\n",
      "==>>> epoch: 0, batch index: 425, train loss: 1.779144\n",
      "==>>> epoch: 0, batch index: 426, train loss: 1.747956\n",
      "==>>> epoch: 0, batch index: 427, train loss: 1.760450\n",
      "==>>> epoch: 0, batch index: 428, train loss: 1.728237\n",
      "==>>> epoch: 0, batch index: 429, train loss: 1.737299\n",
      "==>>> epoch: 0, batch index: 430, train loss: 1.739791\n",
      "==>>> epoch: 0, batch index: 431, train loss: 1.738582\n",
      "==>>> epoch: 0, batch index: 432, train loss: 1.728905\n",
      "==>>> epoch: 0, batch index: 433, train loss: 1.734303\n",
      "==>>> epoch: 0, batch index: 434, train loss: 1.724640\n",
      "==>>> epoch: 0, batch index: 435, train loss: 1.742745\n",
      "==>>> epoch: 0, batch index: 436, train loss: 1.734829\n",
      "==>>> epoch: 0, batch index: 437, train loss: 1.742986\n",
      "==>>> epoch: 0, batch index: 438, train loss: 1.712418\n",
      "==>>> epoch: 0, batch index: 439, train loss: 1.716791\n",
      "==>>> epoch: 0, batch index: 440, train loss: 1.753336\n",
      "==>>> epoch: 0, batch index: 441, train loss: 1.743511\n",
      "==>>> epoch: 0, batch index: 442, train loss: 1.720302\n",
      "==>>> epoch: 0, batch index: 443, train loss: 1.722719\n",
      "==>>> epoch: 0, batch index: 444, train loss: 1.724517\n",
      "==>>> epoch: 0, batch index: 445, train loss: 1.706281\n",
      "==>>> epoch: 0, batch index: 446, train loss: 1.711142\n",
      "==>>> epoch: 0, batch index: 447, train loss: 1.730640\n",
      "==>>> epoch: 0, batch index: 448, train loss: 1.724193\n",
      "==>>> epoch: 0, batch index: 449, train loss: 1.703871\n",
      "==>>> epoch: 0, batch index: 450, train loss: 1.696748\n",
      "==>>> epoch: 0, batch index: 451, train loss: 1.736099\n",
      "==>>> epoch: 0, batch index: 452, train loss: 1.727302\n",
      "==>>> epoch: 0, batch index: 453, train loss: 1.727614\n",
      "==>>> epoch: 0, batch index: 454, train loss: 1.708935\n",
      "==>>> epoch: 0, batch index: 455, train loss: 1.679708\n",
      "==>>> epoch: 0, batch index: 456, train loss: 1.699372\n",
      "==>>> epoch: 0, batch index: 457, train loss: 1.722699\n",
      "==>>> epoch: 0, batch index: 458, train loss: 1.702696\n",
      "==>>> epoch: 0, batch index: 459, train loss: 1.711619\n",
      "==>>> epoch: 0, batch index: 460, train loss: 1.726532\n",
      "==>>> epoch: 0, batch index: 461, train loss: 1.718491\n",
      "==>>> epoch: 0, batch index: 462, train loss: 1.729146\n",
      "==>>> epoch: 0, batch index: 463, train loss: 1.674652\n",
      "==>>> epoch: 0, batch index: 464, train loss: 1.714529\n",
      "==>>> epoch: 0, batch index: 465, train loss: 1.705466\n",
      "==>>> epoch: 0, batch index: 466, train loss: 1.720576\n",
      "==>>> epoch: 0, batch index: 467, train loss: 1.734159\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (96) must match the existing size (128) at non-singleton dimension 0. at /Users/soumith/miniconda2/conda-bld/pytorch_1503975474428/work/torch/lib/TH/generic/THTensor.c:308",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-53eadf0b355e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/apple/anaconda/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-47-2df7704ba193>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, target)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary_capsule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdigital_capsule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/apple/anaconda/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-a39654793ffe>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_capsules\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m                                              \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;31m#             print(\"w with batch\", self.weights.size())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/apple/anaconda/lib/python3.5/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mexpand\u001b[0;34m(self, *sizes)\u001b[0m\n\u001b[1;32m    720\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0msizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mExpand\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexpand_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/apple/anaconda/lib/python3.5/site-packages/torch/autograd/_functions/tensor.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, i, new_size)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;31m# tuple containing torch.Size or a list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnew_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_unsqueezed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         ctx.expanded_dims = [dim for dim, (expanded, original)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (96) must match the existing size (128) at non-singleton dimension 0. at /Users/soumith/miniconda2/conda-bld/pytorch_1503975474428/work/torch/lib/TH/generic/THTensor.c:308"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(net.parameters(), lr=0.01)\n",
    "for epoch in range(10):\n",
    "    # trainning\n",
    "    for batch_idx, (x, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        x, target = Variable(x), Variable(target)\n",
    "        _, loss = net(x, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "#         if batch_idx % 100 == 0:\n",
    "        print('==>>> epoch: {}, batch index: {}, train loss: {:.6f}'.format(epoch, batch_idx, loss.data[0]))\n",
    "    # testing\n",
    "    correct_cnt, ave_loss = 0, 0\n",
    "    for batch_idx, (x, target) in enumerate(test_loader):\n",
    "        x, target = Variable(x, volatile=True), Variable(target, volatile=True)\n",
    "        score, loss = net(x, target)\n",
    "        _, pred_label = torch.max(score.data, 1)\n",
    "        correct_cnt += (pred_label == target.data).sum()\n",
    "        ave_loss += loss.data[0]\n",
    "    accuracy = correct_cnt*1.0/len(test_loader)/batch_size\n",
    "    ave_loss /= len(test_loader)\n",
    "    print('==>>> epoch: {}, test loss: {:.6f}, accuracy: {:.4f}'.format(epoch, ave_loss, accuracy))\n",
    "\n",
    "torch.save(net.state_dict(), 'capsule_net')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
